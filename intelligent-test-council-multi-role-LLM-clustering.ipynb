{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdb33ae6-3526-471c-99c5-d1c32181b861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: numpy<2.0 in ./venv/lib/python3.9/site-packages (1.26.4)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 25.2 is available.\n",
      "You should consider upgrading via the '/Users/sepehr/IdeaProjects/testgen-council/venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: openai in ./venv/lib/python3.9/site-packages (1.108.1)\n",
      "Requirement already satisfied: transformers in ./venv/lib/python3.9/site-packages (4.56.2)\n",
      "Requirement already satisfied: scikit-learn in ./venv/lib/python3.9/site-packages (1.6.1)\n",
      "Requirement already satisfied: ast2json in ./venv/lib/python3.9/site-packages (0.4)\n",
      "Requirement already satisfied: pytest in ./venv/lib/python3.9/site-packages (8.4.2)\n",
      "Requirement already satisfied: coverage in ./venv/lib/python3.9/site-packages (7.10.7)\n",
      "Requirement already satisfied: pytest-cov in ./venv/lib/python3.9/site-packages (7.0.0)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.9/site-packages (2.3.2)\n",
      "Requirement already satisfied: matplotlib in ./venv/lib/python3.9/site-packages (3.9.4)\n",
      "Requirement already satisfied: seaborn in ./venv/lib/python3.9/site-packages (0.13.2)\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.9/site-packages (4.67.1)\n",
      "Requirement already satisfied: nest_asyncio in ./venv/lib/python3.9/site-packages (1.6.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./venv/lib/python3.9/site-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./venv/lib/python3.9/site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./venv/lib/python3.9/site-packages (from openai) (2.11.9)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./venv/lib/python3.9/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./venv/lib/python3.9/site-packages (from openai) (0.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./venv/lib/python3.9/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.9/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.9/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.9/site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./venv/lib/python3.9/site-packages (from transformers) (0.35.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.9/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.9/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.9/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.9/site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./venv/lib/python3.9/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./venv/lib/python3.9/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./venv/lib/python3.9/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./venv/lib/python3.9/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./venv/lib/python3.9/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in ./venv/lib/python3.9/site-packages (from pytest) (1.6.0)\n",
      "Requirement already satisfied: iniconfig>=1 in ./venv/lib/python3.9/site-packages (from pytest) (2.1.0)\n",
      "Requirement already satisfied: exceptiongroup>=1; python_version < \"3.11\" in ./venv/lib/python3.9/site-packages (from pytest) (1.3.0)\n",
      "Requirement already satisfied: pygments>=2.7.2 in ./venv/lib/python3.9/site-packages (from pytest) (2.19.2)\n",
      "Requirement already satisfied: tomli>=1; python_version < \"3.11\" in ./venv/lib/python3.9/site-packages (from pytest) (2.2.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.9/site-packages (from matplotlib) (4.60.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0; python_version < \"3.10\" in ./venv/lib/python3.9/site-packages (from matplotlib) (6.5.2)\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.9/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.9/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./venv/lib/python3.9/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pillow>=8 in ./venv/lib/python3.9/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./venv/lib/python3.9/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: idna>=2.8 in ./venv/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./venv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./venv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3; platform_machine == \"x86_64\" or platform_machine == \"amd64\" or platform_machine == \"arm64\" or platform_machine == \"aarch64\" in ./venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.9/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.9/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in ./venv/lib/python3.9/site-packages (from importlib-resources>=3.2.0; python_version < \"3.10\"->matplotlib) (3.23.0)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 25.2 is available.\n",
      "You should consider upgrading via the '/Users/sepehr/IdeaProjects/testgen-council/venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: ipywidgets in ./venv/lib/python3.9/site-packages (8.1.7)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./venv/lib/python3.9/site-packages (from ipywidgets) (8.18.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in ./venv/lib/python3.9/site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./venv/lib/python3.9/site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./venv/lib/python3.9/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in ./venv/lib/python3.9/site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: typing-extensions; python_version < \"3.10\" in ./venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (4.15.0)\n",
      "Requirement already satisfied: jedi>=0.16 in ./venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: exceptiongroup; python_version < \"3.11\" in ./venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (1.3.0)\n",
      "Requirement already satisfied: decorator in ./venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: matplotlib-inline in ./venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in ./venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pexpect>4.3; sys_platform != \"win32\" in ./venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: stack-data in ./venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./venv/lib/python3.9/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: wcwidth in ./venv/lib/python3.9/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./venv/lib/python3.9/site-packages (from pexpect>4.3; sys_platform != \"win32\"->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./venv/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in ./venv/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./venv/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 25.2 is available.\n",
      "You should consider upgrading via the '/Users/sepehr/IdeaProjects/testgen-council/venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/bin/jupyter-nbextension\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/jupyter_core/application.py\", line 264, in launch_instance\n",
      "    return super(JupyterApp, cls).launch_instance(argv=argv, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/notebook/nbextensions.py\", line 980, in start\n",
      "    super().start()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/jupyter_core/application.py\", line 253, in start\n",
      "    self.subapp.start()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/notebook/nbextensions.py\", line 888, in start\n",
      "    self.toggle_nbextension_python(self.extra_args[0])\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/notebook/nbextensions.py\", line 861, in toggle_nbextension_python\n",
      "    return toggle(module,\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/notebook/nbextensions.py\", line 474, in enable_nbextension_python\n",
      "    return _set_nbextension_state_python(True, module, user, sys_prefix,\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/notebook/nbextensions.py\", line 372, in _set_nbextension_state_python\n",
      "    m, nbexts = _get_nbextension_metadata(module)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/notebook/nbextensions.py\", line 1114, in _get_nbextension_metadata\n",
      "    m = import_item(module)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/traitlets/utils/importstring.py\", line 38, in import_item\n",
      "    return __import__(parts[0])\n",
      "ModuleNotFoundError: No module named 'widgetsnbextension'\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Install Required Dependencies with Version Constraints\n",
    "!pip install \"numpy<2.0\" --upgrade\n",
    "!pip install openai transformers scikit-learn ast2json pytest coverage pytest-cov pandas matplotlib seaborn tqdm nest_asyncio\n",
    "!pip install ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db5552eb-bf52-4777-ac8b-a90a6472b7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.26.4\n",
      "Pandas version: 2.3.2\n",
      "Scikit-learn imported successfully\n",
      "OpenAI library version: 1.108.1\n",
      "Plotting libraries imported successfully\n",
      "tqdm imported successfully\n",
      "All imports completed!\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Import Required Libraries with Error Handling\n",
    "import ast\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "import tempfile\n",
    "from typing import List, Dict, Tuple, Any\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core data science imports\n",
    "try:\n",
    "    import numpy as np\n",
    "    print(f\"NumPy version: {np.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"NumPy import error: {e}\")\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "    print(f\"Pandas version: {pd.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"Pandas import error: {e}\")\n",
    "\n",
    "# ML and NLP imports\n",
    "try:\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    print(\"Scikit-learn imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"Scikit-learn import error: {e}\")\n",
    "\n",
    "# OpenAI import\n",
    "try:\n",
    "    import openai\n",
    "    print(f\"OpenAI library version: {openai.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"OpenAI import error: {e}\")\n",
    "\n",
    "# Plotting imports\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    plt.rcParams['figure.figsize'] = (12, 8)\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    print(\"Plotting libraries imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"Plotting libraries import error: {e}\")\n",
    "\n",
    "# Progress bar\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    print(\"tqdm imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"tqdm import error: {e}\")\n",
    "\n",
    "print(\"All imports completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74a6e284-f0df-44b4-a68a-16d89dfa588e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration loaded with role-based personas:\n",
      "   🎭 By-the-Book QA Engineer\n",
      "   🎭 Agent of Chaos\n",
      "   🎭 Paranoid Security Auditor\n",
      "   🎭 Abstract Thinker\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Configuration and API Setup\n",
    "SYNTHESIZER_MODEL = \"gemini-2.0-flash\"\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Configuration class for the intelligent council system\"\"\"\n",
    "    \n",
    "    # API Keys (replace with your actual keys)\n",
    "    OPENAI_API_KEY = \"sk-JdU36bC7BG2996XHH3YmKOQG8Xm9x9ii5u5E9uwPC54oAkHE\"\n",
    "    \n",
    "    # Base URLs for different providers\n",
    "    OPENAI_BASE_URL = \"https://api.gapgpt.app/v1\"  # Default OpenAI\n",
    "\n",
    "    \n",
    "    # Model configurations\n",
    "    LLM_MODELS = {\n",
    "        \"gemini-2.0-flash\": {\n",
    "            \"type\": \"openai\",\n",
    "            \"model_name\": \"gemini-2.0-flash\",\n",
    "            \"base_url\": OPENAI_BASE_URL,\n",
    "            \"api_key\": OPENAI_API_KEY,\n",
    "        },\n",
    "        \"grok-3-mini\": {\n",
    "            \"type\": \"openai\", \n",
    "            \"model_name\": \"grok-3-mini\",\n",
    "            \"base_url\": OPENAI_BASE_URL,\n",
    "            \"api_key\": OPENAI_API_KEY,\n",
    "        },\n",
    "        \"qwen3-235b-a22b\": {\n",
    "            \"type\": \"openai\",\n",
    "            \"model_name\": \"qwen3-235b-a22b\",\n",
    "            \"base_url\": OPENAI_BASE_URL,\n",
    "            \"api_key\": OPENAI_API_KEY,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Role-Based Test Generation Personas\n",
    "    ROLES = {\n",
    "        \"qa_engineer\": {\n",
    "            \"name\": \"By-the-Book QA Engineer\",\n",
    "            \"philosophy\": \"Meticulous and systematic. Focuses on covering the function's explicit requirements.\",\n",
    "            \"focus_categories\": [\"positive\", \"boundary\"],\n",
    "            \"prompt_persona\": \"\"\"You are a meticulous QA Engineer with 15 years of experience in software testing. Your primary goal is to verify that the function behaves exactly as described in its documentation.\n",
    "\n",
    "YOUR MISSION:\n",
    "- Generate high-quality, standard tests that cover the core functionality\n",
    "- Focus on positive test cases (normal, expected usage)\n",
    "- Test boundary conditions explicitly mentioned in the specification\n",
    "- Ensure every part of the docstring's promise is tested\n",
    "- Write clear, maintainable tests that serve as documentation\n",
    "\n",
    "APPROACH:\n",
    "1. Read the function signature and docstring carefully\n",
    "2. Identify all promised behaviors\n",
    "3. Create tests for typical use cases\n",
    "4. Test boundary values (min, max, empty, single element)\n",
    "5. Verify return types and value ranges match specifications\n",
    "\n",
    "Generate well-structured tests following pytest best practices.\"\"\"\n",
    "        },\n",
    "        \n",
    "        \"agent_of_chaos\": {\n",
    "            \"name\": \"Agent of Chaos\",\n",
    "            \"philosophy\": \"If it can break, I will find a way. Make the function fail.\",\n",
    "            \"focus_categories\": [\"negative\", \"edge_case\"],\n",
    "            \"prompt_persona\": \"\"\"You are a destructive tester known as the \"Agent of Chaos\". Your mission is to BREAK this function by any means necessary.\n",
    "\n",
    "YOUR MISSION:\n",
    "- Find every possible way the function can fail\n",
    "- Generate tests that SHOULD raise exceptions\n",
    "- Think about unexpected, malformed, or adversarial inputs\n",
    "- Test with wrong types, None values, empty data structures\n",
    "- Push the function beyond its limits\n",
    "\n",
    "ATTACK VECTORS TO CONSIDER:\n",
    "1. Type violations (pass string when int expected, etc.)\n",
    "2. Null/None inputs where objects are expected\n",
    "3. Empty collections ([], {}, \"\")\n",
    "4. Extreme values (very large numbers, very long strings)\n",
    "5. Negative numbers where positive expected\n",
    "6. Zero division scenarios\n",
    "7. Invalid combinations of parameters\n",
    "8. Corrupted or malformed data structures\n",
    "\n",
    "Generate tests that you expect will raise specific exceptions (TypeError, ValueError, IndexError, ZeroDivisionError, etc.). Use pytest.raises() to verify these failures.\"\"\"\n",
    "        },\n",
    "        \n",
    "        \"security_auditor\": {\n",
    "            \"name\": \"Paranoid Security Auditor\",\n",
    "            \"philosophy\": \"Trust nothing. Assume all input is hostile.\",\n",
    "            \"focus_categories\": [\"security\", \"negative\"],\n",
    "            \"prompt_persona\": \"\"\"You are a cybersecurity expert and penetration tester. Your task is to find security vulnerabilities in this code.\n",
    "\n",
    "YOUR MISSION:\n",
    "- Analyze the function for potential security flaws\n",
    "- Generate tests that attempt to exploit vulnerabilities\n",
    "- Think like an attacker trying to compromise the system\n",
    "\n",
    "SECURITY CONCERNS TO TEST:\n",
    "1. **Injection Attacks**: SQL injection, command injection, code injection\n",
    "2. **Path Traversal**: Attempts to access files outside intended directory (../, absolute paths)\n",
    "3. **Buffer Overflow**: Oversized inputs that might cause issues\n",
    "4. **Format String Attacks**: Special characters in strings (%s, %d, {}, etc.)\n",
    "5. **Insecure Deserialization**: Malicious pickled objects or JSON\n",
    "6. **Input Validation Bypass**: Special characters, Unicode, null bytes\n",
    "7. **Resource Exhaustion**: Inputs that could cause infinite loops or memory issues\n",
    "8. **Data Leakage**: Can the function expose sensitive information?\n",
    "\n",
    "Generate security-focused tests. If the function has file operations, test path traversal. If it processes strings, test injection. If it handles numbers, test integer overflow. If no obvious vulnerabilities exist, test with security-minded inputs (special characters, scripts, oversized data).\"\"\"\n",
    "        },\n",
    "        \n",
    "        \"abstract_thinker\": {\n",
    "            \"name\": \"Abstract Thinker\",\n",
    "            \"philosophy\": \"Test the underlying properties and invariants, not just specific cases.\",\n",
    "            \"focus_categories\": [\"positive\", \"boundary\", \"edge_case\"],\n",
    "            \"prompt_persona\": \"\"\"You are a computer scientist specializing in formal methods and property-based testing. Your goal is to verify the fundamental mathematical and logical properties of this function.\n",
    "\n",
    "YOUR MISSION:\n",
    "- Think beyond specific test cases to general properties\n",
    "- Identify invariants that must always hold\n",
    "- Create tests that verify logical consistency\n",
    "- Check mathematical properties and relationships\n",
    "\n",
    "PROPERTIES TO CONSIDER:\n",
    "1. **Identity Properties**: f(x) with some operation returns x\n",
    "2. **Inverse Properties**: decode(encode(x)) == x\n",
    "3. **Idempotency**: f(f(x)) == f(x) for some functions\n",
    "4. **Commutativity**: Does order matter? f(a,b) == f(b,a)?\n",
    "5. **Associativity**: f(f(a,b),c) == f(a,f(b,c))?\n",
    "6. **Preservation Properties**: Input length = output length?\n",
    "7. **Boundary Properties**: For sorted output, output[i] <= output[i+1]\n",
    "8. **Type Invariants**: Output type consistent with specification?\n",
    "9. **Domain/Range Properties**: All outputs within valid range?\n",
    "\n",
    "Generate property-based tests. You may use standard pytest format or suggest hypothesis library tests. Focus on testing fundamental truths about the function's behavior rather than specific input-output pairs.\"\"\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Model-Role Assignment Strategy\n",
    "    # This assigns each model to specific roles based on hypothesized strengths\n",
    "    # You can modify this based on your experimental results\n",
    "\n",
    "    # MODEL_ROLE_ASSIGNMENTS = {\n",
    "    #     \"gemini-2.0-flash\": [\"qa_engineer\", \"abstract_thinker\"],\n",
    "    #     \"grok-3-mini\": [\"agent_of_chaos\", \"security_auditor\"],\n",
    "    #     \"qwen3-235b-a22b\": [\"qa_engineer\", \"agent_of_chaos\"]\n",
    "    # }\n",
    "    \n",
    "    MODEL_ROLE_ASSIGNMENTS = {\n",
    "        \"gemini-2.0-flash\": [\"qa_engineer\", \"abstract_thinker\", \"agent_of_chaos\"],\n",
    "        \"grok-3-mini\": [\"qa_engineer\", \"agent_of_chaos\"],\n",
    "        \"qwen3-235b-a22b\": [\"abstract_thinker\", \"security_auditor\"]\n",
    "    }\n",
    "    \n",
    "    # Test categories (kept for backward compatibility)\n",
    "    TEST_CATEGORIES = [\n",
    "        \"positive\",    # مثبت - حالات عادی\n",
    "        \"negative\",    # منفی - حالات خطا\n",
    "        \"boundary\",    # مرزی - مقادیر حدی\n",
    "        \"edge_case\",   # موارد استثنایی\n",
    "        \"security\"     # امنیتی\n",
    "    ]\n",
    "\n",
    "# Initialize configuration\n",
    "config = Config()\n",
    "\n",
    "# Setup API clients\n",
    "if config.OPENAI_API_KEY != \"sk-JdU36bC7BG2996XHH3YmKOQG8Xm9x9ii5u5E9uwPC54oAkHE\":\n",
    "    openai.api_key = config.OPENAI_API_KEY\n",
    "\n",
    "print(\"✅ Configuration loaded with role-based personas:\")\n",
    "for role_id, role_info in config.ROLES.items():\n",
    "    print(f\"   🎭 {role_info['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60fc4cd7-986a-45a1-ad20-1d79a851dda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Code Analysis and AST Processing Module\n",
    "class CodeAnalyzer:\n",
    "    \"\"\"Analyzes Python code and extracts function information using AST\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_function_info(code: str) -> Dict[str, Any]:\n",
    "        \"\"\"Extract function information from Python code\"\"\"\n",
    "        try:\n",
    "            # Clean up the code string and ensure proper formatting\n",
    "            code = code.strip()\n",
    "            \n",
    "            # Try to parse with ast\n",
    "            tree = ast.parse(code)\n",
    "            functions = []\n",
    "            \n",
    "            for node in ast.walk(tree):\n",
    "                if isinstance(node, ast.FunctionDef):\n",
    "                    # Get function source by reconstructing from lines\n",
    "                    lines = code.split('\\n')\n",
    "                    start_line = node.lineno - 1\n",
    "                    end_line = node.end_lineno if hasattr(node, 'end_lineno') else len(lines)\n",
    "                    \n",
    "                    func_source = '\\n'.join(lines[start_line:end_line])\n",
    "                    \n",
    "                    func_info = {\n",
    "                        'name': node.name,\n",
    "                        'args': [arg.arg for arg in node.args.args],\n",
    "                        'docstring': ast.get_docstring(node),\n",
    "                        'source_code': func_source,\n",
    "                        'line_start': node.lineno,\n",
    "                        'line_end': node.end_lineno if hasattr(node, 'end_lineno') else len(lines)\n",
    "                    }\n",
    "                    functions.append(func_info)\n",
    "            \n",
    "            return {\n",
    "                'functions': functions,\n",
    "                'total_functions': len(functions),\n",
    "                'source_code': code\n",
    "            }\n",
    "            \n",
    "        except SyntaxError as e:\n",
    "            print(f\"Syntax error parsing code: {e}\")\n",
    "            print(f\"Error at line {e.lineno}: {e.text}\")\n",
    "            print(f\"Code that failed to parse:\\n{code}\")\n",
    "            return {'functions': [], 'total_functions': 0, 'source_code': code, 'syntax_error': str(e)}\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing code: {e}\")\n",
    "            return {'functions': [], 'total_functions': 0, 'source_code': code, 'error': str(e)}\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_test_methods_from_response(response: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Extract individual test methods from LLM response\"\"\"\n",
    "        test_methods = []\n",
    "        \n",
    "        # Try to find test functions using regex\n",
    "        test_pattern = r'def (test_\\w+)\\([^)]*\\):(.*?)(?=def test_|\\Z)'\n",
    "        matches = re.findall(test_pattern, response, re.DOTALL)\n",
    "        \n",
    "        for match in matches:\n",
    "            func_name, func_body = match\n",
    "            full_test = f\"def {func_name}():{func_body}\"\n",
    "            test_methods.append({\n",
    "                'name': func_name,\n",
    "                'code': full_test.strip()\n",
    "            })\n",
    "        \n",
    "        return test_methods\n",
    "\n",
    "# Initialize code analyzer\n",
    "code_analyzer = CodeAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c80c0ea7-4ed2-4512-82d7-0523fe8d4e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: LLM Council Module (Role-Based Version with Concurrent API Support)\n",
    "import asyncio\n",
    "from typing import Dict, Any, List, Tuple\n",
    "import openai\n",
    "\n",
    "class LLMCouncil:\n",
    "    \"\"\"Manages multiple LLM models with specialized roles for test case generation\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.models = config.LLM_MODELS\n",
    "        self.roles = config.ROLES\n",
    "        self.model_role_assignments = config.MODEL_ROLE_ASSIGNMENTS\n",
    "        self.client = openai.OpenAI(\n",
    "            base_url=config.OPENAI_BASE_URL,\n",
    "            api_key=config.OPENAI_API_KEY\n",
    "        )\n",
    "        self.async_client = openai.AsyncOpenAI(\n",
    "            base_url=config.OPENAI_BASE_URL,\n",
    "            api_key=config.OPENAI_API_KEY\n",
    "        )\n",
    "        \n",
    "    def create_role_based_prompt(self, function_info: Dict[str, Any], role_id: str) -> str:\n",
    "        \"\"\"Create a role-specific prompt for test case generation\"\"\"\n",
    "        func = function_info['functions'][0] if function_info['functions'] else {}\n",
    "        role = self.roles[role_id]\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "{role['prompt_persona']}\n",
    "\n",
    "YOUR ROLE: \"{role['name']}\"\n",
    "PHILOSOPHY: {role['philosophy']}\n",
    "\n",
    "FUNCTION TO TEST:\n",
    "```python\n",
    "{func.get('source_code', function_info['source_code'])}\n",
    "\n",
    "FUNCTION DETAILS:\n",
    "- Name: {func.get('name', 'unknown')}\n",
    "- Parameters: {', '.join(func.get('args', [])) if func.get('args') else 'None'}\n",
    "- Docstring: {func.get('docstring', 'No docstring provided')}\n",
    "\n",
    "REQUIREMENTS:\n",
    "1. Stay true to your role as \"{role['name']}\" - let your {role['philosophy'].lower()} guide your test design\n",
    "2. Focus on test categories: {', '.join(role['focus_categories'])}\n",
    "3. Label each test with a category comment from the definitions below:\n",
    "\n",
    "**CATEGORY DEFINITIONS:**\n",
    "\n",
    "**# Category: positive**\n",
    "Valid, typical inputs representing normal usage. Tests the \"happy path\" to confirm the function fulfills its contract.\n",
    "Examples: sort([3,1,2]), add(5,3), valid user credentials\n",
    "\n",
    "**# Category: negative**\n",
    "Invalid inputs that SHOULD raise exceptions. Tests graceful failure handling. MUST use pytest.raises().\n",
    "Examples: divide(5,0) → ZeroDivisionError, int(\"abc\") → ValueError, accessing non-existent files\n",
    "Key: Tests error handling, not malicious exploitation (that's security)\n",
    "\n",
    "**# Category: boundary**\n",
    "Values at the LIMITS of valid ranges where behavior might change. Tests threshold values and off-by-one errors.\n",
    "Examples: For range [1,100] test: 0, 1, 100, 101; empty list vs single element; MIN_INT/MAX_INT\n",
    "Formula: If valid range is [a,b], test: a-1, a, a+1, b-1, b, b+1\n",
    "\n",
    "**# Category: edge_case**\n",
    "VALID but UNUSUAL scenarios - rare but legitimate use cases that might be overlooked.\n",
    "Examples: already-sorted lists, all duplicates [5,5,5,5], negative indices, float('inf'), unicode \"emoji😊\"\n",
    "Key: Unusual but still valid inputs, not boundaries of ranges\n",
    "\n",
    "**# Category: security**\n",
    "MALICIOUS/ADVERSARIAL inputs testing exploitation resistance. Focus on attack vectors and vulnerabilities.\n",
    "Examples: SQL injection \"'; DROP TABLE;--\", path traversal \"../../../etc/passwd\", XSS \"<script>\", command injection \"; rm -rf /\", extremely long strings (DoS)\n",
    "Key: Testing if function can be exploited, not just validation (that's negative tests)\n",
    "\n",
    "4. Use pytest format with descriptive test names\n",
    "5. Include clear assertions with meaningful error messages\n",
    "6. Add docstrings explaining what each test verifies\n",
    "\n",
    "CRITICAL: Your tests must reflect your role's philosophy: {role['philosophy']}\n",
    "Your unique perspective as \"{role['name']}\" should be evident in test selection and design.\n",
    "\n",
    "EXAMPLE FORMAT:\n",
    "\n",
    "python\n",
    "import pytest\n",
    "\n",
    "def test_function_name_descriptive_scenario():\n",
    "    '''Clear description of what this test verifies'''\n",
    "    # Category: [appropriate category]\n",
    "    # Your test implementation here\n",
    "    result = function_name(test_input)\n",
    "    assert result == expected, \"Clear assertion message\"\n",
    "\n",
    "Generate your role-specific tests now:\n",
    "\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def call_openai_model(self, prompt: str, model_config: Dict) -> str:\n",
    "        \"\"\"Call OpenAI API (synchronous version)\"\"\"\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=model_config[\"model_name\"],\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error calling OpenAI API: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    async def call_openai_model_async(self, prompt: str, model_config: Dict, \n",
    "                                      model_name: str, role_id: str) -> Tuple[str, str, str]:\n",
    "        \"\"\"Call OpenAI API asynchronously with tracking info\"\"\"\n",
    "        try:\n",
    "            response = await self.async_client.chat.completions.create(\n",
    "                model=model_config[\"model_name\"],\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            return (model_name, role_id, response.choices[0].message.content)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error calling {model_name} for role {role_id}: {e}\")\n",
    "            return (model_name, role_id, \"\")\n",
    "\n",
    "    def generate_tests_from_council(self, function_info: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Generate test cases using role-based assignments (synchronous version)\"\"\"\n",
    "        council_results = {}\n",
    "        \n",
    "        print(\"🤖 Consulting Role-Based LLM Council for test generation...\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Calculate total tasks for progress bar\n",
    "        total_tasks = sum(len(roles) for roles in self.model_role_assignments.values())\n",
    "        \n",
    "        with tqdm(total=total_tasks, desc=\"Generating role-based tests\") as pbar:\n",
    "            for model_name, assigned_roles in self.model_role_assignments.items():\n",
    "                if model_name not in self.models:\n",
    "                    print(f\"⚠️  Warning: Model {model_name} not found in configuration\")\n",
    "                    continue\n",
    "                \n",
    "                model_config = self.models[model_name]\n",
    "                model_results = {}\n",
    "                \n",
    "                for role_id in assigned_roles:\n",
    "                    if role_id not in self.roles:\n",
    "                        print(f\"⚠️  Warning: Role {role_id} not defined\")\n",
    "                        continue\n",
    "                    \n",
    "                    role = self.roles[role_id]\n",
    "                    \n",
    "                    try:\n",
    "                        prompt = self.create_role_based_prompt(function_info, role_id)\n",
    "                        \n",
    "                        if model_config[\"type\"] == \"openai\":\n",
    "                            response = self.call_openai_model(prompt, model_config)\n",
    "                        else:\n",
    "                            response = \"\"\n",
    "                        \n",
    "                        test_methods = code_analyzer.extract_test_methods_from_response(response)\n",
    "                        \n",
    "                        model_results[role_id] = {\n",
    "                            'role_name': role['name'],\n",
    "                            'raw_response': response,\n",
    "                            'test_methods': test_methods,\n",
    "                            'test_count': len(test_methods),\n",
    "                            'focus_categories': role['focus_categories']\n",
    "                        }\n",
    "                        \n",
    "                        print(f\"✅ {model_name} as '{role['name']}': {len(test_methods)} tests\")\n",
    "                        pbar.update(1)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"❌ Error with {model_name} in role {role_id}: {e}\")\n",
    "                        model_results[role_id] = {\n",
    "                            'role_name': role['name'],\n",
    "                            'raw_response': \"\",\n",
    "                            'test_methods': [],\n",
    "                            'test_count': 0,\n",
    "                            'focus_categories': role['focus_categories']\n",
    "                        }\n",
    "                        pbar.update(1)\n",
    "                \n",
    "                council_results[model_name] = model_results\n",
    "        \n",
    "        print(f\"{'='*70}\")\n",
    "        return council_results\n",
    "    \n",
    "    async def generate_tests_from_council_async(self, function_info: Dict[str, Any], \n",
    "                                                 max_concurrent: int = 7) -> Dict[str, Any]:\n",
    "        \"\"\"Generate test cases using role-based assignments with concurrent API calls\"\"\"\n",
    "        print(\"🤖 Consulting Role-Based LLM Council for test generation (Concurrent Mode)...\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"⚡ Maximum concurrent requests: {max_concurrent}\")\n",
    "        \n",
    "        # Create a semaphore to limit concurrent requests\n",
    "        semaphore = asyncio.Semaphore(max_concurrent)\n",
    "        \n",
    "        # Prepare all tasks\n",
    "        tasks = []\n",
    "        task_metadata = []  # To track which task belongs to which model-role pair\n",
    "        \n",
    "        for model_name, assigned_roles in self.model_role_assignments.items():\n",
    "            if model_name not in self.models:\n",
    "                print(f\"⚠️  Warning: Model {model_name} not found in configuration\")\n",
    "                continue\n",
    "            \n",
    "            model_config = self.models[model_name]\n",
    "            \n",
    "            for role_id in assigned_roles:\n",
    "                if role_id not in self.roles:\n",
    "                    print(f\"⚠️  Warning: Role {role_id} not defined\")\n",
    "                    continue\n",
    "                \n",
    "                role = self.roles[role_id]\n",
    "                \n",
    "                # Create prompt\n",
    "                prompt = self.create_role_based_prompt(function_info, role_id)\n",
    "                \n",
    "                # Create async task with semaphore\n",
    "                async def bounded_call(sem, p, mc, mn, rid):\n",
    "                    async with sem:\n",
    "                        return await self.call_openai_model_async(p, mc, mn, rid)\n",
    "                \n",
    "                task = bounded_call(semaphore, prompt, model_config, model_name, role_id)\n",
    "                tasks.append(task)\n",
    "                task_metadata.append({\n",
    "                    'model_name': model_name,\n",
    "                    'role_id': role_id,\n",
    "                    'role_name': role['name'],\n",
    "                    'focus_categories': role['focus_categories']\n",
    "                })\n",
    "        \n",
    "        total_tasks = len(tasks)\n",
    "        print(f\"📊 Total API calls to make: {total_tasks}\")\n",
    "        \n",
    "        # Execute all tasks concurrently with progress tracking\n",
    "        results = []\n",
    "        with tqdm(total=total_tasks, desc=\"Concurrent API calls\") as pbar:\n",
    "            # Use asyncio.gather to run all tasks\n",
    "            for coro in asyncio.as_completed(tasks):\n",
    "                result = await coro\n",
    "                results.append(result)\n",
    "                pbar.update(1)\n",
    "        \n",
    "        # Organize results back into the expected structure\n",
    "        council_results = {}\n",
    "        \n",
    "        for (model_name, role_id, response), metadata in zip(results, task_metadata):\n",
    "            # Ensure model_name matches metadata (it should)\n",
    "            if model_name not in council_results:\n",
    "                council_results[model_name] = {}\n",
    "            \n",
    "            # Extract test methods from response\n",
    "            test_methods = code_analyzer.extract_test_methods_from_response(response)\n",
    "            \n",
    "            council_results[model_name][role_id] = {\n",
    "                'role_name': metadata['role_name'],\n",
    "                'raw_response': response,\n",
    "                'test_methods': test_methods,\n",
    "                'test_count': len(test_methods),\n",
    "                'focus_categories': metadata['focus_categories']\n",
    "            }\n",
    "            \n",
    "            print(f\"✅ {model_name} as '{metadata['role_name']}': {len(test_methods)} tests\")\n",
    "        \n",
    "        print(f\"{'='*70}\")\n",
    "        return council_results\n",
    "\n",
    "# Initialize LLM Council\n",
    "llm_council = LLMCouncil(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59b58fba-2980-4725-b916-9a9cefcaffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Test Classification Module (Enhanced for Role Tracking)\n",
    "class TestClassifier:\n",
    "    \"\"\"Classifies test cases by category and tracks role assignments\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_category_from_test(test_code: str) -> str:\n",
    "        \"\"\"Extract category from test code comments\"\"\"\n",
    "        category_pattern = r'#\\s*Category:\\s*(\\w+)'\n",
    "        match = re.search(category_pattern, test_code, re.IGNORECASE)\n",
    "        \n",
    "        if match:\n",
    "            category = match.group(1).lower()\n",
    "            if category in config.TEST_CATEGORIES:\n",
    "                return category\n",
    "        \n",
    "        # Fallback classification based on test name and content\n",
    "        test_code_lower = test_code.lower()\n",
    "        \n",
    "        if 'error' in test_code_lower or 'exception' in test_code_lower or 'invalid' in test_code_lower:\n",
    "            return 'negative'\n",
    "        elif 'boundary' in test_code_lower or 'edge' in test_code_lower or 'limit' in test_code_lower:\n",
    "            return 'boundary'\n",
    "        elif 'security' in test_code_lower or 'auth' in test_code_lower or 'injection' in test_code_lower:\n",
    "            return 'security'\n",
    "        else:\n",
    "            return 'positive'\n",
    "    \n",
    "    @staticmethod\n",
    "    def classify_council_results(council_results: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Classify all test cases from council results with role information\"\"\"\n",
    "        all_classified_tests = []\n",
    "        \n",
    "        for model_name, role_results in council_results.items():\n",
    "            for role_id, results in role_results.items():\n",
    "                for test in results['test_methods']:\n",
    "                    category = TestClassifier.extract_category_from_test(test['code'])\n",
    "                    classified_test = test.copy()\n",
    "                    classified_test['category'] = category\n",
    "                    classified_test['source_model'] = model_name\n",
    "                    classified_test['source_role'] = role_id\n",
    "                    classified_test['role_name'] = results['role_name']\n",
    "                    all_classified_tests.append(classified_test)\n",
    "        \n",
    "        return all_classified_tests\n",
    "\n",
    "# Initialize classifier\n",
    "test_classifier = TestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78b32601-5eda-4908-a881-0aec8924b5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Enhanced Test Synthesizer Module (UPDATED)\n",
    "class TestSynthesizer:\n",
    "    \"\"\"Synthesizes final optimized test file with intelligent duplicate removal\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_council: LLMCouncil):\n",
    "        self.llm_council = llm_council\n",
    "    \n",
    "    def create_synthesis_prompt(self, all_tests: List[Dict], function_info: Dict) -> str:\n",
    "        \"\"\"Create prompt for synthesizing final test file with duplicate removal\"\"\"\n",
    "        func = function_info['functions'][0] if function_info['functions'] else {}\n",
    "        \n",
    "        # Group tests by model for better presentation\n",
    "        tests_by_model = {}\n",
    "        for test in all_tests:\n",
    "            model = test['source_model']\n",
    "            if model not in tests_by_model:\n",
    "                tests_by_model[model] = []\n",
    "            tests_by_model[model].append(test)\n",
    "        \n",
    "        # Create formatted test presentation\n",
    "        test_presentation = \"\"\n",
    "        for model, tests in tests_by_model.items():\n",
    "            test_presentation += f\"\\n--- Tests from {model} ---\\n\"\n",
    "            for i, test in enumerate(tests, 1):\n",
    "                test_presentation += f\"\\nTest {i} (Category: {test['category']}):\\n\"\n",
    "                test_presentation += f\"```python\\n{test['code']}\\n```\\n\"\n",
    "        \n",
    "        function_name = func.get('name', 'unknown_function')\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "You are an expert Python test engineer responsible for creating the final, optimized test suite.\n",
    "\n",
    "ORIGINAL FUNCTION TO TEST:\n",
    "```python\n",
    "{func.get('source_code', function_info['source_code'])}\n",
    "```\n",
    "\n",
    "RAW TESTS FROM MULTIPLE AI MODELS:\n",
    "{test_presentation}\n",
    "\n",
    "YOUR TASK:\n",
    "1. **DO NOT INCLUDE THE SOURCE FUNCTION**: The function will be saved separately in function.py and imported.\n",
    "\n",
    "2. **USE IMPORTS**: Start your test file with: from function import {function_name}\n",
    "\n",
    "3. **ANALYZE ALL TESTS**: Carefully examine each test case for:\n",
    "   - Functionality being tested\n",
    "   - Input/output scenarios\n",
    "   - Edge cases covered\n",
    "   - Error conditions tested\n",
    "\n",
    "4. **REMOVE DUPLICATES**: Identify and eliminate tests that are functionally equivalent, even if they have different:\n",
    "   - Variable names\n",
    "   - Assertion styles\n",
    "   - Code structure\n",
    "   - Comments\n",
    "\n",
    "5. **SELECT BEST REPRESENTATIVES**: When multiple tests cover the same scenario:\n",
    "   - Choose the most comprehensive version\n",
    "   - Prefer tests with better error messages\n",
    "   - Keep tests with clearer documentation\n",
    "\n",
    "6. **ENSURE COMPREHENSIVE COVERAGE**: Make sure the final suite covers:\n",
    "   - All major functionality paths\n",
    "   - Various input types and ranges\n",
    "   - Error conditions and edge cases\n",
    "   - All important test categories (positive, negative, boundary, edge_case, security)\n",
    "\n",
    "7. **CREATE CLEAN, PRODUCTION-READY CODE**: Generate a single, well-organized test file with:\n",
    "   - Proper imports (pytest, and function import)\n",
    "   - Clear test organization by category\n",
    "   - Descriptive test names and docstrings\n",
    "   - Comprehensive comments\n",
    "   - Consistent code style\n",
    "\n",
    "8. **OPTIMIZE FOR QUALITY**: Prioritize test quality over quantity. Include only meaningful, non-redundant tests.\n",
    "\n",
    "REQUIREMENTS:\n",
    "- Start with: import pytest\n",
    "- Then add: from function import {function_name}\n",
    "- Use pytest format for all tests\n",
    "- Group tests logically by category\n",
    "- Ensure each test has a clear purpose\n",
    "- Make test names descriptive and consistent\n",
    "- DO NOT include the original function code\n",
    "- DO NOT use markdown code fences (```python) in the output - provide clean Python code only\n",
    "\n",
    "EXAMPLE OUTPUT FORMAT:\n",
    "\n",
    "import pytest\n",
    "from function import {function_name}\n",
    "\n",
    "def test_{function_name}_positive_case():\n",
    "    '''Test normal functionality with valid inputs'''\n",
    "    # Category: positive\n",
    "    result = {function_name}(valid_input)\n",
    "    assert result == expected_output, \"Should handle normal case correctly\"\n",
    "\n",
    "def test_{function_name}_boundary_case():\n",
    "    '''Test boundary conditions'''\n",
    "    # Category: boundary\n",
    "    # Test implementation here\n",
    "\n",
    "Generate the complete, final test file that represents the best, non-duplicate tests from all models:\n",
    "\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def synthesize_final_test_file(self, all_tests: List[Dict], function_info: Dict) -> Dict[str, Any]:\n",
    "        \"\"\"Synthesize the final optimized test file with duplicate removal\"\"\"\n",
    "        print(\"🔄 Synthesizing final test file with intelligent duplicate removal...\")\n",
    "        \n",
    "        prompt = self.create_synthesis_prompt(all_tests, function_info)\n",
    "        \n",
    "        # Use the best available model for synthesis\n",
    "        best_model = SYNTHESIZER_MODEL if SYNTHESIZER_MODEL in self.llm_council.models else list(self.llm_council.models.keys())[0]\n",
    "        model_config = self.llm_council.models[best_model]\n",
    "        \n",
    "        try:\n",
    "            if model_config[\"type\"] == \"openai\":\n",
    "                synthesized_content = self.llm_council.call_openai_model(prompt, model_config)\n",
    "                \n",
    "                # Clean the synthesized content immediately\n",
    "                cleaned_content = self._clean_synthesized_content(synthesized_content)\n",
    "                \n",
    "                # Extract final tests from synthesized content\n",
    "                final_tests = self._extract_final_tests_from_synthesis(cleaned_content, all_tests)\n",
    "                \n",
    "                print(f\"✅ Final test file synthesized successfully!\")\n",
    "                print(f\"📊 Reduced {len(all_tests)} original tests to {len(final_tests)} unique tests\")\n",
    "                print(f\"📉 Reduction ratio: {((len(all_tests) - len(final_tests)) / len(all_tests)) * 100:.1f}%\")\n",
    "                \n",
    "                return {\n",
    "                    'synthesized_content': cleaned_content,\n",
    "                    'final_tests': final_tests,\n",
    "                    'original_count': len(all_tests),\n",
    "                    'final_count': len(final_tests),\n",
    "                    'reduction_ratio': (len(all_tests) - len(final_tests)) / len(all_tests) if len(all_tests) > 0 else 0,\n",
    "                    'synthesizer_model': best_model\n",
    "                }\n",
    "            else:\n",
    "                return self._fallback_synthesis(all_tests, function_info)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in synthesis, using fallback: {e}\")\n",
    "            return self._fallback_synthesis(all_tests, function_info)\n",
    "    \n",
    "    def _clean_synthesized_content(self, content: str) -> str:\n",
    "        \"\"\"Clean synthesized content by removing markdown artifacts\"\"\"\n",
    "        lines = content.split('\\n')\n",
    "        cleaned_lines = []\n",
    "        \n",
    "        for line in lines:\n",
    "            stripped_line = line.strip()\n",
    "            \n",
    "            # Skip markdown code fence lines\n",
    "            if stripped_line in ['```python', '```py', '```', '```\\n']:\n",
    "                continue\n",
    "            \n",
    "            # Remove leading ```python or ```py from lines that start with it\n",
    "            if stripped_line.startswith('```python'):\n",
    "                line = line.replace('```python', '', 1)\n",
    "            elif stripped_line.startswith('```py'):\n",
    "                line = line.replace('```py', '', 1)\n",
    "            elif stripped_line.startswith('```') and stripped_line.endswith('```'):\n",
    "                # Skip lines that are just code fences\n",
    "                continue\n",
    "            \n",
    "            cleaned_lines.append(line)\n",
    "        \n",
    "        # Join back and clean up any extra newlines at start/end\n",
    "        cleaned_code = '\\n'.join(cleaned_lines).strip()\n",
    "        \n",
    "        # Remove any remaining triple backticks that might be at the end\n",
    "        while cleaned_code.endswith('```'):\n",
    "            cleaned_code = cleaned_code[:-3].strip()\n",
    "        \n",
    "        return cleaned_code\n",
    "    \n",
    "    def _extract_final_tests_from_synthesis(self, synthesized_content: str, original_tests: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Extract final test information from synthesized content\"\"\"\n",
    "        final_tests = []\n",
    "        \n",
    "        # Extract test methods from synthesized content\n",
    "        test_methods = code_analyzer.extract_test_methods_from_response(synthesized_content)\n",
    "        \n",
    "        for test_method in test_methods:\n",
    "            # Classify the synthesized test\n",
    "            category = test_classifier.extract_category_from_test(test_method['code'])\n",
    "            \n",
    "            final_test = {\n",
    "                'name': test_method['name'],\n",
    "                'code': test_method['code'],\n",
    "                'category': category,\n",
    "                'source': 'synthesized',\n",
    "                'original_sources': self._find_original_sources(test_method, original_tests)\n",
    "            }\n",
    "            final_tests.append(final_test)\n",
    "        \n",
    "        return final_tests\n",
    "    \n",
    "    def _find_original_sources(self, synthesized_test: Dict, original_tests: List[Dict]) -> List[str]:\n",
    "        \"\"\"Find which original tests likely contributed to the synthesized test\"\"\"\n",
    "        sources = []\n",
    "        synthesized_name = synthesized_test['name'].lower()\n",
    "        \n",
    "        for original in original_tests:\n",
    "            original_name = original['name'].lower()\n",
    "            # Simple heuristic: if names are similar or test same functionality\n",
    "            if (synthesized_name in original_name or original_name in synthesized_name or\n",
    "                any(word in original_name for word in synthesized_name.split('_') if len(word) > 3)):\n",
    "                sources.append(original['source_model'])\n",
    "        \n",
    "        return list(set(sources))  # Remove duplicates\n",
    "    \n",
    "    def _fallback_synthesis(self, all_tests: List[Dict], function_info: Dict) -> Dict[str, Any]:\n",
    "        \"\"\"Fallback synthesis method using simple deduplication\"\"\"\n",
    "        func = function_info['functions'][0] if function_info['functions'] else {}\n",
    "        function_name = func.get('name', 'unknown_function')\n",
    "        \n",
    "        # Simple deduplication based on test names and categories\n",
    "        seen_tests = set()\n",
    "        unique_tests = []\n",
    "        \n",
    "        for test in all_tests:\n",
    "            test_signature = (test['name'].lower(), test['category'])\n",
    "            if test_signature not in seen_tests:\n",
    "                seen_tests.add(test_signature)\n",
    "                unique_tests.append(test)\n",
    "        \n",
    "        header = f'''\"\"\"\n",
    "Comprehensive Test Suite\n",
    "Generated by Intelligent LLM Council\n",
    "Target Function: {function_name}\n",
    "Total Unique Tests: {len(unique_tests)}\n",
    "Original Tests: {len(all_tests)}\n",
    "Reduction Ratio: {((len(all_tests) - len(unique_tests)) / len(all_tests)) * 100:.1f}%\n",
    "\"\"\"\n",
    "\n",
    "import pytest\n",
    "from function import {function_name}\n",
    "\n",
    "'''\n",
    "        \n",
    "        # Group tests by category\n",
    "        by_category = {}\n",
    "        for test in unique_tests:\n",
    "            category = test['category']\n",
    "            if category not in by_category:\n",
    "                by_category[category] = []\n",
    "            by_category[category].append(test)\n",
    "        \n",
    "        # Generate organized test code\n",
    "        test_code = header\n",
    "        \n",
    "        for category, tests in by_category.items():\n",
    "            test_code += f\"\\n\\n# {category.upper()} TESTS\\n\"\n",
    "            test_code += f\"# {'='*50}\\n\\n\"\n",
    "            \n",
    "            for test in tests:\n",
    "                # Remove any function definition from test code since we're importing\n",
    "                test_lines = test['code'].split('\\n')\n",
    "                cleaned_test_lines = []\n",
    "                for line in test_lines:\n",
    "                    if line.strip().startswith('def ') and not line.strip().startswith('def test_'):\n",
    "                        continue  # Skip function definitions that aren't tests\n",
    "                    cleaned_test_lines.append(line)\n",
    "                \n",
    "                test_code += '\\n'.join(cleaned_test_lines) + \"\\n\\n\"\n",
    "        \n",
    "        return {\n",
    "            'synthesized_content': test_code,\n",
    "            'final_tests': unique_tests,\n",
    "            'original_count': len(all_tests),\n",
    "            'final_count': len(unique_tests),\n",
    "            'reduction_ratio': (len(all_tests) - len(unique_tests)) / len(all_tests) if len(all_tests) > 0 else 0,\n",
    "            'synthesizer_model': 'fallback'\n",
    "        }\n",
    "\n",
    "# Initialize synthesizer\n",
    "test_synthesizer = TestSynthesizer(llm_council)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc3dfb4a-56d8-438f-88e0-f6f26c8a6701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Coverage Analyzer\n",
    "import subprocess\n",
    "import re\n",
    "import os\n",
    "from typing import Dict, Any\n",
    "\n",
    "class CoverageAnalyzer:\n",
    "    \"\"\"Analyzes code coverage for generated test files\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def _clean_terminal_output(output: str) -> str:\n",
    "        \"\"\"Remove terminal formatting and escape sequences\"\"\"\n",
    "        # Remove ANSI escape sequences\n",
    "        ansi_escape = re.compile(r'\\x1B(?:[@-Z\\\\-_]|\\[[0-?]*[ -/]*[@-~])')\n",
    "        cleaned = ansi_escape.sub('', output)\n",
    "        \n",
    "        # Remove carriage returns and normalize line endings\n",
    "        cleaned = cleaned.replace('\\r\\n', '\\n').replace('\\r', '\\n')\n",
    "        \n",
    "        # Remove any remaining control characters\n",
    "        cleaned = re.sub(r'[\\x00-\\x1f\\x7f-\\x9f]', '', cleaned)\n",
    "        \n",
    "        return cleaned\n",
    "    \n",
    "    @staticmethod\n",
    "    def _extract_function_coverage_percentage(stdout: str) -> float:\n",
    "        \"\"\"Extract coverage percentage specifically for function.py from pytest output\"\"\"\n",
    "        try:\n",
    "            # Clean terminal formatting first\n",
    "            cleaned_stdout = CoverageAnalyzer._clean_terminal_output(stdout)\n",
    "            \n",
    "            # More robust regex pattern to find function.py coverage line\n",
    "            # Pattern matches: function.py followed by whitespace, numbers, whitespace, percentage\n",
    "            pattern = r'function\\.py\\s+\\d+\\s+\\d+\\s+(\\d+)%'\n",
    "            match = re.search(pattern, cleaned_stdout)\n",
    "            \n",
    "            if match:\n",
    "                percentage = float(match.group(1))\n",
    "                print(f\"Debug: Found function.py coverage: {percentage}%\")\n",
    "                return percentage\n",
    "            \n",
    "            # Fallback: Look for function.py line manually\n",
    "            lines = cleaned_stdout.split('\\n')\n",
    "            for line in lines:\n",
    "                if 'function.py' in line and '%' in line:\n",
    "                    print(f\"Debug: Processing line: {repr(line)}\")\n",
    "                    \n",
    "                    # Use regex to extract percentage from this specific line\n",
    "                    percent_match = re.search(r'(\\d+)%', line)\n",
    "                    if percent_match:\n",
    "                        percentage = float(percent_match.group(1))\n",
    "                        print(f\"Debug: Extracted percentage: {percentage}%\")\n",
    "                        return percentage\n",
    "                    \n",
    "                    # Alternative parsing if regex fails\n",
    "                    parts = line.split()\n",
    "                    print(f\"Debug: Line parts: {parts}\")\n",
    "                    for part in parts:\n",
    "                        if part.endswith('%'):\n",
    "                            try:\n",
    "                                percentage_str = part.rstrip('%')\n",
    "                                percentage = float(percentage_str)\n",
    "                                print(f\"Debug: Parsed percentage from part '{part}': {percentage}%\")\n",
    "                                return percentage\n",
    "                            except ValueError:\n",
    "                                continue\n",
    "            \n",
    "            # If function.py not found specifically, return 0\n",
    "            print(\"Warning: function.py coverage not found in output\")\n",
    "            print(f\"Debug: Cleaned stdout:\\n{cleaned_stdout}\")\n",
    "            return 0.0\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not extract function.py coverage percentage: {e}\")\n",
    "            print(f\"Debug: Raw stdout:\\n{stdout}\")\n",
    "            return 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def _extract_test_execution_stats(stdout: str) -> Dict[str, int]:\n",
    "        \"\"\"Extract test execution statistics from pytest output\"\"\"\n",
    "        try:\n",
    "            cleaned_stdout = CoverageAnalyzer._clean_terminal_output(stdout)\n",
    "            \n",
    "            stats = {\n",
    "                'total_tests': 0,\n",
    "                'passed_tests': 0,\n",
    "                'failed_tests': 0,\n",
    "                'skipped_tests': 0,\n",
    "                'error_tests': 0\n",
    "            }\n",
    "            \n",
    "            # Look for test collection line: \"collected X items\"\n",
    "            collection_match = re.search(r'collected (\\d+) items?', cleaned_stdout)\n",
    "            if collection_match:\n",
    "                stats['total_tests'] = int(collection_match.group(1))\n",
    "                print(f\"Debug: Found total tests: {stats['total_tests']}\")\n",
    "            \n",
    "            # Look for final result line: \"X passed, Y failed, Z skipped in N.NNs\"\n",
    "            # Various patterns to catch different pytest output formats\n",
    "            result_patterns = [\n",
    "                r'(\\d+) passed.*?in \\d+\\.\\d+s',  # X passed in N.NNs\n",
    "                r'(\\d+) passed, (\\d+) failed.*?in \\d+\\.\\d+s',  # X passed, Y failed in N.NNs\n",
    "                r'(\\d+) passed, (\\d+) skipped.*?in \\d+\\.\\d+s',  # X passed, Y skipped in N.NNs\n",
    "                r'(\\d+) failed.*?in \\d+\\.\\d+s',  # X failed in N.NNs\n",
    "                r'(\\d+) skipped.*?in \\d+\\.\\d+s',  # X skipped in N.NNs\n",
    "            ]\n",
    "            \n",
    "            # Try the comprehensive pattern first\n",
    "            comprehensive_pattern = r'(?:(\\d+) failed.*?)?(?:(\\d+) passed.*?)?(?:(\\d+) skipped.*?)?(?:(\\d+) error.*?)?in \\d+\\.\\d+s'\n",
    "            match = re.search(comprehensive_pattern, cleaned_stdout)\n",
    "            \n",
    "            if match:\n",
    "                failed, passed, skipped, errors = match.groups()\n",
    "                if passed:\n",
    "                    stats['passed_tests'] = int(passed)\n",
    "                if failed:\n",
    "                    stats['failed_tests'] = int(failed)\n",
    "                if skipped:\n",
    "                    stats['skipped_tests'] = int(skipped)\n",
    "                if errors:\n",
    "                    stats['error_tests'] = int(errors)\n",
    "                    \n",
    "                print(f\"Debug: Extracted test stats - Passed: {stats['passed_tests']}, Failed: {stats['failed_tests']}, Skipped: {stats['skipped_tests']}\")\n",
    "            else:\n",
    "                # Fallback: look for simpler patterns\n",
    "                passed_match = re.search(r'(\\d+) passed', cleaned_stdout)\n",
    "                if passed_match:\n",
    "                    stats['passed_tests'] = int(passed_match.group(1))\n",
    "                    \n",
    "                failed_match = re.search(r'(\\d+) failed', cleaned_stdout)\n",
    "                if failed_match:\n",
    "                    stats['failed_tests'] = int(failed_match.group(1))\n",
    "                    \n",
    "                skipped_match = re.search(r'(\\d+) skipped', cleaned_stdout)\n",
    "                if skipped_match:\n",
    "                    stats['skipped_tests'] = int(skipped_match.group(1))\n",
    "                    \n",
    "                print(f\"Debug: Fallback extraction - Passed: {stats['passed_tests']}, Failed: {stats['failed_tests']}\")\n",
    "            \n",
    "            # If we couldn't get total from collection, calculate from individual counts\n",
    "            if stats['total_tests'] == 0:\n",
    "                stats['total_tests'] = stats['passed_tests'] + stats['failed_tests'] + stats['skipped_tests'] + stats['error_tests']\n",
    "            \n",
    "            return stats\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not extract test execution stats: {e}\")\n",
    "            return {\n",
    "                'total_tests': 0,\n",
    "                'passed_tests': 0,\n",
    "                'failed_tests': 0,\n",
    "                'skipped_tests': 0,\n",
    "                'error_tests': 0\n",
    "            }\n",
    "    \n",
    "    @staticmethod\n",
    "    def _clean_test_code(code: str) -> str:\n",
    "        \"\"\"Clean test code by removing markdown formatting and extra whitespace\"\"\"\n",
    "        # Remove markdown code block markers\n",
    "        code = re.sub(r'```python\\s*\\n?', '', code)\n",
    "        code = re.sub(r'```\\s*$', '', code)\n",
    "        \n",
    "        # Remove leading/trailing whitespace from each line and rejoin\n",
    "        lines = [line.rstrip() for line in code.split('\\n')]\n",
    "        cleaned_code = '\\n'.join(lines).strip()\n",
    "        \n",
    "        return cleaned_code\n",
    "    \n",
    "    @staticmethod\n",
    "    def analyze_coverage(function_code: str, test_code: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Analyze code coverage by running the test against the function\n",
    "        \n",
    "        Returns:\n",
    "            Dict containing coverage information and analysis\n",
    "        \"\"\"\n",
    "        import tempfile\n",
    "        import shutil\n",
    "        \n",
    "        coverage_info = {\n",
    "            'coverage_percentage': 0.0,\n",
    "            'test_passed': False,\n",
    "            'error_message': None,\n",
    "            'stdout': '',\n",
    "            'stderr': '',\n",
    "            'total_tests': 0,\n",
    "            'passed_tests': 0,\n",
    "            'failed_tests': 0,\n",
    "            'skipped_tests': 0,\n",
    "            'error_tests': 0,\n",
    "            'success_rate': 0.0\n",
    "        }\n",
    "        \n",
    "        # Create temporary directory\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            try:\n",
    "                # Write function code to file\n",
    "                function_file = os.path.join(temp_dir, \"function.py\")\n",
    "                with open(function_file, 'w') as f:\n",
    "                    f.write(function_code)\n",
    "                \n",
    "                # Clean and write test code to file\n",
    "                cleaned_test_code = CoverageAnalyzer._clean_test_code(test_code)\n",
    "                test_file = os.path.join(temp_dir, \"test_generated.py\")\n",
    "                with open(test_file, 'w') as f:\n",
    "                    f.write(cleaned_test_code)\n",
    "                \n",
    "                # Change to temp directory\n",
    "                original_cwd = os.getcwd()\n",
    "                os.chdir(temp_dir)\n",
    "                \n",
    "                try:\n",
    "                    # Run pytest with coverage\n",
    "                    cmd = [\"python\", \"-m\", \"pytest\", \"test_generated.py\", \"--cov=.\", \"--cov-report=term-missing\", \"-v\"]\n",
    "                    \n",
    "                    result = subprocess.run(\n",
    "                        cmd,\n",
    "                        capture_output=True,\n",
    "                        text=True,\n",
    "                        timeout=30\n",
    "                    )\n",
    "                    \n",
    "                    coverage_info['stdout'] = result.stdout\n",
    "                    coverage_info['stderr'] = result.stderr\n",
    "                    coverage_info['test_passed'] = result.returncode == 0\n",
    "                    \n",
    "                    # Extract coverage percentage for function.py\n",
    "                    coverage_percentage = CoverageAnalyzer._extract_function_coverage_percentage(result.stdout)\n",
    "                    coverage_info['coverage_percentage'] = coverage_percentage\n",
    "                    \n",
    "                    # Extract test execution statistics\n",
    "                    test_stats = CoverageAnalyzer._extract_test_execution_stats(result.stdout)\n",
    "                    coverage_info.update(test_stats)\n",
    "                    \n",
    "                    # Calculate success rate\n",
    "                    if coverage_info['total_tests'] > 0:\n",
    "                        coverage_info['success_rate'] = (coverage_info['passed_tests'] / coverage_info['total_tests']) * 100.0\n",
    "                    else:\n",
    "                        coverage_info['success_rate'] = 0.0\n",
    "                    \n",
    "                    print(f\"Coverage analysis complete. Function.py coverage: {coverage_percentage}%\")\n",
    "                    print(f\"Test results: {coverage_info['passed_tests']}/{coverage_info['total_tests']} passed ({coverage_info['success_rate']:.1f}%)\")\n",
    "                    \n",
    "                finally:\n",
    "                    # Restore original working directory\n",
    "                    os.chdir(original_cwd)\n",
    "                    \n",
    "            except subprocess.TimeoutExpired:\n",
    "                coverage_info['error_message'] = \"Test execution timed out\"\n",
    "                print(\"Error: Test execution timed out\")\n",
    "            except Exception as e:\n",
    "                coverage_info['error_message'] = str(e)\n",
    "                print(f\"Error during coverage analysis: {e}\")\n",
    "        \n",
    "        return coverage_info\n",
    "\n",
    "\n",
    "# Initialize coverage analyzer\n",
    "coverage_analyzer = CoverageAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "697940aa-34b1-4e14-b1a8-b35dcd063654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Main Pipeline Orchestrator (Role-Based Version)\n",
    "class IntelligentTestCouncil:\n",
    "    \"\"\"Main orchestrator for the intelligent role-based test generation pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.code_analyzer = CodeAnalyzer()\n",
    "        self.llm_council = LLMCouncil(config)\n",
    "        self.test_classifier = TestClassifier()\n",
    "        self.test_synthesizer = TestSynthesizer(self.llm_council)\n",
    "        self.coverage_analyzer = CoverageAnalyzer()\n",
    "        \n",
    "    def generate_comprehensive_tests(self, function_code: str) -> Dict[str, Any]:\n",
    "        \"\"\"Main pipeline for generating comprehensive test suite with role-based generation\"\"\"\n",
    "        print(\"🚀 Starting Role-Based Intelligent Test Council Pipeline\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Step 1: Analyze the input function\n",
    "        print(\"\\n📝 Step 1: Analyzing input function...\")\n",
    "        function_info = self.code_analyzer.extract_function_info(function_code)\n",
    "        \n",
    "        if not function_info['functions']:\n",
    "            error_msg = 'No functions found in the provided code'\n",
    "            if 'syntax_error' in function_info:\n",
    "                error_msg += f\". Syntax error: {function_info['syntax_error']}\"\n",
    "            return {'error': error_msg}\n",
    "        \n",
    "        print(f\"✅ Found {function_info['total_functions']} function(s)\")\n",
    "        \n",
    "        # Step 2: Generate tests using role-based LLM council\n",
    "        print(\"\\n🎭 Step 2: Consulting Role-Based LLM Council...\")\n",
    "        council_results = self.llm_council.generate_tests_from_council(function_info)\n",
    "        \n",
    "        # Step 3: Classify all test cases\n",
    "        print(\"\\n🏷️  Step 3: Classifying test cases by category and role...\")\n",
    "        all_classified_tests = self.test_classifier.classify_council_results(council_results)\n",
    "        \n",
    "        print(f\"✅ Total tests generated: {len(all_classified_tests)}\")\n",
    "        \n",
    "        # Display role distribution\n",
    "        role_counts = Counter(test['role_name'] for test in all_classified_tests)\n",
    "        print(\"\\n🎭 Role distribution:\")\n",
    "        for role_name, count in role_counts.items():\n",
    "            print(f\"   • {role_name}: {count} tests\")\n",
    "        \n",
    "        # Display category distribution\n",
    "        category_counts = Counter(test['category'] for test in all_classified_tests)\n",
    "        print(\"\\n📊 Category distribution:\")\n",
    "        for category, count in category_counts.items():\n",
    "            print(f\"   • {category}: {count} tests\")\n",
    "        \n",
    "        # Display model-role performance matrix\n",
    "        print(\"\\n🔬 Model-Role Performance Matrix:\")\n",
    "        for model_name, role_results in council_results.items():\n",
    "            print(f\"\\n   {model_name}:\")\n",
    "            for role_id, results in role_results.items():\n",
    "                print(f\"      └─ {results['role_name']}: {results['test_count']} tests\")\n",
    "        \n",
    "        # Step 4: Synthesize final test file with intelligent duplicate removal\n",
    "        print(f\"\\n🔄 Step 4: Synthesizing final test file with duplicate removal...\")\n",
    "        synthesis_results = self.test_synthesizer.synthesize_final_test_file(all_classified_tests, function_info)\n",
    "        \n",
    "        # Step 5: Analyze coverage\n",
    "        print(\"\\n📊 Step 5: Analyzing code coverage...\")\n",
    "        coverage_results = self.coverage_analyzer.analyze_coverage(\n",
    "            function_code, \n",
    "            synthesis_results['synthesized_content']\n",
    "        )\n",
    "        \n",
    "        # Prepare comprehensive results with role-based metrics\n",
    "        results = {\n",
    "            'function_info': function_info,\n",
    "            'council_results': council_results,\n",
    "            'all_classified_tests': all_classified_tests,\n",
    "            'synthesis_results': synthesis_results,\n",
    "            'final_test_file': synthesis_results['synthesized_content'],\n",
    "            'coverage_results': coverage_results,\n",
    "            'statistics': {\n",
    "                'original_test_count': len(all_classified_tests),\n",
    "                'final_test_count': synthesis_results['final_count'],\n",
    "                'reduction_ratio': synthesis_results['reduction_ratio'],\n",
    "                'coverage_percentage': coverage_results.get('coverage_percentage', 0.0),\n",
    "                'test_success_rate': coverage_results.get('success_rate', 0.0),\n",
    "                'total_tests_run': coverage_results.get('total_tests', 0),\n",
    "                'passed_tests': coverage_results.get('passed_tests', 0),\n",
    "                'failed_tests': coverage_results.get('failed_tests', 0),\n",
    "                'skipped_tests': coverage_results.get('skipped_tests', 0),\n",
    "                'error_tests': coverage_results.get('error_tests', 0),\n",
    "                'models_used': list(council_results.keys()),\n",
    "                'roles_used': list(set(test['role_name'] for test in all_classified_tests)),\n",
    "                'categories_found': list(category_counts.keys()),\n",
    "                'synthesizer_model': synthesis_results['synthesizer_model'],\n",
    "                # Role-specific metrics\n",
    "                'tests_per_role': dict(role_counts),\n",
    "                'tests_per_category': dict(category_counts),\n",
    "                'model_role_matrix': {\n",
    "                    model: {role: results['test_count'] for role, results in roles.items()}\n",
    "                    for model, roles in council_results.items()\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(\"\\n🎉 Pipeline completed successfully!\")\n",
    "        print(f\"📊 Test Success Rate: {coverage_results.get('success_rate', 0.0):.1f}%\")\n",
    "        print(f\"📈 Code Coverage: {coverage_results.get('coverage_percentage', 0.0):.1f}%\")\n",
    "        print(f\"✅ Passed Tests: {coverage_results.get('passed_tests', 0)}/{coverage_results.get('total_tests', 0)}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def clean_python_code(self, code_content: str) -> str:\n",
    "        \"\"\"Clean Python code by removing markdown code fences and extra formatting\"\"\"\n",
    "        lines = code_content.split('\\n')\n",
    "        cleaned_lines = []\n",
    "        \n",
    "        for line in lines:\n",
    "            stripped_line = line.strip()\n",
    "            \n",
    "            # Skip markdown code fence lines\n",
    "            if stripped_line in ['```python', '```py', '```']:\n",
    "                continue\n",
    "            \n",
    "            # Remove leading ```python or ```py from lines that start with it\n",
    "            if stripped_line.startswith('```python'):\n",
    "                line = line.replace('```python', '', 1)\n",
    "            elif stripped_line.startswith('```py'):\n",
    "                line = line.replace('```py', '', 1)\n",
    "            elif stripped_line.startswith('```') and stripped_line.endswith('```'):\n",
    "                # Skip lines that are just code fences\n",
    "                continue\n",
    "            \n",
    "            cleaned_lines.append(line)\n",
    "        \n",
    "        # Join back and clean up any extra newlines at start/end\n",
    "        cleaned_code = '\\n'.join(cleaned_lines).strip()\n",
    "        \n",
    "        # Remove any remaining triple backticks that might be at the end\n",
    "        while cleaned_code.endswith('```'):\n",
    "            cleaned_code = cleaned_code[:-3].strip()\n",
    "        \n",
    "        return cleaned_code\n",
    "    \n",
    "    def save_results(self, results: Dict[str, Any], output_dir: str = \"./test_output\"):\n",
    "        \"\"\"Save all results to files with separate function file\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Get the original function code\n",
    "        function_info = results['function_info']\n",
    "        if function_info['functions']:\n",
    "            function_code = function_info['functions'][0]['source_code']\n",
    "        else:\n",
    "            function_code = function_info['source_code']\n",
    "\n",
    "        # Save the function under test to function.py\n",
    "        with open(f\"{output_dir}/function.py\", 'w', encoding='utf-8') as f:\n",
    "            f.write(function_code)\n",
    "\n",
    "        # The test file content should already be cleaned from synthesis\n",
    "        test_file_content = results['final_test_file']\n",
    "\n",
    "        # Additional cleaning just in case\n",
    "        cleaned_test_content = self.clean_python_code(test_file_content)\n",
    "\n",
    "        # Save final test file (cleaned Python code) \n",
    "        with open(f\"{output_dir}/test_generated.py\", 'w', encoding='utf-8') as f:\n",
    "            f.write(cleaned_test_content)\n",
    "\n",
    "        # Save detailed results\n",
    "        with open(f\"{output_dir}/analysis_results.json\", 'w', encoding='utf-8') as f:\n",
    "            # Convert results to JSON-serializable format\n",
    "            json_results = results.copy()\n",
    "            # Remove non-serializable content\n",
    "            if 'synthesis_results' in json_results:\n",
    "                json_results['synthesis_results'] = {\n",
    "                    k: v for k, v in json_results['synthesis_results'].items() \n",
    "                    if k != 'synthesized_content'  # This is already in final_test_file\n",
    "                }\n",
    "            json.dump(json_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        print(f\"\\n📁 Results saved to {output_dir}/\")\n",
    "        print(f\"🔧 Function under test: {output_dir}/function.py\")\n",
    "        print(f\"✅ Clean Python test file: {output_dir}/test_generated.py\")\n",
    "        print(f\"📊 Analysis results: {output_dir}/analysis_results.json\")\n",
    "        \n",
    "        # Print role-based summary\n",
    "        if 'statistics' in results and 'tests_per_role' in results['statistics']:\n",
    "            print(f\"\\n🎭 Role-Based Generation Summary:\")\n",
    "            for role, count in results['statistics']['tests_per_role'].items():\n",
    "                print(f\"   • {role}: {count} tests\")\n",
    "\n",
    "# Initialize the main pipeline\n",
    "intelligent_council = IntelligentTestCouncil(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac018163-e05f-45b8-bc9b-6d59d2d4030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 10: Role Assignment Optimization Experiment\n",
    "# import random\n",
    "# import json\n",
    "# from collections import defaultdict\n",
    "# from datetime import datetime\n",
    "# import pandas as pd\n",
    "# import asyncio\n",
    "# import nest_asyncio\n",
    "\n",
    "# # Enable nested event loops for Jupyter compatibility\n",
    "# nest_asyncio.apply()\n",
    "\n",
    "# class RoleAssignmentExperiment:\n",
    "#     \"\"\"Conducts experiments to determine optimal model-role assignments with concurrent processing\"\"\"\n",
    "    \n",
    "#     def __init__(self, llm_council, code_analyzer, test_classifier):\n",
    "#         self.llm_council = llm_council\n",
    "#         self.code_analyzer = code_analyzer\n",
    "#         self.test_classifier = test_classifier\n",
    "#         self.results = []\n",
    "        \n",
    "#     def load_dataset(self, dataset_path: str) -> List[Dict]:\n",
    "#         \"\"\"Load functions from dataset\"\"\"\n",
    "#         try:\n",
    "#             with open(dataset_path, 'r') as f:\n",
    "#                 data = json.load(f)\n",
    "#             functions = data.get('functions', [])\n",
    "#             print(f\"✅ Loaded {len(functions)} functions from dataset\")\n",
    "#             return functions\n",
    "#         except Exception as e:\n",
    "#             print(f\"❌ Error loading dataset: {e}\")\n",
    "#             return []\n",
    "    \n",
    "#     def sample_functions(self, functions: List[Dict], n: int = 20, seed: int = 42) -> List[Dict]:\n",
    "#         \"\"\"Randomly sample n functions from dataset\"\"\"\n",
    "#         random.seed(seed)\n",
    "#         sampled = random.sample(functions, min(n, len(functions)))\n",
    "#         print(f\"📊 Sampled {len(sampled)} functions for experiment\")\n",
    "#         return sampled\n",
    "    \n",
    "#     async def process_single_function_async(self, func_data: Dict, func_idx: int, total: int) -> Dict[str, Any]:\n",
    "#         \"\"\"Process a single function through the council asynchronously\"\"\"\n",
    "#         print(f\"\\n{'='*80}\")\n",
    "#         print(f\"Processing function {func_idx}/{total}: {func_data.get('name', 'unknown')}\")\n",
    "#         print(f\"Category: {func_data.get('category', 'unknown')}\")\n",
    "#         print(f\"{'='*80}\")\n",
    "        \n",
    "#         try:\n",
    "#             # Extract function info\n",
    "#             function_info = self.code_analyzer.extract_function_info(func_data['source'])\n",
    "            \n",
    "#             if not function_info['functions']:\n",
    "#                 print(f\"⚠️ Could not parse function {func_data.get('name', 'unknown')}\")\n",
    "#                 return None\n",
    "            \n",
    "#             # Generate tests from council using concurrent API calls\n",
    "#             council_results = await self.llm_council.generate_tests_from_council_async(\n",
    "#                 function_info, \n",
    "#                 max_concurrent=7\n",
    "#             )\n",
    "            \n",
    "#             # Classify tests\n",
    "#             classified_tests = self.test_classifier.classify_council_results(council_results)\n",
    "            \n",
    "#             # Aggregate statistics\n",
    "#             stats = self._aggregate_function_stats(func_data, classified_tests)\n",
    "            \n",
    "#             return stats\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"❌ Error processing function {func_data.get('name', 'unknown')}: {e}\")\n",
    "#             import traceback\n",
    "#             traceback.print_exc()\n",
    "#             return None\n",
    "    \n",
    "#     def _aggregate_function_stats(self, func_data: Dict, classified_tests: List[Dict]) -> Dict[str, Any]:\n",
    "#         \"\"\"Aggregate statistics for a single function\"\"\"\n",
    "#         stats = {\n",
    "#             'function_name': func_data.get('name', 'unknown'),\n",
    "#             'function_category': func_data.get('category', 'unknown'),\n",
    "#             'function_file': func_data.get('file', 'unknown'),\n",
    "#             'total_tests_generated': len(classified_tests),\n",
    "#             'model_role_category_matrix': defaultdict(lambda: defaultdict(lambda: defaultdict(int))),\n",
    "#             'model_totals': defaultdict(int),\n",
    "#             'role_totals': defaultdict(int),\n",
    "#             'category_totals': defaultdict(int),\n",
    "#             'tests': classified_tests\n",
    "#         }\n",
    "        \n",
    "#         # Count tests by model, role, and category\n",
    "#         for test in classified_tests:\n",
    "#             model = test['source_model']\n",
    "#             role = test['source_role']\n",
    "#             category = test['category']\n",
    "            \n",
    "#             stats['model_role_category_matrix'][model][role][category] += 1\n",
    "#             stats['model_totals'][model] += 1\n",
    "#             stats['role_totals'][role] += 1\n",
    "#             stats['category_totals'][category] += 1\n",
    "        \n",
    "#         return stats\n",
    "    \n",
    "#     async def run_experiment_async(self, dataset_path: str, n_functions: int = 20, seed: int = 42) -> Dict[str, Any]:\n",
    "#         \"\"\"Run the complete role assignment optimization experiment with concurrent processing\"\"\"\n",
    "#         print(\"🚀 Starting Role Assignment Optimization Experiment (Concurrent Mode)\")\n",
    "#         print(f\"Target: {n_functions} functions\")\n",
    "#         print(f\"Random seed: {seed}\")\n",
    "#         print(f\"Maximum concurrent API requests: 10\")\n",
    "#         print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "#         # Load and sample functions\n",
    "#         all_functions = self.load_dataset(dataset_path)\n",
    "#         if not all_functions:\n",
    "#             return {'error': 'Failed to load dataset'}\n",
    "        \n",
    "#         sampled_functions = self.sample_functions(all_functions, n_functions, seed)\n",
    "        \n",
    "#         # Process each function\n",
    "#         function_results = []\n",
    "#         for idx, func_data in enumerate(sampled_functions, 1):\n",
    "#             result = await self.process_single_function_async(func_data, idx, len(sampled_functions))\n",
    "#             if result:\n",
    "#                 function_results.append(result)\n",
    "#                 self._save_checkpoint(function_results, idx)\n",
    "        \n",
    "#         # Aggregate cross-function statistics\n",
    "#         aggregated_stats = self._aggregate_cross_function_stats(function_results)\n",
    "        \n",
    "#         # Generate recommendations\n",
    "#         recommendations = self._generate_role_recommendations(aggregated_stats)\n",
    "        \n",
    "#         experiment_results = {\n",
    "#             'timestamp': datetime.now().isoformat(),\n",
    "#             'n_functions_processed': len(function_results),\n",
    "#             'n_functions_target': n_functions,\n",
    "#             'seed': seed,\n",
    "#             'max_concurrent_requests': 10,\n",
    "#             'function_results': function_results,\n",
    "#             'aggregated_stats': aggregated_stats,\n",
    "#             'recommendations': recommendations\n",
    "#         }\n",
    "        \n",
    "#         # Save final results\n",
    "#         self._save_experiment_results(experiment_results)\n",
    "        \n",
    "#         # Print summary\n",
    "#         self._print_experiment_summary(experiment_results)\n",
    "        \n",
    "#         return experiment_results\n",
    "    \n",
    "#     def run_experiment(self, dataset_path: str, n_functions: int = 20, seed: int = 42) -> Dict[str, Any]:\n",
    "#         \"\"\"Wrapper to run async experiment from sync context (Jupyter-compatible)\"\"\"\n",
    "#         # Check if there's already a running event loop (Jupyter)\n",
    "#         try:\n",
    "#             loop = asyncio.get_running_loop()\n",
    "#             # We're in Jupyter, use the existing loop\n",
    "#             return loop.run_until_complete(self.run_experiment_async(dataset_path, n_functions, seed))\n",
    "#         except RuntimeError:\n",
    "#             # No running loop, create a new one\n",
    "#             return asyncio.run(self.run_experiment_async(dataset_path, n_functions, seed))\n",
    "    \n",
    "#     def _aggregate_cross_function_stats(self, function_results: List[Dict]) -> Dict[str, Any]:\n",
    "#         \"\"\"Aggregate statistics across all functions\"\"\"\n",
    "#         aggregated = {\n",
    "#             'total_tests': 0,\n",
    "#             'model_role_category_totals': defaultdict(lambda: defaultdict(lambda: defaultdict(int))),\n",
    "#             'model_category_totals': defaultdict(lambda: defaultdict(int)),\n",
    "#             'role_category_totals': defaultdict(lambda: defaultdict(int)),\n",
    "#             'model_totals': defaultdict(int),\n",
    "#             'role_totals': defaultdict(int),\n",
    "#             'category_totals': defaultdict(int)\n",
    "#         }\n",
    "        \n",
    "#         for func_result in function_results:\n",
    "#             aggregated['total_tests'] += func_result['total_tests_generated']\n",
    "            \n",
    "#             # Aggregate model × role × category\n",
    "#             for model, roles in func_result['model_role_category_matrix'].items():\n",
    "#                 for role, categories in roles.items():\n",
    "#                     for category, count in categories.items():\n",
    "#                         aggregated['model_role_category_totals'][model][role][category] += count\n",
    "#                         aggregated['model_category_totals'][model][category] += count\n",
    "#                         aggregated['role_category_totals'][role][category] += count\n",
    "            \n",
    "#             # Aggregate totals\n",
    "#             for model, count in func_result['model_totals'].items():\n",
    "#                 aggregated['model_totals'][model] += count\n",
    "#             for role, count in func_result['role_totals'].items():\n",
    "#                 aggregated['role_totals'][role] += count\n",
    "#             for category, count in func_result['category_totals'].items():\n",
    "#                 aggregated['category_totals'][category] += count\n",
    "        \n",
    "#         return aggregated\n",
    "    \n",
    "#     def _generate_role_recommendations(self, aggregated_stats: Dict) -> Dict[str, Any]:\n",
    "#         \"\"\"Generate role assignment recommendations based on performance\"\"\"\n",
    "#         recommendations = {\n",
    "#             'model_strengths': {},\n",
    "#             'optimal_assignments': {},\n",
    "#             'specialization_scores': {}\n",
    "#         }\n",
    "        \n",
    "#         # Calculate specialization scores for each model-role-category combination\n",
    "#         for model, roles in aggregated_stats['model_role_category_totals'].items():\n",
    "#             model_strengths = defaultdict(dict)\n",
    "            \n",
    "#             for role, categories in roles.items():\n",
    "#                 role_info = self.llm_council.roles[role]\n",
    "#                 focus_categories = role_info['focus_categories']\n",
    "                \n",
    "#                 # Calculate alignment score: tests in focus categories / total tests\n",
    "#                 focus_count = sum(categories.get(cat, 0) for cat in focus_categories)\n",
    "#                 total_count = sum(categories.values())\n",
    "                \n",
    "#                 if total_count > 0:\n",
    "#                     alignment_score = focus_count / total_count\n",
    "#                     productivity_score = total_count  # Raw number of tests\n",
    "                    \n",
    "#                     # Combined score: weighted average of alignment and productivity\n",
    "#                     combined_score = (alignment_score * 0.6) + (min(productivity_score / 10, 1.0) * 0.4)\n",
    "                    \n",
    "#                     model_strengths[role] = {\n",
    "#                         'alignment_score': alignment_score,\n",
    "#                         'productivity_score': productivity_score,\n",
    "#                         'combined_score': combined_score,\n",
    "#                         'focus_categories': focus_categories,\n",
    "#                         'category_distribution': dict(categories)\n",
    "#                     }\n",
    "            \n",
    "#             recommendations['model_strengths'][model] = dict(model_strengths)\n",
    "        \n",
    "#         # Determine optimal assignments (highest combined score for each model)\n",
    "#         for model, strengths in recommendations['model_strengths'].items():\n",
    "#             if strengths:\n",
    "#                 best_roles = sorted(strengths.items(), key=lambda x: x[1]['combined_score'], reverse=True)\n",
    "#                 recommendations['optimal_assignments'][model] = [role for role, _ in best_roles[:2]]  # Top 2 roles\n",
    "        \n",
    "#         return recommendations\n",
    "    \n",
    "#     def _save_checkpoint(self, function_results: List[Dict], checkpoint_num: int):\n",
    "#         \"\"\"Save checkpoint of results\"\"\"\n",
    "#         checkpoint_path = f'experiment_checkpoint_{checkpoint_num}.json'\n",
    "#         try:\n",
    "#             # Convert defaultdict to regular dict for JSON serialization\n",
    "#             serializable_results = []\n",
    "#             for result in function_results:\n",
    "#                 serializable_result = result.copy()\n",
    "#                 serializable_result['model_role_category_matrix'] = {\n",
    "#                     model: {\n",
    "#                         role: dict(categories)\n",
    "#                         for role, categories in roles.items()\n",
    "#                     }\n",
    "#                     for model, roles in result['model_role_category_matrix'].items()\n",
    "#                 }\n",
    "#                 serializable_result['model_totals'] = dict(result['model_totals'])\n",
    "#                 serializable_result['role_totals'] = dict(result['role_totals'])\n",
    "#                 serializable_result['category_totals'] = dict(result['category_totals'])\n",
    "#                 serializable_results.append(serializable_result)\n",
    "            \n",
    "#             with open(checkpoint_path, 'w') as f:\n",
    "#                 json.dump(serializable_results, f, indent=2)\n",
    "#         except Exception as e:\n",
    "#             print(f\"⚠️ Could not save checkpoint: {e}\")\n",
    "    \n",
    "#     def _save_experiment_results(self, experiment_results: Dict):\n",
    "#         \"\"\"Save final experiment results\"\"\"\n",
    "#         output_path = f'role_assignment_experiment_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
    "        \n",
    "#         try:\n",
    "#             # Make results JSON serializable\n",
    "#             serializable_results = self._make_serializable(experiment_results)\n",
    "            \n",
    "#             with open(output_path, 'w') as f:\n",
    "#                 json.dump(serializable_results, f, indent=2)\n",
    "            \n",
    "#             print(f\"\\n💾 Results saved to: {output_path}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"⚠️ Could not save experiment results: {e}\")\n",
    "    \n",
    "#     def _make_serializable(self, obj):\n",
    "#         \"\"\"Convert defaultdicts and other non-serializable objects to regular dicts\"\"\"\n",
    "#         if isinstance(obj, defaultdict):\n",
    "#             return {k: self._make_serializable(v) for k, v in obj.items()}\n",
    "#         elif isinstance(obj, dict):\n",
    "#             return {k: self._make_serializable(v) for k, v in obj.items()}\n",
    "#         elif isinstance(obj, list):\n",
    "#             return [self._make_serializable(item) for item in obj]\n",
    "#         else:\n",
    "#             return obj\n",
    "    \n",
    "#     def _print_experiment_summary(self, experiment_results: Dict):\n",
    "#         \"\"\"Print a comprehensive summary of experiment results\"\"\"\n",
    "#         print(f\"\\n{'='*80}\")\n",
    "#         print(\"📊 ROLE ASSIGNMENT OPTIMIZATION EXPERIMENT SUMMARY\")\n",
    "#         print(f\"{'='*80}\\n\")\n",
    "        \n",
    "#         stats = experiment_results['aggregated_stats']\n",
    "#         recs = experiment_results['recommendations']\n",
    "        \n",
    "#         print(f\"Functions Processed: {experiment_results['n_functions_processed']}/{experiment_results['n_functions_target']}\")\n",
    "#         print(f\"Total Tests Generated: {stats['total_tests']}\")\n",
    "#         print(f\"Max Concurrent Requests: {experiment_results.get('max_concurrent_requests', 'N/A')}\")\n",
    "#         print(f\"\\n{'─'*80}\\n\")\n",
    "        \n",
    "#         # Model productivity\n",
    "#         print(\"📈 MODEL PRODUCTIVITY (Total Tests Generated):\")\n",
    "#         for model, count in sorted(stats['model_totals'].items(), key=lambda x: x[1], reverse=True):\n",
    "#             print(f\"   {model}: {count} tests\")\n",
    "        \n",
    "#         print(f\"\\n{'─'*80}\\n\")\n",
    "        \n",
    "#         # Role distribution\n",
    "#         print(\"🎭 ROLE DISTRIBUTION (Total Tests per Role):\")\n",
    "#         for role, count in sorted(stats['role_totals'].items(), key=lambda x: x[1], reverse=True):\n",
    "#             role_name = self.llm_council.roles[role]['name']\n",
    "#             print(f\"   {role_name}: {count} tests\")\n",
    "        \n",
    "#         print(f\"\\n{'─'*80}\\n\")\n",
    "        \n",
    "#         # Category distribution\n",
    "#         print(\"📁 CATEGORY DISTRIBUTION (Total Tests per Category):\")\n",
    "#         for category, count in sorted(stats['category_totals'].items(), key=lambda x: x[1], reverse=True):\n",
    "#             print(f\"   {category}: {count} tests\")\n",
    "        \n",
    "#         print(f\"\\n{'='*80}\\n\")\n",
    "#         print(\"🎯 RECOMMENDED OPTIMAL ROLE ASSIGNMENTS:\")\n",
    "#         print(f\"{'='*80}\\n\")\n",
    "        \n",
    "#         for model, roles in recs['optimal_assignments'].items():\n",
    "#             print(f\"💡 {model}:\")\n",
    "#             for role in roles:\n",
    "#                 role_name = self.llm_council.roles[role]['name']\n",
    "#                 strength = recs['model_strengths'][model][role]\n",
    "#                 print(f\"   → {role_name}\")\n",
    "#                 print(f\"      Alignment: {strength['alignment_score']:.2%}\")\n",
    "#                 print(f\"      Productivity: {strength['productivity_score']} tests\")\n",
    "#                 print(f\"      Combined Score: {strength['combined_score']:.3f}\")\n",
    "#                 print()\n",
    "        \n",
    "#         print(f\"{'='*80}\\n\")\n",
    "        \n",
    "#         # Create detailed performance matrix\n",
    "#         self._print_performance_matrix(stats, recs)\n",
    "    \n",
    "#     def _print_performance_matrix(self, stats: Dict, recs: Dict):\n",
    "#         \"\"\"Print detailed performance matrix\"\"\"\n",
    "#         print(\"📊 DETAILED PERFORMANCE MATRIX (Model × Role × Category):\")\n",
    "#         print(f\"{'='*80}\\n\")\n",
    "        \n",
    "#         for model in stats['model_role_category_totals'].keys():\n",
    "#             print(f\"🤖 {model}:\")\n",
    "#             print(f\"{'─'*76}\")\n",
    "            \n",
    "#             if model in recs['model_strengths']:\n",
    "#                 for role, strength_data in recs['model_strengths'][model].items():\n",
    "#                     role_name = self.llm_council.roles[role]['name']\n",
    "#                     categories = strength_data['category_distribution']\n",
    "                    \n",
    "#                     print(f\"\\n   🎭 {role_name}:\")\n",
    "#                     print(f\"      Focus Categories: {', '.join(strength_data['focus_categories'])}\")\n",
    "#                     print(f\"      Performance:\")\n",
    "                    \n",
    "#                     for category in sorted(categories.keys()):\n",
    "#                         count = categories[category]\n",
    "#                         is_focus = category in strength_data['focus_categories']\n",
    "#                         marker = \"★\" if is_focus else \" \"\n",
    "#                         print(f\"         {marker} {category}: {count} tests\")\n",
    "                    \n",
    "#                     print(f\"      → Alignment: {strength_data['alignment_score']:.2%} | \"\n",
    "#                           f\"Combined Score: {strength_data['combined_score']:.3f}\")\n",
    "            \n",
    "#             print()\n",
    "\n",
    "# # Initialize and run experiment\n",
    "# experiment = RoleAssignmentExperiment(llm_council, code_analyzer, test_classifier)\n",
    "\n",
    "# # Run the experiment with 20 random functions using concurrent API calls\n",
    "# print(\"⚡ Starting experiment with concurrent API processing...\")\n",
    "# results = experiment.run_experiment(\n",
    "#     dataset_path='data/python_algorithms_dataset.json',\n",
    "#     n_functions=20,\n",
    "#     seed=42\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "194dbff7-2025-4ff8-95ea-d4ce96b449f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 11: Analyze Role Assignment Results from Checkpoints\n",
    "# import json\n",
    "# import glob\n",
    "# from collections import defaultdict\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import numpy as np\n",
    "\n",
    "# class RoleAssignmentAnalyzer:\n",
    "#     \"\"\"Analyze checkpoint results to determine optimal model-role assignments\"\"\"\n",
    "    \n",
    "#     def __init__(self, roles_config):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             roles_config: Dictionary of role configurations from llm_council.roles\n",
    "#         \"\"\"\n",
    "#         self.roles_config = roles_config\n",
    "#         self.aggregated_data = None\n",
    "#         self.recommendations = None\n",
    "    \n",
    "#     def load_checkpoints(self, checkpoint_pattern='experiment_checkpoint_*.json'):\n",
    "#         \"\"\"Load all checkpoint files and aggregate results\"\"\"\n",
    "#         checkpoint_files = sorted(glob.glob(checkpoint_pattern), \n",
    "#                                  key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "        \n",
    "#         if not checkpoint_files:\n",
    "#             print(f\"❌ No checkpoint files found matching pattern: {checkpoint_pattern}\")\n",
    "#             return None\n",
    "        \n",
    "#         print(f\"📂 Found {len(checkpoint_files)} checkpoint files\")\n",
    "        \n",
    "#         # Aggregate all checkpoint data\n",
    "#         all_function_results = []\n",
    "#         for checkpoint_file in checkpoint_files:\n",
    "#             with open(checkpoint_file, 'r') as f:\n",
    "#                 checkpoint_data = json.load(f)\n",
    "#                 # Each checkpoint file contains a list with one function result\n",
    "#                 all_function_results.extend(checkpoint_data)\n",
    "        \n",
    "#         print(f\"✅ Loaded {len(all_function_results)} function results\")\n",
    "        \n",
    "#         # Aggregate statistics\n",
    "#         self.aggregated_data = self._aggregate_statistics(all_function_results)\n",
    "#         return self.aggregated_data\n",
    "    \n",
    "#     def _aggregate_statistics(self, function_results):\n",
    "#         \"\"\"Aggregate statistics across all function results\"\"\"\n",
    "#         aggregated = {\n",
    "#             'total_tests': 0,\n",
    "#             'model_role_category_matrix': defaultdict(lambda: defaultdict(lambda: defaultdict(int))),\n",
    "#             'model_totals': defaultdict(int),\n",
    "#             'role_totals': defaultdict(int),\n",
    "#             'category_totals': defaultdict(int)\n",
    "#         }\n",
    "        \n",
    "#         for func_result in function_results:\n",
    "#             aggregated['total_tests'] += func_result['total_tests_generated']\n",
    "            \n",
    "#             # Aggregate model × role × category\n",
    "#             for model, roles in func_result['model_role_category_matrix'].items():\n",
    "#                 for role, categories in roles.items():\n",
    "#                     for category, count in categories.items():\n",
    "#                         aggregated['model_role_category_matrix'][model][role][category] += count\n",
    "            \n",
    "#             # Aggregate totals\n",
    "#             for model, count in func_result['model_totals'].items():\n",
    "#                 aggregated['model_totals'][model] += count\n",
    "#             for role, count in func_result['role_totals'].items():\n",
    "#                 aggregated['role_totals'][role] += count\n",
    "#             for category, count in func_result['category_totals'].items():\n",
    "#                 aggregated['category_totals'][category] += count\n",
    "        \n",
    "#         return aggregated\n",
    "    \n",
    "#     def calculate_scores(self):\n",
    "#         \"\"\"Calculate alignment, productivity, and combined scores for each model-role combination\"\"\"\n",
    "#         if self.aggregated_data is None:\n",
    "#             print(\"❌ No aggregated data. Run load_checkpoints() first.\")\n",
    "#             return None\n",
    "        \n",
    "#         scores = defaultdict(lambda: defaultdict(dict))\n",
    "        \n",
    "#         for model, roles in self.aggregated_data['model_role_category_matrix'].items():\n",
    "#             for role, categories in roles.items():\n",
    "#                 role_info = self.roles_config[role]\n",
    "#                 focus_categories = role_info['focus_categories']\n",
    "                \n",
    "#                 # Calculate metrics\n",
    "#                 focus_count = sum(categories.get(cat, 0) for cat in focus_categories)\n",
    "#                 total_count = sum(categories.values())\n",
    "                \n",
    "#                 if total_count > 0:\n",
    "#                     alignment_score = focus_count / total_count\n",
    "#                     productivity_score = total_count\n",
    "#                     # Combined score: weighted average (60% alignment, 40% normalized productivity)\n",
    "#                     combined_score = (alignment_score * 0.6) + (min(productivity_score / 10, 1.0) * 0.4)\n",
    "                    \n",
    "#                     scores[model][role] = {\n",
    "#                         'alignment': alignment_score,\n",
    "#                         'productivity': productivity_score,\n",
    "#                         'combined': combined_score,\n",
    "#                         'focus_categories': focus_categories,\n",
    "#                         'category_distribution': dict(categories)\n",
    "#                     }\n",
    "        \n",
    "#         self.recommendations = dict(scores)\n",
    "#         return self.recommendations\n",
    "    \n",
    "#     def create_summary_dataframes(self):\n",
    "#         \"\"\"Create pandas DataFrames for easy viewing and analysis\"\"\"\n",
    "#         if self.recommendations is None:\n",
    "#             print(\"❌ No recommendations. Run calculate_scores() first.\")\n",
    "#             return None\n",
    "        \n",
    "#         # Create separate DataFrames for each metric\n",
    "#         models = sorted(self.recommendations.keys())\n",
    "#         roles = sorted(set(role for model_roles in self.recommendations.values() \n",
    "#                           for role in model_roles.keys()))\n",
    "        \n",
    "#         # Productivity DataFrame\n",
    "#         productivity_data = []\n",
    "#         for model in models:\n",
    "#             row = {'Model': model}\n",
    "#             for role in roles:\n",
    "#                 if role in self.recommendations[model]:\n",
    "#                     row[self.roles_config[role]['name']] = self.recommendations[model][role]['productivity']\n",
    "#                 else:\n",
    "#                     row[self.roles_config[role]['name']] = 0\n",
    "#             productivity_data.append(row)\n",
    "#         df_productivity = pd.DataFrame(productivity_data).set_index('Model')\n",
    "        \n",
    "#         # Alignment DataFrame\n",
    "#         alignment_data = []\n",
    "#         for model in models:\n",
    "#             row = {'Model': model}\n",
    "#             for role in roles:\n",
    "#                 if role in self.recommendations[model]:\n",
    "#                     row[self.roles_config[role]['name']] = self.recommendations[model][role]['alignment']\n",
    "#                 else:\n",
    "#                     row[self.roles_config[role]['name']] = 0\n",
    "#             alignment_data.append(row)\n",
    "#         df_alignment = pd.DataFrame(alignment_data).set_index('Model')\n",
    "        \n",
    "#         # Combined Score DataFrame\n",
    "#         combined_data = []\n",
    "#         for model in models:\n",
    "#             row = {'Model': model}\n",
    "#             for role in roles:\n",
    "#                 if role in self.recommendations[model]:\n",
    "#                     row[self.roles_config[role]['name']] = self.recommendations[model][role]['combined']\n",
    "#                 else:\n",
    "#                     row[self.roles_config[role]['name']] = 0\n",
    "#             combined_data.append(row)\n",
    "#         df_combined = pd.DataFrame(combined_data).set_index('Model')\n",
    "        \n",
    "#         return {\n",
    "#             'productivity': df_productivity,\n",
    "#             'alignment': df_alignment,\n",
    "#             'combined': df_combined\n",
    "#         }\n",
    "    \n",
    "#     def visualize_results(self, figsize=(20, 12)):\n",
    "#         \"\"\"Create comprehensive visualizations of model-role performance\"\"\"\n",
    "#         dataframes = self.create_summary_dataframes()\n",
    "#         if dataframes is None:\n",
    "#             return\n",
    "        \n",
    "#         fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "#         fig.suptitle('Model-Role Performance Analysis', fontsize=16, fontweight='bold', y=1.00)\n",
    "        \n",
    "#         # 1. Productivity Heatmap\n",
    "#         ax1 = axes[0, 0]\n",
    "#         sns.heatmap(dataframes['productivity'], annot=True, fmt='.0f', cmap='YlOrRd', \n",
    "#                    ax=ax1, cbar_kws={'label': 'Tests Generated'})\n",
    "#         ax1.set_title('Productivity Score (Total Tests Generated)', fontweight='bold')\n",
    "#         ax1.set_xlabel('')\n",
    "        \n",
    "#         # 2. Alignment Heatmap\n",
    "#         ax2 = axes[0, 1]\n",
    "#         sns.heatmap(dataframes['alignment'], annot=True, fmt='.2%', cmap='YlGnBu', \n",
    "#                    ax=ax2, cbar_kws={'label': 'Alignment Score'}, vmin=0, vmax=1)\n",
    "#         ax2.set_title('Alignment Score (Focus Category Accuracy)', fontweight='bold')\n",
    "#         ax2.set_xlabel('')\n",
    "        \n",
    "#         # 3. Combined Score Heatmap\n",
    "#         ax3 = axes[1, 0]\n",
    "#         sns.heatmap(dataframes['combined'], annot=True, fmt='.3f', cmap='RdYlGn', \n",
    "#                    ax=ax3, cbar_kws={'label': 'Combined Score'}, vmin=0, vmax=1)\n",
    "#         ax3.set_title('Combined Score (60% Alignment + 40% Productivity)', fontweight='bold')\n",
    "#         ax3.set_xlabel('')\n",
    "        \n",
    "#         # 4. Best Model per Role (Bar Chart)\n",
    "#         ax4 = axes[1, 1]\n",
    "#         best_models_per_role = {}\n",
    "#         for role in dataframes['combined'].columns:\n",
    "#             best_model = dataframes['combined'][role].idxmax()\n",
    "#             best_score = dataframes['combined'][role].max()\n",
    "#             best_models_per_role[role] = (best_model, best_score)\n",
    "        \n",
    "#         roles_list = list(best_models_per_role.keys())\n",
    "#         scores_list = [score for _, score in best_models_per_role.values()]\n",
    "#         models_list = [model for model, _ in best_models_per_role.values()]\n",
    "        \n",
    "#         colors = plt.cm.Set3(np.linspace(0, 1, len(set(models_list))))\n",
    "#         model_to_color = {model: colors[i] for i, model in enumerate(sorted(set(models_list)))}\n",
    "#         bar_colors = [model_to_color[model] for model in models_list]\n",
    "        \n",
    "#         bars = ax4.barh(roles_list, scores_list, color=bar_colors)\n",
    "#         ax4.set_xlabel('Combined Score', fontweight='bold')\n",
    "#         ax4.set_title('Best Model per Role', fontweight='bold')\n",
    "#         ax4.set_xlim(0, 1)\n",
    "        \n",
    "#         # Add model names on bars\n",
    "#         for i, (bar, model) in enumerate(zip(bars, models_list)):\n",
    "#             width = bar.get_width()\n",
    "#             ax4.text(width + 0.02, bar.get_y() + bar.get_height()/2, \n",
    "#                     f'{model}', ha='left', va='center', fontsize=9)\n",
    "        \n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "    \n",
    "#     def print_recommendations(self):\n",
    "#         \"\"\"Print detailed recommendations for role assignments\"\"\"\n",
    "#         if self.recommendations is None:\n",
    "#             print(\"❌ No recommendations. Run calculate_scores() first.\")\n",
    "#             return\n",
    "        \n",
    "#         print(f\"\\n{'='*80}\")\n",
    "#         print(\"🎯 ROLE ASSIGNMENT RECOMMENDATIONS\")\n",
    "#         print(f\"{'='*80}\\n\")\n",
    "        \n",
    "#         # For each role, show which model performs best\n",
    "#         roles = sorted(set(role for model_roles in self.recommendations.values() \n",
    "#                           for role in model_roles.keys()))\n",
    "        \n",
    "#         for role in roles:\n",
    "#             role_name = self.roles_config[role]['name']\n",
    "#             focus_cats = self.roles_config[role]['focus_categories']\n",
    "            \n",
    "#             print(f\"\\n{'─'*80}\")\n",
    "#             print(f\"📋 Role: {role_name}\")\n",
    "#             print(f\"   Focus Categories: {', '.join(focus_cats)}\")\n",
    "#             print(f\"{'─'*80}\")\n",
    "            \n",
    "#             # Collect scores for this role across all models\n",
    "#             model_scores = []\n",
    "#             for model in sorted(self.recommendations.keys()):\n",
    "#                 if role in self.recommendations[model]:\n",
    "#                     scores = self.recommendations[model][role]\n",
    "#                     model_scores.append((model, scores))\n",
    "            \n",
    "#             # Sort by combined score\n",
    "#             model_scores.sort(key=lambda x: x[1]['combined'], reverse=True)\n",
    "            \n",
    "#             print(f\"\\n   Model Performance Ranking:\\n\")\n",
    "#             for rank, (model, scores) in enumerate(model_scores, 1):\n",
    "#                 marker = \"🥇\" if rank == 1 else \"🥈\" if rank == 2 else \"🥉\" if rank == 3 else f\"  {rank}.\"\n",
    "#                 print(f\"   {marker} {model}\")\n",
    "#                 print(f\"      ├─ Productivity: {scores['productivity']} tests\")\n",
    "#                 print(f\"      ├─ Alignment:    {scores['alignment']:.1%}\")\n",
    "#                 print(f\"      └─ Combined:     {scores['combined']:.3f}\")\n",
    "                \n",
    "#                 if rank == 1:\n",
    "#                     print(f\"      ✅ RECOMMENDED for this role\")\n",
    "#                 print()\n",
    "        \n",
    "#         print(f\"\\n{'='*80}\")\n",
    "#         print(\"💡 SUMMARY: Optimal Model-Role Assignments\")\n",
    "#         print(f\"{'='*80}\\n\")\n",
    "        \n",
    "#         # Show best model for each role\n",
    "#         for role in roles:\n",
    "#             role_name = self.roles_config[role]['name']\n",
    "#             best_model = None\n",
    "#             best_score = -1\n",
    "            \n",
    "#             for model in self.recommendations.keys():\n",
    "#                 if role in self.recommendations[model]:\n",
    "#                     if self.recommendations[model][role]['combined'] > best_score:\n",
    "#                         best_score = self.recommendations[model][role]['combined']\n",
    "#                         best_model = model\n",
    "            \n",
    "#             if best_model:\n",
    "#                 print(f\"   {role_name:30s} → {best_model}\")\n",
    "        \n",
    "#         print(f\"\\n{'='*80}\\n\")\n",
    "\n",
    "# # Create analyzer instance\n",
    "# analyzer = RoleAssignmentAnalyzer(llm_council.roles)\n",
    "\n",
    "# # Load and analyze checkpoint data\n",
    "# print(\"📊 Loading checkpoint files...\")\n",
    "# aggregated_data = analyzer.load_checkpoints('experiment_checkpoint_*.json')\n",
    "\n",
    "# if aggregated_data:\n",
    "#     print(\"\\n🔍 Calculating performance scores...\")\n",
    "#     recommendations = analyzer.calculate_scores()\n",
    "    \n",
    "#     print(\"\\n📈 Creating visualizations...\")\n",
    "#     analyzer.visualize_results(figsize=(20, 12))\n",
    "    \n",
    "#     print(\"\\n📋 Generating recommendations...\")\n",
    "#     analyzer.print_recommendations()\n",
    "    \n",
    "#     # Show summary tables\n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "#     print(\"📊 DETAILED SCORE TABLES\")\n",
    "#     print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "#     dfs = analyzer.create_summary_dataframes()\n",
    "    \n",
    "#     print(\"\\n1️⃣ PRODUCTIVITY SCORES (Total Tests Generated):\")\n",
    "#     print(dfs['productivity'].to_string())\n",
    "    \n",
    "#     print(\"\\n\\n2️⃣ ALIGNMENT SCORES (Focus Category Accuracy):\")\n",
    "#     print(dfs['alignment'].applymap(lambda x: f\"{x:.1%}\").to_string())\n",
    "    \n",
    "#     print(\"\\n\\n3️⃣ COMBINED SCORES (Weighted Performance):\")\n",
    "#     print(dfs['combined'].applymap(lambda x: f\"{x:.3f}\").to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73375304-0d18-482b-9ea6-dcda44d41ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting async demo with concurrent API calls...\n",
      "🎯 Demonstrating Intelligent Test Council (Async + Concurrent Mode)\n",
      "======================================================================\n",
      "⚡ Maximum concurrent API requests: 7\n",
      "======================================================================\n",
      "🚀 Starting Role-Based Intelligent Test Council Pipeline (Async Mode)\n",
      "======================================================================\n",
      "\n",
      "📝 Step 1: Analyzing input function...\n",
      "✅ Found 1 function(s)\n",
      "\n",
      "🎭 Step 2: Consulting Role-Based LLM Council (Concurrent Mode)...\n",
      "🤖 Consulting Role-Based LLM Council for test generation (Concurrent Mode)...\n",
      "======================================================================\n",
      "⚡ Maximum concurrent requests: 7\n",
      "📊 Total API calls to make: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concurrent API calls: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:45<00:00,  6.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ gemini-2.0-flash as 'By-the-Book QA Engineer': 13 tests\n",
      "✅ gemini-2.0-flash as 'Abstract Thinker': 10 tests\n",
      "✅ gemini-2.0-flash as 'Agent of Chaos': 14 tests\n",
      "✅ qwen3-235b-a22b as 'By-the-Book QA Engineer': 11 tests\n",
      "✅ qwen3-235b-a22b as 'Agent of Chaos': 14 tests\n",
      "✅ grok-3-mini as 'Abstract Thinker': 17 tests\n",
      "✅ grok-3-mini as 'Paranoid Security Auditor': 9 tests\n",
      "======================================================================\n",
      "\n",
      "🏷️  Step 3: Classifying test cases by category and role...\n",
      "✅ Total tests generated: 88\n",
      "\n",
      "🎭 Role distribution:\n",
      "   • By-the-Book QA Engineer: 24 tests\n",
      "   • Abstract Thinker: 27 tests\n",
      "   • Agent of Chaos: 28 tests\n",
      "   • Paranoid Security Auditor: 9 tests\n",
      "\n",
      "📊 Category distribution:\n",
      "   • positive: 27 tests\n",
      "   • negative: 27 tests\n",
      "   • edge_case: 17 tests\n",
      "   • boundary: 10 tests\n",
      "   • security: 7 tests\n",
      "\n",
      "🔬 Model-Role Performance Matrix:\n",
      "\n",
      "   gemini-2.0-flash:\n",
      "      └─ By-the-Book QA Engineer: 13 tests\n",
      "      └─ Abstract Thinker: 10 tests\n",
      "      └─ Agent of Chaos: 14 tests\n",
      "\n",
      "   qwen3-235b-a22b:\n",
      "      └─ By-the-Book QA Engineer: 11 tests\n",
      "      └─ Agent of Chaos: 14 tests\n",
      "\n",
      "   grok-3-mini:\n",
      "      └─ Abstract Thinker: 17 tests\n",
      "      └─ Paranoid Security Auditor: 9 tests\n",
      "\n",
      "🔄 Step 4: Synthesizing final test file with duplicate removal...\n",
      "🔄 Synthesizing final test file with intelligent duplicate removal...\n",
      "✅ Final test file synthesized successfully!\n",
      "📊 Reduced 88 original tests to 24 unique tests\n",
      "📉 Reduction ratio: 72.7%\n",
      "\n",
      "📊 Step 5: Analyzing code coverage...\n",
      "Debug: Found function.py coverage: 100.0%\n",
      "Debug: Found total tests: 24\n",
      "Debug: Extracted test stats - Passed: 23, Failed: 1, Skipped: 0\n",
      "Coverage analysis complete. Function.py coverage: 100.0%\n",
      "Test results: 23/24 passed (95.8%)\n",
      "\n",
      "🎉 Pipeline completed successfully!\n",
      "📊 Test Success Rate: 95.8%\n",
      "📈 Code Coverage: 100.0%\n",
      "✅ Passed Tests: 23/24\n",
      "======================================================================\n",
      "\n",
      "📊 Key Statistics:\n",
      "   • Execution time: 61.71 seconds\n",
      "   • Original tests generated: 88\n",
      "   • Final tests after synthesis: 24\n",
      "   • Reduction ratio: 72.73%\n",
      "   • Code coverage: 100.0%\n",
      "   • Test success rate: 95.8%\n",
      "   • Models used: gemini-2.0-flash, qwen3-235b-a22b, grok-3-mini\n",
      "   • Roles used: Abstract Thinker, By-the-Book QA Engineer, Paranoid Security Auditor, Agent of Chaos\n",
      "   • Test categories: positive, negative, edge_case, boundary, security\n",
      "   • Synthesizer model: gemini-2.0-flash\n",
      "\n",
      "🎭 Role-Based Generation Summary:\n",
      "   • By-the-Book QA Engineer: 24 tests\n",
      "   • Abstract Thinker: 27 tests\n",
      "   • Agent of Chaos: 28 tests\n",
      "   • Paranoid Security Auditor: 9 tests\n",
      "\n",
      "📁 Results saved to ./test_output/\n",
      "🔧 Function under test: ./test_output/function.py\n",
      "✅ Clean Python test file: ./test_output/test_generated.py\n",
      "📊 Analysis results: ./test_output/analysis_results.json\n",
      "\n",
      "🎭 Role-Based Generation Summary:\n",
      "   • By-the-Book QA Engineer: 24 tests\n",
      "   • Abstract Thinker: 27 tests\n",
      "   • Agent of Chaos: 28 tests\n",
      "   • Paranoid Security Auditor: 9 tests\n",
      "\n",
      "📋 Final Test File Preview:\n",
      "----------------------------------------------------------------------\n",
      "import pytest\n",
      "from function import divide_numbers\n",
      "import math\n",
      "\n",
      "# Positive Tests\n",
      "def test_divide_numbers_valid_division():\n",
      "    \"\"\"Verify normal division works with integers.\"\"\"\n",
      "    # Category: positive\n",
      "    result = divide_numbers(10, 2)\n",
      "    assert result == 5.0, \"Normal division should work correctly\"\n",
      "\n",
      "def test_divide_numbers_float_division():\n",
      "    \"\"\"Verify dividing floats.\"\"\"\n",
      "    # Category: positive\n",
      "    result = divide_numbers(5.5, 2.5)\n",
      "    assert result == 2.2, \"Float division should work correctly\"\n",
      "\n",
      "def test_divide_numbers_numerator_zero():\n",
      "    \"\"\"Verify dividing zero by a number.\"\"\"\n",
      "    # Category: positive\n",
      "    result = divide_numbers(0, 5)\n",
      "    assert result == 0.0, \"Dividing zero should return zero\"\n",
      "\n",
      "def test_divide_numbers_negative_numbers():\n",
      "    \"\"\"Verify dividing negative numbers.\"\"\"\n",
      "    # Category: positive\n",
      "    result = divide_numbers(-10, -2)\n",
      "    assert result == 5.0, \"Negative division failed\"\n",
      "\n",
      "def test_divide_numbers_mixed_negative_positive():\n",
      "    \"\"\"Verify dividing negative by positive and vice versa.\"\"\"\n",
      "    # Category: positive\n",
      "    result = divide_numbers(-10, 2)\n",
      "    assert result == -5.0, \"Mixed division failed\"\n",
      "    result = divide_numbers(10, -2)\n",
      "    assert result == -5.0, \"Mixed division failed\"\n",
      "\n",
      "def test_divide_numbers_inverse_property():\n",
      "    \"\"\"Verify that dividing a number by itself returns 1.0.\"\"\"\n",
      "    # Category: positive\n",
      "    result = divide_numbers(5, 5)\n",
      "    assert result == 1.0, \"Division identity property failed: a / a should equal 1.0\"\n",
      "\n",
      "def test_divid\n",
      "... (truncated)\n",
      "\n",
      "⏱️  Total execution time: 61.71 seconds\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Async Demo with Concurrent API Calls\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "\n",
    "# Enable nested asyncio for Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "class AsyncIntelligentTestCouncil(IntelligentTestCouncil):\n",
    "    \"\"\"Async version of the test council with concurrent API calls\"\"\"\n",
    "    \n",
    "    async def generate_comprehensive_tests_async(self, function_code: str, max_concurrent: int = 7) -> Dict[str, Any]:\n",
    "        \"\"\"Async version of main pipeline with concurrent API calls\"\"\"\n",
    "        print(\"🚀 Starting Role-Based Intelligent Test Council Pipeline (Async Mode)\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Step 1: Analyze the input function\n",
    "        print(\"\\n📝 Step 1: Analyzing input function...\")\n",
    "        function_info = self.code_analyzer.extract_function_info(function_code)\n",
    "        \n",
    "        if not function_info['functions']:\n",
    "            error_msg = 'No functions found in the provided code'\n",
    "            if 'syntax_error' in function_info:\n",
    "                error_msg += f\". Syntax error: {function_info['syntax_error']}\"\n",
    "            return {'error': error_msg}\n",
    "        \n",
    "        print(f\"✅ Found {function_info['total_functions']} function(s)\")\n",
    "        \n",
    "        # Step 2: Generate tests using role-based LLM council with CONCURRENT API calls\n",
    "        print(f\"\\n🎭 Step 2: Consulting Role-Based LLM Council (Concurrent Mode)...\")\n",
    "        council_results = await self.llm_council.generate_tests_from_council_async(\n",
    "            function_info, \n",
    "            max_concurrent=max_concurrent\n",
    "        )\n",
    "        \n",
    "        # Step 3: Classify all test cases\n",
    "        print(\"\\n🏷️  Step 3: Classifying test cases by category and role...\")\n",
    "        all_classified_tests = self.test_classifier.classify_council_results(council_results)\n",
    "        \n",
    "        print(f\"✅ Total tests generated: {len(all_classified_tests)}\")\n",
    "        \n",
    "        # Display role distribution\n",
    "        role_counts = Counter(test['role_name'] for test in all_classified_tests)\n",
    "        print(\"\\n🎭 Role distribution:\")\n",
    "        for role_name, count in role_counts.items():\n",
    "            print(f\"   • {role_name}: {count} tests\")\n",
    "        \n",
    "        # Display category distribution\n",
    "        category_counts = Counter(test['category'] for test in all_classified_tests)\n",
    "        print(\"\\n📊 Category distribution:\")\n",
    "        for category, count in category_counts.items():\n",
    "            print(f\"   • {category}: {count} tests\")\n",
    "        \n",
    "        # Display model-role performance matrix\n",
    "        print(\"\\n🔬 Model-Role Performance Matrix:\")\n",
    "        for model_name, role_results in council_results.items():\n",
    "            print(f\"\\n   {model_name}:\")\n",
    "            for role_id, results in role_results.items():\n",
    "                print(f\"      └─ {results['role_name']}: {results['test_count']} tests\")\n",
    "        \n",
    "        # Step 4: Synthesize final test file\n",
    "        print(f\"\\n🔄 Step 4: Synthesizing final test file with duplicate removal...\")\n",
    "        synthesis_results = self.test_synthesizer.synthesize_final_test_file(all_classified_tests, function_info)\n",
    "        \n",
    "        # Step 5: Analyze coverage\n",
    "        print(\"\\n📊 Step 5: Analyzing code coverage...\")\n",
    "        coverage_results = self.coverage_analyzer.analyze_coverage(\n",
    "            function_code, \n",
    "            synthesis_results['synthesized_content']\n",
    "        )\n",
    "        \n",
    "        # Prepare comprehensive results\n",
    "        results = {\n",
    "            'function_info': function_info,\n",
    "            'council_results': council_results,\n",
    "            'all_classified_tests': all_classified_tests,\n",
    "            'synthesis_results': synthesis_results,\n",
    "            'final_test_file': synthesis_results['synthesized_content'],\n",
    "            'coverage_results': coverage_results,\n",
    "            'statistics': {\n",
    "                'original_test_count': len(all_classified_tests),\n",
    "                'final_test_count': synthesis_results['final_count'],\n",
    "                'reduction_ratio': synthesis_results['reduction_ratio'],\n",
    "                'coverage_percentage': coverage_results.get('coverage_percentage', 0.0),\n",
    "                'test_success_rate': coverage_results.get('success_rate', 0.0),\n",
    "                'total_tests_run': coverage_results.get('total_tests', 0),\n",
    "                'passed_tests': coverage_results.get('passed_tests', 0),\n",
    "                'failed_tests': coverage_results.get('failed_tests', 0),\n",
    "                'skipped_tests': coverage_results.get('skipped_tests', 0),\n",
    "                'error_tests': coverage_results.get('error_tests', 0),\n",
    "                'models_used': list(council_results.keys()),\n",
    "                'roles_used': list(set(test['role_name'] for test in all_classified_tests)),\n",
    "                'categories_found': list(category_counts.keys()),\n",
    "                'synthesizer_model': synthesis_results['synthesizer_model'],\n",
    "                'tests_per_role': dict(role_counts),\n",
    "                'tests_per_category': dict(category_counts),\n",
    "                'model_role_matrix': {\n",
    "                    model: {role: results['test_count'] for role, results in roles.items()}\n",
    "                    for model, roles in council_results.items()\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(\"\\n🎉 Pipeline completed successfully!\")\n",
    "        print(f\"📊 Test Success Rate: {coverage_results.get('success_rate', 0.0):.1f}%\")\n",
    "        print(f\"📈 Code Coverage: {coverage_results.get('coverage_percentage', 0.0):.1f}%\")\n",
    "        print(f\"✅ Passed Tests: {coverage_results.get('passed_tests', 0)}/{coverage_results.get('total_tests', 0)}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Example function to test\n",
    "example_function_1 = '''def divide_numbers(a, b):\n",
    "    \"\"\"\n",
    "    Divide two numbers with error handling\n",
    "\n",
    "    Args:\n",
    "        a (float): Numerator\n",
    "        b (float): Denominator\n",
    "\n",
    "    Returns:\n",
    "        float: Result of division\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If denominator is zero\n",
    "        TypeError: If inputs are not numeric\n",
    "    \"\"\"\n",
    "    if not isinstance(a, (int, float)) or not isinstance(b, (int, float)):\n",
    "        raise TypeError(\"Both arguments must be numeric\")\n",
    "\n",
    "    if b == 0:\n",
    "        raise ValueError(\"Cannot divide by zero\")\n",
    "\n",
    "    return a / b'''\n",
    "\n",
    "# Example 2: More complex function (PROPERLY INDENTED)\n",
    "example_function_2 = '''def validate_password(password):\n",
    "    \"\"\"\n",
    "    Validate password strength\n",
    "\n",
    "    Args:\n",
    "        password (str): Password to validate\n",
    "\n",
    "    Returns:\n",
    "        dict: Validation results with 'valid' boolean and 'errors' list\n",
    "    \"\"\"\n",
    "    if not isinstance(password, str):\n",
    "        return {'valid': False, 'errors': ['Password must be a string']}\n",
    "\n",
    "    errors = []\n",
    "\n",
    "    if len(password) < 8:\n",
    "        errors.append('Password must be at least 8 characters long')\n",
    "\n",
    "    if not any(c.isupper() for c in password):\n",
    "        errors.append('Password must contain at least one uppercase letter')\n",
    "\n",
    "    if not any(c.islower() for c in password):\n",
    "        errors.append('Password must contain at least one lowercase letter')\n",
    "\n",
    "    if not any(c.isdigit() for c in password):\n",
    "        errors.append('Password must contain at least one digit')\n",
    "\n",
    "    special_chars = '!@#$%^&*(),.?\":{}|<>'\n",
    "    if not any(c in special_chars for c in password):\n",
    "        errors.append('Password must contain at least one special character')\n",
    "\n",
    "    return {'valid': len(errors) == 0, 'errors': errors}'''\n",
    "\n",
    "async def demonstrate_council_async(max_concurrent: int = 7):\n",
    "    \"\"\"Async demonstration of the intelligent council with concurrent API calls\"\"\"\n",
    "    \n",
    "    print(\"🎯 Demonstrating Intelligent Test Council (Async + Concurrent Mode)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"⚡ Maximum concurrent API requests: {max_concurrent}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Initialize async version of the council\n",
    "    async_council = AsyncIntelligentTestCouncil(config)\n",
    "    \n",
    "    # Choose example to run\n",
    "    selected_function = example_function_1\n",
    "    \n",
    "    try:\n",
    "        # Run the intelligent council pipeline with concurrent API calls\n",
    "        results = await async_council.generate_comprehensive_tests_async(\n",
    "            selected_function,\n",
    "            max_concurrent=max_concurrent\n",
    "        )\n",
    "        \n",
    "        if 'error' in results:\n",
    "            print(f\"❌ Error: {results['error']}\")\n",
    "            return results\n",
    "        \n",
    "        # Calculate execution time\n",
    "        end_time = datetime.now()\n",
    "        execution_time = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        # Display key statistics\n",
    "        stats = results['statistics']\n",
    "        print(f\"\\n📊 Key Statistics:\")\n",
    "        print(f\"   • Execution time: {execution_time:.2f} seconds\")\n",
    "        print(f\"   • Original tests generated: {stats['original_test_count']}\")\n",
    "        print(f\"   • Final tests after synthesis: {stats['final_test_count']}\")\n",
    "        print(f\"   • Reduction ratio: {stats['reduction_ratio']:.2%}\")\n",
    "        print(f\"   • Code coverage: {stats['coverage_percentage']:.1f}%\")\n",
    "        print(f\"   • Test success rate: {stats['test_success_rate']:.1f}%\")\n",
    "        print(f\"   • Models used: {', '.join(stats['models_used'])}\")\n",
    "        print(f\"   • Roles used: {', '.join(stats['roles_used'])}\")\n",
    "        print(f\"   • Test categories: {', '.join(stats['categories_found'])}\")\n",
    "        print(f\"   • Synthesizer model: {stats['synthesizer_model']}\")\n",
    "        \n",
    "        # Display role-based metrics\n",
    "        print(f\"\\n🎭 Role-Based Generation Summary:\")\n",
    "        for role, count in stats['tests_per_role'].items():\n",
    "            print(f\"   • {role}: {count} tests\")\n",
    "        \n",
    "        # Save results\n",
    "        async_council.save_results(results)\n",
    "        \n",
    "        # Display final test file preview\n",
    "        print(f\"\\n📋 Final Test File Preview:\")\n",
    "        print(\"-\" * 70)\n",
    "        preview_length = 1500\n",
    "        if len(results['final_test_file']) > preview_length:\n",
    "            print(results['final_test_file'][:preview_length] + \"\\n... (truncated)\")\n",
    "        else:\n",
    "            print(results['final_test_file'])\n",
    "        \n",
    "        print(f\"\\n⏱️  Total execution time: {execution_time:.2f} seconds\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Pipeline failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {'error': str(e)}\n",
    "\n",
    "# Run the async demo\n",
    "print(\"🚀 Starting async demo with concurrent API calls...\")\n",
    "demo_results = await demonstrate_council_async(max_concurrent=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d1665fd-d2c6-41b8-af17-aca35b4e2738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 14: Batch Testing and Evaluation (FIXED)\n",
    "# def batch_evaluate_functions(function_list: List[str], output_file: str = \"batch_evaluation.csv\"):\n",
    "#     \"\"\"Evaluate multiple functions in batch and generate comparison report\"\"\"\n",
    "    \n",
    "#     print(f\"🔄 Starting batch evaluation of {len(function_list)} functions...\")\n",
    "    \n",
    "#     results_data = []\n",
    "    \n",
    "#     for i, func_code in enumerate(tqdm(function_list, desc=\"Processing functions\")):\n",
    "#         try:\n",
    "#             print(f\"\\n📝 Processing function {i+1}/{len(function_list)}\")\n",
    "            \n",
    "#             # Run council pipeline\n",
    "#             results = intelligent_council.generate_comprehensive_tests(func_code)\n",
    "            \n",
    "#             if 'error' in results:\n",
    "#                 print(f\"❌ Error processing function {i+1}: {results['error']}\")\n",
    "#                 row_data = {\n",
    "#                     'function_name': f'function_{i+1}',\n",
    "#                     'original_tests': 0,\n",
    "#                     'final_tests': 0,\n",
    "#                     'reduction_ratio': 0,\n",
    "#                     'coverage_percentage': 0,\n",
    "#                     'models_used': 0,\n",
    "#                     'categories_count': 0,\n",
    "#                     'categories': '',\n",
    "#                     'error': results['error'],\n",
    "#                     'success': False\n",
    "#                 }\n",
    "#                 results_data.append(row_data)\n",
    "#                 continue\n",
    "            \n",
    "#             # Extract key metrics\n",
    "#             stats = results['statistics']\n",
    "#             func_info = results['function_info']\n",
    "#             func_name = func_info['functions'][0]['name'] if func_info['functions'] else f\"function_{i+1}\"\n",
    "            \n",
    "#             row_data = {\n",
    "#                 'function_name': func_name,\n",
    "#                 'original_tests': stats['original_test_count'],\n",
    "#                 'final_tests': stats['final_test_count'],\n",
    "#                 'reduction_ratio': stats['reduction_ratio'],\n",
    "#                 'coverage_percentage': stats['coverage_percentage'],\n",
    "#                 'models_used': len(stats['models_used']),\n",
    "#                 'categories_count': len(stats['categories_found']),\n",
    "#                 'categories': ','.join(stats['categories_found']),\n",
    "#                 'synthesizer_model': stats['synthesizer_model'],\n",
    "#                 'success': True\n",
    "#             }\n",
    "            \n",
    "#             results_data.append(row_data)\n",
    "#             print(f\"✅ Function {i+1} processed successfully\")\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"❌ Error processing function {i+1}: {e}\")\n",
    "#             results_data.append({\n",
    "#                 'function_name': f'function_{i+1}',\n",
    "#                 'original_tests': 0,\n",
    "#                 'final_tests': 0,\n",
    "#                 'reduction_ratio': 0,\n",
    "#                 'coverage_percentage': 0,\n",
    "#                 'models_used': 0,\n",
    "#                 'categories_count': 0,\n",
    "#                 'categories': '',\n",
    "#                 'error': str(e),\n",
    "#                 'success': False\n",
    "#             })\n",
    "    \n",
    "#     # Create DataFrame and save results\n",
    "#     df = pd.DataFrame(results_data)\n",
    "#     df.to_csv(output_file, index=False)\n",
    "    \n",
    "#     # Generate summary statistics\n",
    "#     successful_runs = df[df['success'] == True]\n",
    "    \n",
    "#     if len(successful_runs) > 0:\n",
    "#         print(f\"\\n📊 Batch Evaluation Summary:\")\n",
    "#         print(f\"   • Successful runs: {len(successful_runs)}/{len(function_list)}\")\n",
    "#         print(f\"   • Average original tests: {successful_runs['original_tests'].mean():.1f}\")\n",
    "#         print(f\"   • Average final tests: {successful_runs['final_tests'].mean():.1f}\")\n",
    "#         print(f\"   • Average reduction ratio: {successful_runs['reduction_ratio'].mean():.2%}\")\n",
    "#         print(f\"   • Average coverage: {successful_runs['coverage_percentage'].mean():.1f}%\")\n",
    "        \n",
    "#         # Visualization\n",
    "#         fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "#         # Test count distribution\n",
    "#         axes[0, 0].hist(successful_runs['final_tests'], bins=10, alpha=0.7, edgecolor='black')\n",
    "#         axes[0, 0].set_title('Distribution of Final Test Counts')\n",
    "#         axes[0, 0].set_xlabel('Number of Final Tests')\n",
    "#         axes[0, 0].set_ylabel('Frequency')\n",
    "        \n",
    "#         # Coverage distribution\n",
    "#         axes[0, 1].hist(successful_runs['coverage_percentage'], bins=10, alpha=0.7, edgecolor='black')\n",
    "#         axes[0, 1].set_title('Distribution of Code Coverage')\n",
    "#         axes[0, 1].set_xlabel('Coverage Percentage')\n",
    "#         axes[0, 1].set_ylabel('Frequency')\n",
    "        \n",
    "#         # Reduction ratio distribution\n",
    "#         axes[1, 0].hist(successful_runs['reduction_ratio'], bins=10, alpha=0.7, edgecolor='black')\n",
    "#         axes[1, 0].set_title('Distribution of Test Reduction Ratios')\n",
    "#         axes[1, 0].set_xlabel('Reduction Ratio')\n",
    "#         axes[1, 0].set_ylabel('Frequency')\n",
    "        \n",
    "#         # Correlation plot\n",
    "#         axes[1, 1].scatter(successful_runs['original_tests'], successful_runs['coverage_percentage'])\n",
    "#         axes[1, 1].set_title('Original Tests vs Coverage')\n",
    "#         axes[1, 1].set_xlabel('Original Test Count')\n",
    "#         axes[1, 1].set_ylabel('Coverage Percentage')\n",
    "        \n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "#     else:\n",
    "#         print(\"❌ No successful runs to analyze\")\n",
    "    \n",
    "#     print(f\"📁 Detailed results saved to {output_file}\")\n",
    "#     return df\n",
    "\n",
    "# # Example batch evaluation with the corrected functions\n",
    "# example_functions = [example_function_1, example_function_2]\n",
    "# batch_results = batch_evaluate_functions(example_functions)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "TestGen Council",
   "language": "python",
   "name": "testgen-council"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
