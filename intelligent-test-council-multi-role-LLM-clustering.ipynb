{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdb33ae6-3526-471c-99c5-d1c32181b861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: numpy<2.0 in ./venv/lib/python3.9/site-packages (1.26.4)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 25.2 is available.\n",
      "You should consider upgrading via the '/Users/sepehr/IdeaProjects/testgen-council/venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: openai in ./venv/lib/python3.9/site-packages (1.108.1)\n",
      "Requirement already satisfied: transformers in ./venv/lib/python3.9/site-packages (4.56.2)\n",
      "Requirement already satisfied: scikit-learn in ./venv/lib/python3.9/site-packages (1.6.1)\n",
      "Requirement already satisfied: ast2json in ./venv/lib/python3.9/site-packages (0.4)\n",
      "Requirement already satisfied: pytest in ./venv/lib/python3.9/site-packages (8.4.2)\n",
      "Requirement already satisfied: coverage in ./venv/lib/python3.9/site-packages (7.10.7)\n",
      "Requirement already satisfied: pytest-cov in ./venv/lib/python3.9/site-packages (7.0.0)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.9/site-packages (2.3.2)\n",
      "Requirement already satisfied: matplotlib in ./venv/lib/python3.9/site-packages (3.9.4)\n",
      "Requirement already satisfied: seaborn in ./venv/lib/python3.9/site-packages (0.13.2)\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.9/site-packages (4.67.1)\n",
      "Requirement already satisfied: nest_asyncio in ./venv/lib/python3.9/site-packages (1.6.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./venv/lib/python3.9/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.9/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./venv/lib/python3.9/site-packages (from openai) (0.11.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in ./venv/lib/python3.9/site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./venv/lib/python3.9/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./venv/lib/python3.9/site-packages (from openai) (4.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./venv/lib/python3.9/site-packages (from openai) (2.11.9)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.9/site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.9/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.9/site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in ./venv/lib/python3.9/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./venv/lib/python3.9/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.9/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./venv/lib/python3.9/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.9/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./venv/lib/python3.9/site-packages (from transformers) (0.35.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./venv/lib/python3.9/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./venv/lib/python3.9/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./venv/lib/python3.9/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: tomli>=1; python_version < \"3.11\" in ./venv/lib/python3.9/site-packages (from pytest) (2.2.1)\n",
      "Requirement already satisfied: exceptiongroup>=1; python_version < \"3.11\" in ./venv/lib/python3.9/site-packages (from pytest) (1.3.0)\n",
      "Requirement already satisfied: pygments>=2.7.2 in ./venv/lib/python3.9/site-packages (from pytest) (2.19.2)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in ./venv/lib/python3.9/site-packages (from pytest) (1.6.0)\n",
      "Requirement already satisfied: iniconfig>=1 in ./venv/lib/python3.9/site-packages (from pytest) (2.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.9/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: pillow>=8 in ./venv/lib/python3.9/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./venv/lib/python3.9/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.9/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0; python_version < \"3.10\" in ./venv/lib/python3.9/site-packages (from matplotlib) (6.5.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.9/site-packages (from matplotlib) (4.60.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./venv/lib/python3.9/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: idna in ./venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./venv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./venv/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.9/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.9/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3; platform_machine == \"x86_64\" or platform_machine == \"amd64\" or platform_machine == \"arm64\" or platform_machine == \"aarch64\" in ./venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in ./venv/lib/python3.9/site-packages (from importlib-resources>=3.2.0; python_version < \"3.10\"->matplotlib) (3.23.0)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 25.2 is available.\n",
      "You should consider upgrading via the '/Users/sepehr/IdeaProjects/testgen-council/venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: ipywidgets in ./venv/lib/python3.9/site-packages (8.1.7)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./venv/lib/python3.9/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./venv/lib/python3.9/site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./venv/lib/python3.9/site-packages (from ipywidgets) (8.18.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in ./venv/lib/python3.9/site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in ./venv/lib/python3.9/site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack-data in ./venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in ./venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: exceptiongroup; python_version < \"3.11\" in ./venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (1.3.0)\n",
      "Requirement already satisfied: pexpect>4.3; sys_platform != \"win32\" in ./venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: typing-extensions; python_version < \"3.10\" in ./venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (4.15.0)\n",
      "Requirement already satisfied: decorator in ./venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: matplotlib-inline in ./venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: jedi>=0.16 in ./venv/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./venv/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in ./venv/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./venv/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: wcwidth in ./venv/lib/python3.9/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./venv/lib/python3.9/site-packages (from pexpect>4.3; sys_platform != \"win32\"->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./venv/lib/python3.9/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 25.2 is available.\n",
      "You should consider upgrading via the '/Users/sepehr/IdeaProjects/testgen-council/venv/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/bin/jupyter-nbextension\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/jupyter_core/application.py\", line 264, in launch_instance\n",
      "    return super(JupyterApp, cls).launch_instance(argv=argv, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/notebook/nbextensions.py\", line 980, in start\n",
      "    super().start()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/jupyter_core/application.py\", line 253, in start\n",
      "    self.subapp.start()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/notebook/nbextensions.py\", line 888, in start\n",
      "    self.toggle_nbextension_python(self.extra_args[0])\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/notebook/nbextensions.py\", line 861, in toggle_nbextension_python\n",
      "    return toggle(module,\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/notebook/nbextensions.py\", line 474, in enable_nbextension_python\n",
      "    return _set_nbextension_state_python(True, module, user, sys_prefix,\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/notebook/nbextensions.py\", line 372, in _set_nbextension_state_python\n",
      "    m, nbexts = _get_nbextension_metadata(module)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/notebook/nbextensions.py\", line 1114, in _get_nbextension_metadata\n",
      "    m = import_item(module)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/traitlets/utils/importstring.py\", line 38, in import_item\n",
      "    return __import__(parts[0])\n",
      "ModuleNotFoundError: No module named 'widgetsnbextension'\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Install Required Dependencies with Version Constraints\n",
    "!pip install \"numpy<2.0\" --upgrade\n",
    "!pip install openai transformers scikit-learn ast2json pytest coverage pytest-cov pandas matplotlib seaborn tqdm nest_asyncio\n",
    "!pip install ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db5552eb-bf52-4777-ac8b-a90a6472b7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.26.4\n",
      "Pandas version: 2.3.2\n",
      "Scikit-learn imported successfully\n",
      "OpenAI library version: 1.108.1\n",
      "Plotting libraries imported successfully\n",
      "tqdm imported successfully\n",
      "All imports completed!\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Import Required Libraries with Error Handling\n",
    "import ast\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "import tempfile\n",
    "from typing import List, Dict, Tuple, Any\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core data science imports\n",
    "try:\n",
    "    import numpy as np\n",
    "    print(f\"NumPy version: {np.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"NumPy import error: {e}\")\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "    print(f\"Pandas version: {pd.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"Pandas import error: {e}\")\n",
    "\n",
    "# ML and NLP imports\n",
    "try:\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    print(\"Scikit-learn imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"Scikit-learn import error: {e}\")\n",
    "\n",
    "# OpenAI import\n",
    "try:\n",
    "    import openai\n",
    "    print(f\"OpenAI library version: {openai.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"OpenAI import error: {e}\")\n",
    "\n",
    "# Plotting imports\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    plt.rcParams['figure.figsize'] = (12, 8)\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    print(\"Plotting libraries imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"Plotting libraries import error: {e}\")\n",
    "\n",
    "# Progress bar\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    print(\"tqdm imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"tqdm import error: {e}\")\n",
    "\n",
    "print(\"All imports completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74a6e284-f0df-44b4-a68a-16d89dfa588e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration loaded with role-based personas:\n",
      "   🎭 By-the-Book QA Engineer\n",
      "   🎭 Agent of Chaos\n",
      "   🎭 Paranoid Security Auditor\n",
      "   🎭 Abstract Thinker\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Configuration and API Setup\n",
    "SYNTHESIZER_MODEL = \"gemini-2.0-flash\"\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Configuration class for the intelligent council system\"\"\"\n",
    "    \n",
    "    # API Keys (replace with your actual keys)\n",
    "    OPENAI_API_KEY = \"sk-JdU36bC7BG2996XHH3YmKOQG8Xm9x9ii5u5E9uwPC54oAkHE\"\n",
    "    \n",
    "    # Base URLs for different providers\n",
    "    OPENAI_BASE_URL = \"https://api.gapgpt.app/v1\"  # Default OpenAI\n",
    "\n",
    "    \n",
    "    # Model configurations\n",
    "    LLM_MODELS = {\n",
    "        \"gemini-2.0-flash\": {\n",
    "            \"type\": \"openai\",\n",
    "            \"model_name\": \"gemini-2.0-flash\",\n",
    "            \"base_url\": OPENAI_BASE_URL,\n",
    "            \"api_key\": OPENAI_API_KEY,\n",
    "        },\n",
    "        \"grok-3-mini\": {\n",
    "            \"type\": \"openai\", \n",
    "            \"model_name\": \"grok-3-mini\",\n",
    "            \"base_url\": OPENAI_BASE_URL,\n",
    "            \"api_key\": OPENAI_API_KEY,\n",
    "        },\n",
    "        \"qwen3-235b-a22b\": {\n",
    "            \"type\": \"openai\",\n",
    "            \"model_name\": \"qwen3-235b-a22b\",\n",
    "            \"base_url\": OPENAI_BASE_URL,\n",
    "            \"api_key\": OPENAI_API_KEY,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Role-Based Test Generation Personas\n",
    "    ROLES = {\n",
    "        \"qa_engineer\": {\n",
    "            \"name\": \"By-the-Book QA Engineer\",\n",
    "            \"philosophy\": \"Meticulous and systematic. Focuses on covering the function's explicit requirements.\",\n",
    "            \"focus_categories\": [\"positive\", \"boundary\"],\n",
    "            \"prompt_persona\": \"\"\"You are a meticulous QA Engineer with 15 years of experience in software testing. Your primary goal is to verify that the function behaves exactly as described in its documentation.\n",
    "\n",
    "YOUR MISSION:\n",
    "- Generate high-quality, standard tests that cover the core functionality\n",
    "- Focus on positive test cases (normal, expected usage)\n",
    "- Test boundary conditions explicitly mentioned in the specification\n",
    "- Ensure every part of the docstring's promise is tested\n",
    "- Write clear, maintainable tests that serve as documentation\n",
    "\n",
    "APPROACH:\n",
    "1. Read the function signature and docstring carefully\n",
    "2. Identify all promised behaviors\n",
    "3. Create tests for typical use cases\n",
    "4. Test boundary values (min, max, empty, single element)\n",
    "5. Verify return types and value ranges match specifications\n",
    "\n",
    "Generate well-structured tests following pytest best practices.\"\"\"\n",
    "        },\n",
    "        \n",
    "        \"agent_of_chaos\": {\n",
    "            \"name\": \"Agent of Chaos\",\n",
    "            \"philosophy\": \"If it can break, I will find a way. Make the function fail.\",\n",
    "            \"focus_categories\": [\"negative\", \"edge_case\"],\n",
    "            \"prompt_persona\": \"\"\"You are a destructive tester known as the \"Agent of Chaos\". Your mission is to BREAK this function by any means necessary.\n",
    "\n",
    "YOUR MISSION:\n",
    "- Find every possible way the function can fail\n",
    "- Generate tests that SHOULD raise exceptions\n",
    "- Think about unexpected, malformed, or adversarial inputs\n",
    "- Test with wrong types, None values, empty data structures\n",
    "- Push the function beyond its limits\n",
    "\n",
    "ATTACK VECTORS TO CONSIDER:\n",
    "1. Type violations (pass string when int expected, etc.)\n",
    "2. Null/None inputs where objects are expected\n",
    "3. Empty collections ([], {}, \"\")\n",
    "4. Extreme values (very large numbers, very long strings)\n",
    "5. Negative numbers where positive expected\n",
    "6. Zero division scenarios\n",
    "7. Invalid combinations of parameters\n",
    "8. Corrupted or malformed data structures\n",
    "\n",
    "Generate tests that you expect will raise specific exceptions (TypeError, ValueError, IndexError, ZeroDivisionError, etc.). Use pytest.raises() to verify these failures.\"\"\"\n",
    "        },\n",
    "        \n",
    "        \"security_auditor\": {\n",
    "            \"name\": \"Paranoid Security Auditor\",\n",
    "            \"philosophy\": \"Trust nothing. Assume all input is hostile.\",\n",
    "            \"focus_categories\": [\"security\", \"negative\"],\n",
    "            \"prompt_persona\": \"\"\"You are a cybersecurity expert and penetration tester. Your task is to find security vulnerabilities in this code.\n",
    "\n",
    "YOUR MISSION:\n",
    "- Analyze the function for potential security flaws\n",
    "- Generate tests that attempt to exploit vulnerabilities\n",
    "- Think like an attacker trying to compromise the system\n",
    "\n",
    "SECURITY CONCERNS TO TEST:\n",
    "1. **Injection Attacks**: SQL injection, command injection, code injection\n",
    "2. **Path Traversal**: Attempts to access files outside intended directory (../, absolute paths)\n",
    "3. **Buffer Overflow**: Oversized inputs that might cause issues\n",
    "4. **Format String Attacks**: Special characters in strings (%s, %d, {}, etc.)\n",
    "5. **Insecure Deserialization**: Malicious pickled objects or JSON\n",
    "6. **Input Validation Bypass**: Special characters, Unicode, null bytes\n",
    "7. **Resource Exhaustion**: Inputs that could cause infinite loops or memory issues\n",
    "8. **Data Leakage**: Can the function expose sensitive information?\n",
    "\n",
    "Generate security-focused tests. If the function has file operations, test path traversal. If it processes strings, test injection. If it handles numbers, test integer overflow. If no obvious vulnerabilities exist, test with security-minded inputs (special characters, scripts, oversized data).\"\"\"\n",
    "        },\n",
    "        \n",
    "        \"abstract_thinker\": {\n",
    "            \"name\": \"Abstract Thinker\",\n",
    "            \"philosophy\": \"Test the underlying properties and invariants, not just specific cases.\",\n",
    "            \"focus_categories\": [\"positive\", \"boundary\", \"edge_case\"],\n",
    "            \"prompt_persona\": \"\"\"You are a computer scientist specializing in formal methods and property-based testing. Your goal is to verify the fundamental mathematical and logical properties of this function.\n",
    "\n",
    "YOUR MISSION:\n",
    "- Think beyond specific test cases to general properties\n",
    "- Identify invariants that must always hold\n",
    "- Create tests that verify logical consistency\n",
    "- Check mathematical properties and relationships\n",
    "\n",
    "PROPERTIES TO CONSIDER:\n",
    "1. **Identity Properties**: f(x) with some operation returns x\n",
    "2. **Inverse Properties**: decode(encode(x)) == x\n",
    "3. **Idempotency**: f(f(x)) == f(x) for some functions\n",
    "4. **Commutativity**: Does order matter? f(a,b) == f(b,a)?\n",
    "5. **Associativity**: f(f(a,b),c) == f(a,f(b,c))?\n",
    "6. **Preservation Properties**: Input length = output length?\n",
    "7. **Boundary Properties**: For sorted output, output[i] <= output[i+1]\n",
    "8. **Type Invariants**: Output type consistent with specification?\n",
    "9. **Domain/Range Properties**: All outputs within valid range?\n",
    "\n",
    "Generate property-based tests. You may use standard pytest format or suggest hypothesis library tests. Focus on testing fundamental truths about the function's behavior rather than specific input-output pairs.\"\"\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Model-Role Assignment Strategy\n",
    "    # This assigns each model to specific roles based on hypothesized strengths\n",
    "    # You can modify this based on your experimental results\n",
    "\n",
    "    # MODEL_ROLE_ASSIGNMENTS = {\n",
    "    #     \"gemini-2.0-flash\": [\"qa_engineer\", \"abstract_thinker\"],\n",
    "    #     \"grok-3-mini\": [\"agent_of_chaos\", \"security_auditor\"],\n",
    "    #     \"qwen3-235b-a22b\": [\"qa_engineer\", \"agent_of_chaos\"]\n",
    "    # }\n",
    "    \n",
    "    MODEL_ROLE_ASSIGNMENTS = {\n",
    "        \"gemini-2.0-flash\": [\"qa_engineer\", \"abstract_thinker\", \"agent_of_chaos\"],\n",
    "        \"grok-3-mini\": [\"qa_engineer\", \"agent_of_chaos\"],\n",
    "        \"qwen3-235b-a22b\": [\"abstract_thinker\", \"security_auditor\"]\n",
    "    }\n",
    "    \n",
    "    # Test categories (kept for backward compatibility)\n",
    "    TEST_CATEGORIES = [\n",
    "        \"positive\",    # مثبت - حالات عادی\n",
    "        \"negative\",    # منفی - حالات خطا\n",
    "        \"boundary\",    # مرزی - مقادیر حدی\n",
    "        \"edge_case\",   # موارد استثنایی\n",
    "        \"security\"     # امنیتی\n",
    "    ]\n",
    "\n",
    "# Initialize configuration\n",
    "config = Config()\n",
    "\n",
    "# Setup API clients\n",
    "if config.OPENAI_API_KEY != \"sk-JdU36bC7BG2996XHH3YmKOQG8Xm9x9ii5u5E9uwPC54oAkHE\":\n",
    "    openai.api_key = config.OPENAI_API_KEY\n",
    "\n",
    "print(\"✅ Configuration loaded with role-based personas:\")\n",
    "for role_id, role_info in config.ROLES.items():\n",
    "    print(f\"   🎭 {role_info['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60fc4cd7-986a-45a1-ad20-1d79a851dda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Code Analysis and AST Processing Module\n",
    "class CodeAnalyzer:\n",
    "    \"\"\"Analyzes Python code and extracts function information using AST\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_function_info(code: str) -> Dict[str, Any]:\n",
    "        \"\"\"Extract function information from Python code\"\"\"\n",
    "        try:\n",
    "            # Clean up the code string and ensure proper formatting\n",
    "            code = code.strip()\n",
    "            \n",
    "            # Try to parse with ast\n",
    "            tree = ast.parse(code)\n",
    "            functions = []\n",
    "            \n",
    "            for node in ast.walk(tree):\n",
    "                if isinstance(node, ast.FunctionDef):\n",
    "                    # Get function source by reconstructing from lines\n",
    "                    lines = code.split('\\n')\n",
    "                    start_line = node.lineno - 1\n",
    "                    end_line = node.end_lineno if hasattr(node, 'end_lineno') else len(lines)\n",
    "                    \n",
    "                    func_source = '\\n'.join(lines[start_line:end_line])\n",
    "                    \n",
    "                    func_info = {\n",
    "                        'name': node.name,\n",
    "                        'args': [arg.arg for arg in node.args.args],\n",
    "                        'docstring': ast.get_docstring(node),\n",
    "                        'source_code': func_source,\n",
    "                        'line_start': node.lineno,\n",
    "                        'line_end': node.end_lineno if hasattr(node, 'end_lineno') else len(lines)\n",
    "                    }\n",
    "                    functions.append(func_info)\n",
    "            \n",
    "            return {\n",
    "                'functions': functions,\n",
    "                'total_functions': len(functions),\n",
    "                'source_code': code\n",
    "            }\n",
    "            \n",
    "        except SyntaxError as e:\n",
    "            print(f\"Syntax error parsing code: {e}\")\n",
    "            print(f\"Error at line {e.lineno}: {e.text}\")\n",
    "            print(f\"Code that failed to parse:\\n{code}\")\n",
    "            return {'functions': [], 'total_functions': 0, 'source_code': code, 'syntax_error': str(e)}\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing code: {e}\")\n",
    "            return {'functions': [], 'total_functions': 0, 'source_code': code, 'error': str(e)}\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_test_methods_from_response(response: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Extract individual test methods from LLM response\"\"\"\n",
    "        test_methods = []\n",
    "        \n",
    "        # Try to find test functions using regex\n",
    "        test_pattern = r'def (test_\\w+)\\([^)]*\\):(.*?)(?=def test_|\\Z)'\n",
    "        matches = re.findall(test_pattern, response, re.DOTALL)\n",
    "        \n",
    "        for match in matches:\n",
    "            func_name, func_body = match\n",
    "            full_test = f\"def {func_name}():{func_body}\"\n",
    "            test_methods.append({\n",
    "                'name': func_name,\n",
    "                'code': full_test.strip()\n",
    "            })\n",
    "        \n",
    "        return test_methods\n",
    "\n",
    "# Initialize code analyzer\n",
    "code_analyzer = CodeAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c80c0ea7-4ed2-4512-82d7-0523fe8d4e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: LLM Council Module (Role-Based Version with Concurrent API Support)\n",
    "import asyncio\n",
    "from typing import Dict, Any, List, Tuple\n",
    "import openai\n",
    "\n",
    "class LLMCouncil:\n",
    "    \"\"\"Manages multiple LLM models with specialized roles for test case generation\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.models = config.LLM_MODELS\n",
    "        self.roles = config.ROLES\n",
    "        self.model_role_assignments = config.MODEL_ROLE_ASSIGNMENTS\n",
    "        self.client = openai.OpenAI(\n",
    "            base_url=config.OPENAI_BASE_URL,\n",
    "            api_key=config.OPENAI_API_KEY\n",
    "        )\n",
    "        self.async_client = openai.AsyncOpenAI(\n",
    "            base_url=config.OPENAI_BASE_URL,\n",
    "            api_key=config.OPENAI_API_KEY\n",
    "        )\n",
    "        \n",
    "    def create_role_based_prompt(self, function_info: Dict[str, Any], role_id: str) -> str:\n",
    "        \"\"\"Create a role-specific prompt for test case generation\"\"\"\n",
    "        func = function_info['functions'][0] if function_info['functions'] else {}\n",
    "        role = self.roles[role_id]\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "{role['prompt_persona']}\n",
    "\n",
    "YOUR ROLE: \"{role['name']}\"\n",
    "PHILOSOPHY: {role['philosophy']}\n",
    "\n",
    "FUNCTION TO TEST:\n",
    "```python\n",
    "{func.get('source_code', function_info['source_code'])}\n",
    "\n",
    "FUNCTION DETAILS:\n",
    "- Name: {func.get('name', 'unknown')}\n",
    "- Parameters: {', '.join(func.get('args', [])) if func.get('args') else 'None'}\n",
    "- Docstring: {func.get('docstring', 'No docstring provided')}\n",
    "\n",
    "REQUIREMENTS:\n",
    "1. Stay true to your role as \"{role['name']}\" - let your {role['philosophy'].lower()} guide your test design\n",
    "2. Focus on test categories: {', '.join(role['focus_categories'])}\n",
    "3. Label each test with a category comment from the definitions below:\n",
    "\n",
    "**CATEGORY DEFINITIONS:**\n",
    "\n",
    "**# Category: positive**\n",
    "Valid, typical inputs representing normal usage. Tests the \"happy path\" to confirm the function fulfills its contract.\n",
    "Examples: sort([3,1,2]), add(5,3), valid user credentials\n",
    "\n",
    "**# Category: negative**\n",
    "Invalid inputs that SHOULD raise exceptions. Tests graceful failure handling. MUST use pytest.raises().\n",
    "Examples: divide(5,0) → ZeroDivisionError, int(\"abc\") → ValueError, accessing non-existent files\n",
    "Key: Tests error handling, not malicious exploitation (that's security)\n",
    "\n",
    "**# Category: boundary**\n",
    "Values at the LIMITS of valid ranges where behavior might change. Tests threshold values and off-by-one errors.\n",
    "Examples: For range [1,100] test: 0, 1, 100, 101; empty list vs single element; MIN_INT/MAX_INT\n",
    "Formula: If valid range is [a,b], test: a-1, a, a+1, b-1, b, b+1\n",
    "\n",
    "**# Category: edge_case**\n",
    "VALID but UNUSUAL scenarios - rare but legitimate use cases that might be overlooked.\n",
    "Examples: already-sorted lists, all duplicates [5,5,5,5], negative indices, float('inf'), unicode \"emoji😊\"\n",
    "Key: Unusual but still valid inputs, not boundaries of ranges\n",
    "\n",
    "**# Category: security**\n",
    "MALICIOUS/ADVERSARIAL inputs testing exploitation resistance. Focus on attack vectors and vulnerabilities.\n",
    "Examples: SQL injection \"'; DROP TABLE;--\", path traversal \"../../../etc/passwd\", XSS \"<script>\", command injection \"; rm -rf /\", extremely long strings (DoS)\n",
    "Key: Testing if function can be exploited, not just validation (that's negative tests)\n",
    "\n",
    "4. Use pytest format with descriptive test names\n",
    "5. Include clear assertions with meaningful error messages\n",
    "6. Add docstrings explaining what each test verifies\n",
    "\n",
    "CRITICAL: Your tests must reflect your role's philosophy: {role['philosophy']}\n",
    "Your unique perspective as \"{role['name']}\" should be evident in test selection and design.\n",
    "\n",
    "EXAMPLE FORMAT:\n",
    "\n",
    "python\n",
    "import pytest\n",
    "\n",
    "def test_function_name_descriptive_scenario():\n",
    "    '''Clear description of what this test verifies'''\n",
    "    # Category: [appropriate category]\n",
    "    # Your test implementation here\n",
    "    result = function_name(test_input)\n",
    "    assert result == expected, \"Clear assertion message\"\n",
    "\n",
    "Generate your role-specific tests now:\n",
    "\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def call_openai_model(self, prompt: str, model_config: Dict) -> str:\n",
    "        \"\"\"Call OpenAI API (synchronous version)\"\"\"\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=model_config[\"model_name\"],\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"Error calling OpenAI API: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    async def call_openai_model_async(self, prompt: str, model_config: Dict, \n",
    "                                      model_name: str, role_id: str) -> Tuple[str, str, str]:\n",
    "        \"\"\"Call OpenAI API asynchronously with tracking info\"\"\"\n",
    "        try:\n",
    "            response = await self.async_client.chat.completions.create(\n",
    "                model=model_config[\"model_name\"],\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "            )\n",
    "            return (model_name, role_id, response.choices[0].message.content)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error calling {model_name} for role {role_id}: {e}\")\n",
    "            return (model_name, role_id, \"\")\n",
    "\n",
    "    def generate_tests_from_council(self, function_info: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Generate test cases using role-based assignments (synchronous version)\"\"\"\n",
    "        council_results = {}\n",
    "        \n",
    "        print(\"🤖 Consulting Role-Based LLM Council for test generation...\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Calculate total tasks for progress bar\n",
    "        total_tasks = sum(len(roles) for roles in self.model_role_assignments.values())\n",
    "        \n",
    "        with tqdm(total=total_tasks, desc=\"Generating role-based tests\") as pbar:\n",
    "            for model_name, assigned_roles in self.model_role_assignments.items():\n",
    "                if model_name not in self.models:\n",
    "                    print(f\"⚠️  Warning: Model {model_name} not found in configuration\")\n",
    "                    continue\n",
    "                \n",
    "                model_config = self.models[model_name]\n",
    "                model_results = {}\n",
    "                \n",
    "                for role_id in assigned_roles:\n",
    "                    if role_id not in self.roles:\n",
    "                        print(f\"⚠️  Warning: Role {role_id} not defined\")\n",
    "                        continue\n",
    "                    \n",
    "                    role = self.roles[role_id]\n",
    "                    \n",
    "                    try:\n",
    "                        prompt = self.create_role_based_prompt(function_info, role_id)\n",
    "                        \n",
    "                        if model_config[\"type\"] == \"openai\":\n",
    "                            response = self.call_openai_model(prompt, model_config)\n",
    "                        else:\n",
    "                            response = \"\"\n",
    "                        \n",
    "                        test_methods = code_analyzer.extract_test_methods_from_response(response)\n",
    "                        \n",
    "                        model_results[role_id] = {\n",
    "                            'role_name': role['name'],\n",
    "                            'raw_response': response,\n",
    "                            'test_methods': test_methods,\n",
    "                            'test_count': len(test_methods),\n",
    "                            'focus_categories': role['focus_categories']\n",
    "                        }\n",
    "                        \n",
    "                        print(f\"✅ {model_name} as '{role['name']}': {len(test_methods)} tests\")\n",
    "                        pbar.update(1)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"❌ Error with {model_name} in role {role_id}: {e}\")\n",
    "                        model_results[role_id] = {\n",
    "                            'role_name': role['name'],\n",
    "                            'raw_response': \"\",\n",
    "                            'test_methods': [],\n",
    "                            'test_count': 0,\n",
    "                            'focus_categories': role['focus_categories']\n",
    "                        }\n",
    "                        pbar.update(1)\n",
    "                \n",
    "                council_results[model_name] = model_results\n",
    "        \n",
    "        print(f\"{'='*70}\")\n",
    "        return council_results\n",
    "    \n",
    "    async def generate_tests_from_council_async(self, function_info: Dict[str, Any], \n",
    "                                                 max_concurrent: int = 7) -> Dict[str, Any]:\n",
    "        \"\"\"Generate test cases using role-based assignments with concurrent API calls\"\"\"\n",
    "        print(\"🤖 Consulting Role-Based LLM Council for test generation (Concurrent Mode)...\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"⚡ Maximum concurrent requests: {max_concurrent}\")\n",
    "        \n",
    "        # Create a semaphore to limit concurrent requests\n",
    "        semaphore = asyncio.Semaphore(max_concurrent)\n",
    "        \n",
    "        # Prepare all tasks\n",
    "        tasks = []\n",
    "        task_metadata = []  # To track which task belongs to which model-role pair\n",
    "        \n",
    "        for model_name, assigned_roles in self.model_role_assignments.items():\n",
    "            if model_name not in self.models:\n",
    "                print(f\"⚠️  Warning: Model {model_name} not found in configuration\")\n",
    "                continue\n",
    "            \n",
    "            model_config = self.models[model_name]\n",
    "            \n",
    "            for role_id in assigned_roles:\n",
    "                if role_id not in self.roles:\n",
    "                    print(f\"⚠️  Warning: Role {role_id} not defined\")\n",
    "                    continue\n",
    "                \n",
    "                role = self.roles[role_id]\n",
    "                \n",
    "                # Create prompt\n",
    "                prompt = self.create_role_based_prompt(function_info, role_id)\n",
    "                \n",
    "                # Create async task with semaphore\n",
    "                async def bounded_call(sem, p, mc, mn, rid):\n",
    "                    async with sem:\n",
    "                        return await self.call_openai_model_async(p, mc, mn, rid)\n",
    "                \n",
    "                task = bounded_call(semaphore, prompt, model_config, model_name, role_id)\n",
    "                tasks.append(task)\n",
    "                task_metadata.append({\n",
    "                    'model_name': model_name,\n",
    "                    'role_id': role_id,\n",
    "                    'role_name': role['name'],\n",
    "                    'focus_categories': role['focus_categories']\n",
    "                })\n",
    "        \n",
    "        total_tasks = len(tasks)\n",
    "        print(f\"📊 Total API calls to make: {total_tasks}\")\n",
    "        \n",
    "        # Execute all tasks concurrently with progress tracking\n",
    "        results = []\n",
    "        with tqdm(total=total_tasks, desc=\"Concurrent API calls\") as pbar:\n",
    "            # Use asyncio.gather to run all tasks\n",
    "            for coro in asyncio.as_completed(tasks):\n",
    "                result = await coro\n",
    "                results.append(result)\n",
    "                pbar.update(1)\n",
    "        \n",
    "        # Organize results back into the expected structure\n",
    "        council_results = {}\n",
    "        \n",
    "        for (model_name, role_id, response), metadata in zip(results, task_metadata):\n",
    "            # Ensure model_name matches metadata (it should)\n",
    "            if model_name not in council_results:\n",
    "                council_results[model_name] = {}\n",
    "            \n",
    "            # Extract test methods from response\n",
    "            test_methods = code_analyzer.extract_test_methods_from_response(response)\n",
    "            \n",
    "            council_results[model_name][role_id] = {\n",
    "                'role_name': metadata['role_name'],\n",
    "                'raw_response': response,\n",
    "                'test_methods': test_methods,\n",
    "                'test_count': len(test_methods),\n",
    "                'focus_categories': metadata['focus_categories']\n",
    "            }\n",
    "            \n",
    "            print(f\"✅ {model_name} as '{metadata['role_name']}': {len(test_methods)} tests\")\n",
    "        \n",
    "        print(f\"{'='*70}\")\n",
    "        return council_results\n",
    "\n",
    "# Initialize LLM Council\n",
    "llm_council = LLMCouncil(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59b58fba-2980-4725-b916-9a9cefcaffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Test Classification Module (Enhanced for Role Tracking)\n",
    "class TestClassifier:\n",
    "    \"\"\"Classifies test cases by category and tracks role assignments\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_category_from_test(test_code: str) -> str:\n",
    "        \"\"\"Extract category from test code comments\"\"\"\n",
    "        category_pattern = r'#\\s*Category:\\s*(\\w+)'\n",
    "        match = re.search(category_pattern, test_code, re.IGNORECASE)\n",
    "        \n",
    "        if match:\n",
    "            category = match.group(1).lower()\n",
    "            if category in config.TEST_CATEGORIES:\n",
    "                return category\n",
    "        \n",
    "        # Fallback classification based on test name and content\n",
    "        test_code_lower = test_code.lower()\n",
    "        \n",
    "        if 'error' in test_code_lower or 'exception' in test_code_lower or 'invalid' in test_code_lower:\n",
    "            return 'negative'\n",
    "        elif 'boundary' in test_code_lower or 'edge' in test_code_lower or 'limit' in test_code_lower:\n",
    "            return 'boundary'\n",
    "        elif 'security' in test_code_lower or 'auth' in test_code_lower or 'injection' in test_code_lower:\n",
    "            return 'security'\n",
    "        else:\n",
    "            return 'positive'\n",
    "    \n",
    "    @staticmethod\n",
    "    def classify_council_results(council_results: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Classify all test cases from council results with role information\"\"\"\n",
    "        all_classified_tests = []\n",
    "        \n",
    "        for model_name, role_results in council_results.items():\n",
    "            for role_id, results in role_results.items():\n",
    "                for test in results['test_methods']:\n",
    "                    category = TestClassifier.extract_category_from_test(test['code'])\n",
    "                    classified_test = test.copy()\n",
    "                    classified_test['category'] = category\n",
    "                    classified_test['source_model'] = model_name\n",
    "                    classified_test['source_role'] = role_id\n",
    "                    classified_test['role_name'] = results['role_name']\n",
    "                    all_classified_tests.append(classified_test)\n",
    "        \n",
    "        return all_classified_tests\n",
    "\n",
    "# Initialize classifier\n",
    "test_classifier = TestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78b32601-5eda-4908-a881-0aec8924b5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Hybrid Cluster-then-Synthesize system initialized!\n",
      "   🔬 AST-based structural clustering\n",
      "   🤖 LLM-powered cluster synthesis\n",
      "   ✨ LLM-powered final test file generation\n",
      "   📊 Two clustering methods available: 'hash' (fast) and 'vector' (advanced)\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: AST-Based Clustering and Enhanced Test Synthesizer Module (Updated)\n",
    "import ast\n",
    "import hashlib\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import numpy as np\n",
    "\n",
    "class ASTNormalizer:\n",
    "    \"\"\"Normalizes AST for structural comparison\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.var_counter = 0\n",
    "        self.var_map = {}\n",
    "    \n",
    "    def normalize_ast(self, tree: ast.AST) -> ast.AST:\n",
    "        \"\"\"Normalize an AST by anonymizing variables and canonicalizing structure\"\"\"\n",
    "        self.var_counter = 0\n",
    "        self.var_map = {}\n",
    "        return self._normalize_node(tree)\n",
    "    \n",
    "    def _normalize_node(self, node: ast.AST) -> ast.AST:\n",
    "        \"\"\"Recursively normalize AST nodes\"\"\"\n",
    "        if isinstance(node, ast.Name):\n",
    "            # Anonymize variable names\n",
    "            if node.id not in self.var_map:\n",
    "                self.var_map[node.id] = f\"var_{self.var_counter}\"\n",
    "                self.var_counter += 1\n",
    "            node.id = self.var_map[node.id]\n",
    "        \n",
    "        elif isinstance(node, ast.FunctionDef):\n",
    "            # Normalize function names (except test functions)\n",
    "            if not node.name.startswith('test_'):\n",
    "                node.name = \"func\"\n",
    "            # Remove docstrings\n",
    "            if (node.body and isinstance(node.body[0], ast.Expr) and\n",
    "                isinstance(node.body[0].value, (ast.Str, ast.Constant))):\n",
    "                node.body = node.body[1:]\n",
    "        \n",
    "        elif isinstance(node, (ast.List, ast.Tuple, ast.Set)):\n",
    "            # Sort literals in collections for canonicalization (if all are constants)\n",
    "            try:\n",
    "                if all(isinstance(elt, ast.Constant) for elt in node.elts):\n",
    "                    node.elts = sorted(node.elts, key=lambda x: str(x.value))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Recursively process child nodes\n",
    "        for field, value in ast.iter_fields(node):\n",
    "            if isinstance(value, list):\n",
    "                for item in value:\n",
    "                    if isinstance(item, ast.AST):\n",
    "                        self._normalize_node(item)\n",
    "            elif isinstance(value, ast.AST):\n",
    "                self._normalize_node(value)\n",
    "        \n",
    "        return node\n",
    "\n",
    "\n",
    "class ASTClusterer:\n",
    "    \"\"\"Clusters test functions based on structural similarity using AST analysis\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.normalizer = ASTNormalizer()\n",
    "    \n",
    "    def parse_test_to_ast(self, test_code: str) -> Tuple[ast.AST, bool]:\n",
    "        \"\"\"Parse test code to AST, return (tree, success)\"\"\"\n",
    "        try:\n",
    "            tree = ast.parse(test_code)\n",
    "            return tree, True\n",
    "        except SyntaxError as e:\n",
    "            print(f\"⚠️  Syntax error in test code: {e}\")\n",
    "            return None, False\n",
    "    \n",
    "    def get_structural_hash(self, test_code: str) -> str:\n",
    "        \"\"\"Generate structural hash from normalized AST\"\"\"\n",
    "        tree, success = self.parse_test_to_ast(test_code)\n",
    "        if not success or tree is None:\n",
    "            return hashlib.md5(test_code.encode()).hexdigest()\n",
    "        \n",
    "        # Normalize the AST\n",
    "        normalized_tree = self.normalizer.normalize_ast(tree)\n",
    "        \n",
    "        # Convert to string and hash\n",
    "        ast_str = ast.dump(normalized_tree, annotate_fields=False)\n",
    "        return hashlib.md5(ast_str.encode()).hexdigest()\n",
    "    \n",
    "    def vectorize_ast(self, test_code: str) -> np.ndarray:\n",
    "        \"\"\"Convert AST to numerical feature vector\"\"\"\n",
    "        tree, success = self.parse_test_to_ast(test_code)\n",
    "        if not success or tree is None:\n",
    "            return np.zeros(20)\n",
    "        \n",
    "        # Normalize the AST\n",
    "        normalized_tree = self.normalizer.normalize_ast(tree)\n",
    "        \n",
    "        # Extract structural features\n",
    "        features = {\n",
    "            'num_nodes': 0,\n",
    "            'num_functions': 0,\n",
    "            'num_calls': 0,\n",
    "            'num_asserts': 0,\n",
    "            'num_comparisons': 0,\n",
    "            'num_binops': 0,\n",
    "            'num_unaryops': 0,\n",
    "            'num_if': 0,\n",
    "            'num_for': 0,\n",
    "            'num_while': 0,\n",
    "            'num_with': 0,\n",
    "            'num_try': 0,\n",
    "            'num_raise': 0,\n",
    "            'max_depth': 0,\n",
    "            'num_literals': 0,\n",
    "            'num_list': 0,\n",
    "            'num_dict': 0,\n",
    "            'num_tuple': 0,\n",
    "            'has_pytest_raises': 0,\n",
    "            'num_attributes': 0\n",
    "        }\n",
    "        \n",
    "        def count_nodes(node, depth=0):\n",
    "            features['num_nodes'] += 1\n",
    "            features['max_depth'] = max(features['max_depth'], depth)\n",
    "            \n",
    "            if isinstance(node, ast.FunctionDef):\n",
    "                features['num_functions'] += 1\n",
    "            elif isinstance(node, ast.Call):\n",
    "                features['num_calls'] += 1\n",
    "                # Check for pytest.raises\n",
    "                if isinstance(node.func, ast.Attribute):\n",
    "                    if node.func.attr == 'raises':\n",
    "                        features['has_pytest_raises'] = 1\n",
    "            elif isinstance(node, ast.Assert):\n",
    "                features['num_asserts'] += 1\n",
    "            elif isinstance(node, ast.Compare):\n",
    "                features['num_comparisons'] += 1\n",
    "            elif isinstance(node, ast.BinOp):\n",
    "                features['num_binops'] += 1\n",
    "            elif isinstance(node, ast.UnaryOp):\n",
    "                features['num_unaryops'] += 1\n",
    "            elif isinstance(node, ast.If):\n",
    "                features['num_if'] += 1\n",
    "            elif isinstance(node, ast.For):\n",
    "                features['num_for'] += 1\n",
    "            elif isinstance(node, ast.While):\n",
    "                features['num_while'] += 1\n",
    "            elif isinstance(node, ast.With):\n",
    "                features['num_with'] += 1\n",
    "            elif isinstance(node, ast.Try):\n",
    "                features['num_try'] += 1\n",
    "            elif isinstance(node, ast.Raise):\n",
    "                features['num_raise'] += 1\n",
    "            elif isinstance(node, (ast.Constant, ast.Num, ast.Str)):\n",
    "                features['num_literals'] += 1\n",
    "            elif isinstance(node, ast.List):\n",
    "                features['num_list'] += 1\n",
    "            elif isinstance(node, ast.Dict):\n",
    "                features['num_dict'] += 1\n",
    "            elif isinstance(node, ast.Tuple):\n",
    "                features['num_tuple'] += 1\n",
    "            elif isinstance(node, ast.Attribute):\n",
    "                features['num_attributes'] += 1\n",
    "            \n",
    "            for child in ast.iter_child_nodes(node):\n",
    "                count_nodes(child, depth + 1)\n",
    "        \n",
    "        count_nodes(normalized_tree)\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        return np.array(list(features.values()), dtype=float)\n",
    "    \n",
    "    def cluster_tests(self, tests: List[Dict[str, Any]], \n",
    "                     method: str = 'vector', \n",
    "                     eps: float = 0.3, \n",
    "                     min_samples: int = 2) -> Dict[int, List[int]]:\n",
    "        \"\"\"\n",
    "        Cluster tests based on structural similarity\n",
    "        \n",
    "        Args:\n",
    "            tests: List of test dictionaries with 'code' field\n",
    "            method: 'hash' for simple hashing, 'vector' for feature-based clustering\n",
    "            eps: DBSCAN epsilon parameter (for vector method)\n",
    "            min_samples: DBSCAN min_samples parameter (for vector method)\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary mapping cluster_id to list of test indices\n",
    "        \"\"\"\n",
    "        print(f\"🔬 Clustering {len(tests)} tests using {method} method...\")\n",
    "        \n",
    "        if method == 'hash':\n",
    "            return self._cluster_by_hash(tests)\n",
    "        elif method == 'vector':\n",
    "            return self._cluster_by_vector(tests, eps, min_samples)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown clustering method: {method}\")\n",
    "    \n",
    "    def _cluster_by_hash(self, tests: List[Dict[str, Any]]) -> Dict[int, List[int]]:\n",
    "        \"\"\"Fast clustering using structural hashes\"\"\"\n",
    "        hash_to_indices = {}\n",
    "        \n",
    "        for idx, test in enumerate(tests):\n",
    "            struct_hash = self.get_structural_hash(test['code'])\n",
    "            if struct_hash not in hash_to_indices:\n",
    "                hash_to_indices[struct_hash] = []\n",
    "            hash_to_indices[struct_hash].append(idx)\n",
    "        \n",
    "        # Convert to cluster format\n",
    "        clusters = {}\n",
    "        for cluster_id, indices in enumerate(hash_to_indices.values()):\n",
    "            clusters[cluster_id] = indices\n",
    "        \n",
    "        # Calculate statistics\n",
    "        singleton_clusters = sum(1 for indices in clusters.values() if len(indices) == 1)\n",
    "        multi_test_clusters = len(clusters) - singleton_clusters\n",
    "        \n",
    "        print(f\"✅ Hash-based clustering complete:\")\n",
    "        print(f\"   • Total clusters: {len(clusters)}\")\n",
    "        print(f\"   • Singleton clusters (unique tests): {singleton_clusters}\")\n",
    "        print(f\"   • Multi-test clusters (duplicates found): {multi_test_clusters}\")\n",
    "        \n",
    "        return clusters\n",
    "    \n",
    "    def _cluster_by_vector(self, tests: List[Dict[str, Any]], \n",
    "                          eps: float, min_samples: int) -> Dict[int, List[int]]:\n",
    "        \"\"\"Advanced clustering using AST feature vectors\"\"\"\n",
    "        # Vectorize all tests\n",
    "        vectors = np.array([self.vectorize_ast(test['code']) for test in tests])\n",
    "        \n",
    "        # Normalize features\n",
    "        scaler = StandardScaler()\n",
    "        vectors_normalized = scaler.fit_transform(vectors)\n",
    "        \n",
    "        # Cluster with DBSCAN\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='euclidean')\n",
    "        labels = dbscan.fit_predict(vectors_normalized)\n",
    "        \n",
    "        # Organize by cluster\n",
    "        clusters = {}\n",
    "        for idx, label in enumerate(labels):\n",
    "            if label == -1:  # Noise points get individual clusters\n",
    "                label = max(clusters.keys(), default=-1) + 1\n",
    "            if label not in clusters:\n",
    "                clusters[label] = []\n",
    "            clusters[label].append(idx)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        singleton_clusters = sum(1 for indices in clusters.values() if len(indices) == 1)\n",
    "        multi_test_clusters = len(clusters) - singleton_clusters\n",
    "        avg_cluster_size = np.mean([len(indices) for indices in clusters.values()])\n",
    "        \n",
    "        print(f\"✅ Vector-based clustering complete:\")\n",
    "        print(f\"   • Total clusters: {len(clusters)}\")\n",
    "        print(f\"   • Singleton clusters: {singleton_clusters}\")\n",
    "        print(f\"   • Multi-test clusters: {multi_test_clusters}\")\n",
    "        print(f\"   • Average cluster size: {avg_cluster_size:.2f}\")\n",
    "        \n",
    "        return clusters\n",
    "\n",
    "\n",
    "class TestSynthesizer:\n",
    "    \"\"\"\n",
    "    Synthesizes final optimized test file using hybrid Cluster-then-Synthesize approach\n",
    "    Combines AST-based structural clustering with LLM-powered semantic synthesis\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm_council: LLMCouncil):\n",
    "        self.llm_council = llm_council\n",
    "        self.clusterer = ASTClusterer()\n",
    "        self.finalizer_model = \"gemini-2.0-flash\"\n",
    "    \n",
    "    def synthesize_final_test_file(self, all_tests: List[Dict], function_info: Dict,\n",
    "                                   clustering_method: str = 'vector') -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Synthesize final test file using Cluster-then-Synthesize approach\n",
    "        \n",
    "        Args:\n",
    "            all_tests: List of all generated tests\n",
    "            function_info: Information about the function under test\n",
    "            clustering_method: 'hash' for fast structural hashing, 'vector' for advanced clustering\n",
    "        \"\"\"\n",
    "        print(\"🔬 Starting Hybrid Cluster-then-Synthesize Pipeline\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Stage 1: AST-Based Structural Clustering\n",
    "        print(\"\\n📊 Stage 1: AST-Based Structural Clustering\")\n",
    "        clusters = self.clusterer.cluster_tests(all_tests, method=clustering_method)\n",
    "        \n",
    "        # Stage 2: LLM-Powered Cluster Synthesis\n",
    "        print(f\"\\n🤖 Stage 2: LLM-Powered Cluster Synthesis\")\n",
    "        print(f\"   Synthesizing {len(clusters)} clusters...\")\n",
    "        \n",
    "        synthesized_tests = []\n",
    "        func = function_info['functions'][0] if function_info['functions'] else {}\n",
    "        function_name = func.get('name', 'unknown_function')\n",
    "        \n",
    "        # Select best model for synthesis\n",
    "        best_model = SYNTHESIZER_MODEL if SYNTHESIZER_MODEL in self.llm_council.models else list(self.llm_council.models.keys())[0]\n",
    "        model_config = self.llm_council.models[best_model]\n",
    "        \n",
    "        with tqdm(total=len(clusters), desc=\"Synthesizing clusters\") as pbar:\n",
    "            for cluster_id, test_indices in clusters.items():\n",
    "                cluster_tests = [all_tests[idx] for idx in test_indices]\n",
    "                \n",
    "                if len(cluster_tests) == 1:\n",
    "                    # Singleton cluster - use test as-is (already unique)\n",
    "                    synthesized_tests.append(cluster_tests[0])\n",
    "                    pbar.update(1)\n",
    "                else:\n",
    "                    # Multi-test cluster - synthesize representative test\n",
    "                    try:\n",
    "                        representative_test = self._synthesize_cluster(\n",
    "                            cluster_tests, function_info, model_config, cluster_id\n",
    "                        )\n",
    "                        synthesized_tests.append(representative_test)\n",
    "                    except Exception as e:\n",
    "                        print(f\"⚠️  Cluster {cluster_id} synthesis failed, using first test: {e}\")\n",
    "                        synthesized_tests.append(cluster_tests[0])\n",
    "                    pbar.update(1)\n",
    "        \n",
    "        # Stage 3: LLM-Powered Final Test File Generation\n",
    "        print(\"\\n📝 Stage 3: LLM-Powered Final Test File Generation...\")\n",
    "        print(f\"   Using {self.finalizer_model} to generate clean, unified test file...\")\n",
    "        \n",
    "        final_content = self._llm_finalize_test_file(synthesized_tests, function_info)\n",
    "        \n",
    "        # Extract final tests\n",
    "        final_tests = self._extract_tests_from_content(final_content, synthesized_tests)\n",
    "        \n",
    "        # Calculate statistics\n",
    "        reduction_ratio = (len(all_tests) - len(final_tests)) / len(all_tests) if len(all_tests) > 0 else 0\n",
    "        \n",
    "        print(f\"\\n✅ Hybrid synthesis complete!\")\n",
    "        print(f\"   📊 Original tests: {len(all_tests)}\")\n",
    "        print(f\"   🔬 Clusters identified: {len(clusters)}\")\n",
    "        print(f\"   ✨ Final unique tests: {len(final_tests)}\")\n",
    "        print(f\"   📉 Reduction: {reduction_ratio*100:.1f}%\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        return {\n",
    "            'synthesized_content': final_content,\n",
    "            'final_tests': final_tests,\n",
    "            'original_count': len(all_tests),\n",
    "            'final_count': len(final_tests),\n",
    "            'cluster_count': len(clusters),\n",
    "            'reduction_ratio': reduction_ratio,\n",
    "            'synthesizer_model': best_model,\n",
    "            'finalizer_model': self.finalizer_model,\n",
    "            'clustering_method': clustering_method,\n",
    "            'clusters': clusters  # Include cluster info for analysis\n",
    "        }\n",
    "    \n",
    "    def _synthesize_cluster(self, cluster_tests: List[Dict], function_info: Dict,\n",
    "                           model_config: Dict, cluster_id: int) -> Dict:\n",
    "        \"\"\"Synthesize a single representative test from a cluster\"\"\"\n",
    "        func = function_info['functions'][0] if function_info['functions'] else {}\n",
    "        function_name = func.get('name', 'unknown_function')\n",
    "        \n",
    "        # Build cluster-specific prompt\n",
    "        prompt = f\"\"\"You are an expert test synthesis engineer. The following {len(cluster_tests)} test cases have been algorithmically identified as testing similar functionality through AST structural analysis.\n",
    "\n",
    "ORIGINAL FUNCTION UNDER TEST:\n",
    "```python\n",
    "{func.get('source_code', function_info['source_code'])}\n",
    "\n",
    "CLUSTERED TESTS (structurally similar):\n",
    "\"\"\"\n",
    "        \n",
    "        for i, test in enumerate(cluster_tests, 1):\n",
    "            prompt += f\"\\n--- Test {i} (from {test['source_model']}, role: {test.get('role_name', 'unknown')}) ---\\n\"\n",
    "            prompt += f\"Category: {test['category']}\\n\"\n",
    "            prompt += f\"```python\\n{test['code']}\\n```\\n\"\n",
    "        \n",
    "        prompt += f\"\"\"\n",
    "\n",
    "YOUR TASK:\n",
    "Create ONE superior \"representative test\" that captures the best aspects of all these similar tests.\n",
    "\n",
    "SYNTHESIS GUIDELINES:\n",
    "1. **Analyze Test Logic**: Identify what specific scenario/behavior all these tests are verifying\n",
    "2. **Select Best Elements**:\n",
    "   - Choose the clearest, most descriptive test name\n",
    "   - Use the most comprehensive assertion message\n",
    "   - Select input values that best represent the test scenario\n",
    "   - Keep the most readable code structure\n",
    "\n",
    "3. **Merge Insights**: If different tests check slightly different aspects:\n",
    "   - Combine assertions if they test the same logical path\n",
    "   - Keep the most thorough error checking\n",
    "   - Preserve important edge cases\n",
    "\n",
    "4. **Output Format**:\n",
    "   - Import statement: `from function import {function_name}`\n",
    "   - Single pytest function\n",
    "   - Include category comment from original tests\n",
    "   - Add clear docstring explaining what is tested\n",
    "   - NO markdown fences - just clean Python code\n",
    "\n",
    "CRITICAL: Output ONLY the synthesized test function code. No explanations, no markdown, just Python.\n",
    "\n",
    "Representative test:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            if model_config[\"type\"] == \"openai\":\n",
    "                response = self.llm_council.call_openai_model(prompt, model_config)\n",
    "                # Clean response\n",
    "                cleaned_code = self._clean_synthesized_content(response)\n",
    "                \n",
    "                # Extract test method\n",
    "                test_methods = code_analyzer.extract_test_methods_from_response(cleaned_code)\n",
    "                \n",
    "                if test_methods:\n",
    "                    # Use the synthesized test\n",
    "                    representative = test_methods[0].copy()\n",
    "                    representative['category'] = cluster_tests[0]['category']  # Inherit category\n",
    "                    representative['source_model'] = 'synthesized'\n",
    "                    representative['source_role'] = 'cluster_synthesis'\n",
    "                    representative['role_name'] = f\"Synthesized from cluster {cluster_id}\"\n",
    "                    representative['cluster_id'] = cluster_id\n",
    "                    representative['cluster_size'] = len(cluster_tests)\n",
    "                    return representative\n",
    "                else:\n",
    "                    # Fallback to best test in cluster\n",
    "                    return cluster_tests[0]\n",
    "            else:\n",
    "                return cluster_tests[0]\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Synthesis error for cluster {cluster_id}: {e}\")\n",
    "            return cluster_tests[0]\n",
    "    \n",
    "    def _llm_finalize_test_file(self, synthesized_tests: List[Dict], function_info: Dict) -> str:\n",
    "        \"\"\"Use LLM to generate the final, clean test file with all synthesized tests\"\"\"\n",
    "        func = function_info['functions'][0] if function_info['functions'] else {}\n",
    "        function_name = func.get('name', 'unknown_function')\n",
    "        \n",
    "        # Extract all function names from function_info for import\n",
    "        all_function_names = [f['name'] for f in function_info.get('functions', [])]\n",
    "        if not all_function_names:\n",
    "            all_function_names = [function_name]\n",
    "        \n",
    "        # Organize tests by category\n",
    "        by_category = {}\n",
    "        for test in synthesized_tests:\n",
    "            category = test['category']\n",
    "            if category not in by_category:\n",
    "                by_category[category] = []\n",
    "            by_category[category].append(test)\n",
    "        \n",
    "        # Build comprehensive prompt for final generation\n",
    "        prompt = f\"\"\"You are an expert Python test engineer. Generate a COMPLETE, CLEAN, PRODUCTION-READY pytest test file.\n",
    "\n",
    "FUNCTION(S) UNDER TEST (saved in function.py):\n",
    "python\n",
    "{function_info['source_code']}\n",
    "\n",
    "CRITICAL IMPORT REQUIREMENT:\n",
    "The function(s) being tested are in a file called `function.py`. You MUST import them using:\n",
    "`from function import {', '.join(all_function_names)}`\n",
    "\n",
    "SYNTHESIZED TEST SCENARIOS:\n",
    "You have {len(synthesized_tests)} unique test scenarios to include. Here they are organized by category:\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        for category in ['positive', 'negative', 'boundary', 'edge_case', 'security']:\n",
    "            if category not in by_category:\n",
    "                continue\n",
    "            \n",
    "            tests = by_category[category]\n",
    "            prompt += f\"\\n{'='*70}\\n\"\n",
    "            prompt += f\"CATEGORY: {category.upper()} ({len(tests)} tests)\\n\"\n",
    "            prompt += f\"{'='*70}\\n\"\n",
    "            \n",
    "            for i, test in enumerate(tests, 1):\n",
    "                prompt += f\"\\nTest {i}:\\n\"\n",
    "                prompt += f\"```python\\n{test['code']}\\n```\\n\"\n",
    "        \n",
    "        prompt += f\"\"\"\n",
    "\n",
    "YOUR TASK:\n",
    "Generate a SINGLE, COMPLETE pytest test file that includes ALL {len(synthesized_tests)} test scenarios above.\n",
    "\n",
    "REQUIREMENTS:\n",
    "1. **File Header**: Add a clean docstring explaining this is an auto-generated comprehensive test suite\n",
    "2. **Imports Section**: \n",
    "   - MUST include: `import pytest`\n",
    "   - MUST include: `from function import {', '.join(all_function_names)}`\n",
    "   - Include any other necessary imports (e.g., contextlib, etc.)\n",
    "3. **Organization**: Group tests by category with clear section comments\n",
    "4. **Naming Convention**: Use consistent, descriptive test names (e.g., test_<scenario>_<condition>)\n",
    "5. **Code Style**: Follow PEP 8, use clear assertions with messages\n",
    "6. **Completeness**: Include EVERY test scenario provided above\n",
    "7. **Clean Code**: No markdown fences in output, no redundant code\n",
    "\n",
    "CRITICAL: The import statement MUST be exactly: `from function import {', '.join(all_function_names)}`\n",
    "This is because the function(s) under test will be saved in a file called function.py.\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "- Start with module docstring\n",
    "- Import section (pytest + function imports from function.py)\n",
    "- Test functions organized by category\n",
    "- Each category section has a header comment\n",
    "- NO markdown code fences in the output\n",
    "- Just clean, executable Python code\n",
    "\n",
    "Generate the complete test file now:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Use Gemini Flash 2 for final generation\n",
    "            if self.finalizer_model in self.llm_council.models:\n",
    "                model_config = self.llm_council.models[self.finalizer_model]\n",
    "                \n",
    "                if model_config[\"type\"] == \"openai\":\n",
    "                    response = self.llm_council.call_openai_model(prompt, model_config)\n",
    "                    final_content = self._clean_synthesized_content(response)\n",
    "                    \n",
    "                    # Verify the content has the required import statement\n",
    "                    required_import = f'from function import {\", \".join(all_function_names)}'\n",
    "                    has_required_import = required_import in final_content or any(\n",
    "                        f'from function import {name}' in final_content for name in all_function_names\n",
    "                    )\n",
    "                    \n",
    "                    if has_required_import:\n",
    "                        print(f\"✅ LLM-generated final test file created successfully\")\n",
    "                        return final_content\n",
    "                    else:\n",
    "                        print(f\"⚠️  LLM output missing required import statement, using fallback\")\n",
    "                        return self._build_final_test_file_fallback(synthesized_tests, function_info)\n",
    "                else:\n",
    "                    return self._build_final_test_file_fallback(synthesized_tests, function_info)\n",
    "            else:\n",
    "                print(f\"⚠️  Finalizer model {self.finalizer_model} not available, using fallback\")\n",
    "                return self._build_final_test_file_fallback(synthesized_tests, function_info)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  LLM finalization error: {e}, using fallback\")\n",
    "            return self._build_final_test_file_fallback(synthesized_tests, function_info)\n",
    "    \n",
    "    def _build_final_test_file_fallback(self, synthesized_tests: List[Dict], function_info: Dict) -> str:\n",
    "        \"\"\"Fallback method to build test file if LLM fails\"\"\"\n",
    "        func = function_info['functions'][0] if function_info['functions'] else {}\n",
    "        function_name = func.get('name', 'unknown_function')\n",
    "        \n",
    "        # Extract all function names for import\n",
    "        all_function_names = [f['name'] for f in function_info.get('functions', [])]\n",
    "        if not all_function_names:\n",
    "            all_function_names = [function_name]\n",
    "        \n",
    "        # Count statistics\n",
    "        cluster_synthesized = sum(1 for t in synthesized_tests if t.get('cluster_size', 1) > 1)\n",
    "        singleton_tests = len(synthesized_tests) - cluster_synthesized\n",
    "        \n",
    "        header = f'''\"\"\"\n",
    "Intelligent Test Suite - Hybrid Cluster-then-Synthesize Approach\n",
    "Generated by Role-Based LLM Council with AST-Clustered Deduplication\n",
    "\n",
    "Target Function(s): {', '.join(all_function_names)}\n",
    "Total Tests: {len(synthesized_tests)}\n",
    "  • Cluster-synthesized tests: {cluster_synthesized}\n",
    "  • Unique singleton tests: {singleton_tests}\n",
    "\n",
    "Pipeline:\n",
    "  1. Role-based test generation by specialized LLM agents\n",
    "  2. AST-based structural clustering for duplicate detection\n",
    "  3. Cluster-wise LLM synthesis for optimal test selection\n",
    "  4. Final LLM-powered test file generation\n",
    "\"\"\"\n",
    "\n",
    "import pytest\n",
    "from function import {', '.join(all_function_names)}\n",
    "\n",
    "\n",
    "'''\n",
    "        \n",
    "        # Group by category\n",
    "        by_category = {}\n",
    "        for test in synthesized_tests:\n",
    "            category = test['category']\n",
    "            if category not in by_category:\n",
    "                by_category[category] = []\n",
    "            by_category[category].append(test)\n",
    "        \n",
    "        # Build test code organized by category\n",
    "        test_code = header\n",
    "        \n",
    "        for category in ['positive', 'negative', 'boundary', 'edge_case', 'security']:\n",
    "            if category not in by_category:\n",
    "                continue\n",
    "            \n",
    "            tests = by_category[category]\n",
    "            test_code += f\"\\n# {category.upper()} TESTS\\n\"\n",
    "            test_code += f\"# {'='*70}\\n\\n\"\n",
    "            \n",
    "            for test in tests:\n",
    "                test_code += test['code'] + \"\\n\\n\"\n",
    "        \n",
    "        return test_code\n",
    "    \n",
    "    def _extract_tests_from_content(self, content: str, synthesized_tests: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Extract final test information from content\"\"\"\n",
    "        final_tests = []\n",
    "        \n",
    "        for test in synthesized_tests:\n",
    "            final_test = {\n",
    "                'name': test['name'],\n",
    "                'code': test['code'],\n",
    "                'category': test['category'],\n",
    "                'source': test.get('source_model', 'unknown'),\n",
    "                'cluster_id': test.get('cluster_id', -1),\n",
    "                'cluster_size': test.get('cluster_size', 1),\n",
    "                'is_synthesized': test.get('cluster_size', 1) > 1\n",
    "            }\n",
    "            final_tests.append(final_test)\n",
    "        \n",
    "        return final_tests\n",
    "    \n",
    "    def _clean_synthesized_content(self, content: str) -> str:\n",
    "        \"\"\"Clean synthesized content by removing markdown artifacts\"\"\"\n",
    "        lines = content.split('\\n')\n",
    "        cleaned_lines = []\n",
    "        \n",
    "        for line in lines:\n",
    "            stripped_line = line.strip()\n",
    "            \n",
    "            # Skip markdown code fence lines\n",
    "            if stripped_line in ['```python', '```py', '```', '```\\n']:\n",
    "                continue\n",
    "            \n",
    "            # Remove leading markdown\n",
    "            if stripped_line.startswith('```python'):\n",
    "                line = line.replace('```python', '', 1)\n",
    "            elif stripped_line.startswith('```py'):\n",
    "                line = line.replace('```py', '', 1)\n",
    "            elif stripped_line.startswith('```') and stripped_line.endswith('```'):\n",
    "                continue\n",
    "            \n",
    "            cleaned_lines.append(line)\n",
    "        \n",
    "        cleaned_code = '\\n'.join(cleaned_lines).strip()\n",
    "        \n",
    "        # Remove trailing backticks\n",
    "        while cleaned_code.endswith('```'):\n",
    "            cleaned_code = cleaned_code[:-3].strip()\n",
    "        \n",
    "        return cleaned_code\n",
    "\n",
    "# Initialize synthesizer\n",
    "test_synthesizer = TestSynthesizer(llm_council)\n",
    "\n",
    "print(\"✅ Hybrid Cluster-then-Synthesize system initialized!\")\n",
    "print(\"   🔬 AST-based structural clustering\")\n",
    "print(\"   🤖 LLM-powered cluster synthesis\")\n",
    "print(\"   ✨ LLM-powered final test file generation\")\n",
    "print(\"   📊 Two clustering methods available: 'hash' (fast) and 'vector' (advanced)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc3dfb4a-56d8-438f-88e0-f6f26c8a6701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Coverage Analyzer Module (Updated - use output directory)\n",
    "class CoverageAnalyzer:\n",
    "    \"\"\"Analyzes code coverage and test execution results\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def analyze_coverage(source_code: str, test_code: str, output_dir: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Analyze code coverage by executing tests\n",
    "        \n",
    "        Args:\n",
    "            source_code: Source code of the function\n",
    "            test_code: Test code\n",
    "            output_dir: Optional directory containing function.py and test files\n",
    "        \n",
    "        Returns dictionary with coverage metrics and test results\n",
    "        \"\"\"\n",
    "        print(\"📊 Analyzing code coverage...\")\n",
    "        \n",
    "        if output_dir and os.path.exists(output_dir):\n",
    "            # Use existing output directory\n",
    "            work_dir = output_dir\n",
    "            cleanup = False\n",
    "            print(f\"   Using output directory: {output_dir}\")\n",
    "        else:\n",
    "            # Create temporary directory\n",
    "            work_dir = tempfile.mkdtemp()\n",
    "            cleanup = True\n",
    "            print(f\"   Using temporary directory\")\n",
    "        \n",
    "        try:\n",
    "            # Write source code to function.py\n",
    "            function_file = os.path.join(work_dir, 'function.py')\n",
    "            with open(function_file, 'w') as f:\n",
    "                f.write(source_code)\n",
    "            \n",
    "            # Write test code to test_function.py\n",
    "            test_file = os.path.join(work_dir, 'test_function.py')\n",
    "            with open(test_file, 'w') as f:\n",
    "                f.write(test_code)\n",
    "            \n",
    "            # Run pytest with coverage\n",
    "            result = subprocess.run(\n",
    "                ['pytest', test_file, '--cov=function', '--cov-report=json', \n",
    "                 '--tb=short', '-v'],\n",
    "                cwd=work_dir,\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=30\n",
    "            )\n",
    "            \n",
    "            # Parse pytest output for test results\n",
    "            test_results = CoverageAnalyzer._parse_pytest_output(result.stdout)\n",
    "            \n",
    "            # Read coverage report\n",
    "            coverage_file = os.path.join(work_dir, 'coverage.json')\n",
    "            coverage_data = {}\n",
    "            if os.path.exists(coverage_file):\n",
    "                with open(coverage_file, 'r') as f:\n",
    "                    coverage_data = json.load(f)\n",
    "            \n",
    "            # Extract coverage percentage\n",
    "            coverage_percentage = 0.0\n",
    "            if 'totals' in coverage_data:\n",
    "                coverage_percentage = coverage_data['totals'].get('percent_covered', 0.0)\n",
    "            \n",
    "            print(f\"✅ Coverage analysis complete:\")\n",
    "            print(f\"   • Code coverage: {coverage_percentage:.1f}%\")\n",
    "            print(f\"   • Tests run: {test_results['total_tests']}\")\n",
    "            print(f\"   • Tests passed: {test_results['passed_tests']}\")\n",
    "            print(f\"   • Tests failed: {test_results['failed_tests']}\")\n",
    "            \n",
    "            return {\n",
    "                'coverage_percentage': coverage_percentage,\n",
    "                'coverage_data': coverage_data,\n",
    "                'test_results': result.stdout,\n",
    "                'test_stderr': result.stderr,\n",
    "                'return_code': result.returncode,\n",
    "                'total_tests': test_results['total_tests'],\n",
    "                'passed_tests': test_results['passed_tests'],\n",
    "                'failed_tests': test_results['failed_tests'],\n",
    "                'skipped_tests': test_results['skipped_tests'],\n",
    "                'error_tests': test_results['error_tests'],\n",
    "                'success_rate': test_results['success_rate']\n",
    "            }\n",
    "            \n",
    "        except subprocess.TimeoutExpired:\n",
    "            print(\"⚠️  Coverage analysis timed out\")\n",
    "            return {\n",
    "                'coverage_percentage': 0.0,\n",
    "                'error': 'Timeout during test execution',\n",
    "                'total_tests': 0,\n",
    "                'passed_tests': 0,\n",
    "                'failed_tests': 0,\n",
    "                'success_rate': 0.0\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Coverage analysis error: {e}\")\n",
    "            return {\n",
    "                'coverage_percentage': 0.0,\n",
    "                'error': str(e),\n",
    "                'total_tests': 0,\n",
    "                'passed_tests': 0,\n",
    "                'failed_tests': 0,\n",
    "                'success_rate': 0.0\n",
    "            }\n",
    "        finally:\n",
    "            # Cleanup temporary directory if created\n",
    "            if cleanup:\n",
    "                import shutil\n",
    "                shutil.rmtree(work_dir, ignore_errors=True)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _parse_pytest_output(output: str) -> Dict[str, Any]:\n",
    "        \"\"\"Parse pytest output to extract test results\"\"\"\n",
    "        results = {\n",
    "            'total_tests': 0,\n",
    "            'passed_tests': 0,\n",
    "            'failed_tests': 0,\n",
    "            'skipped_tests': 0,\n",
    "            'error_tests': 0,\n",
    "            'success_rate': 0.0\n",
    "        }\n",
    "        \n",
    "        # Look for summary line like \"5 passed, 2 failed in 0.50s\"\n",
    "        summary_pattern = r'(\\d+)\\s+passed|(\\d+)\\s+failed|(\\d+)\\s+skipped|(\\d+)\\s+error'\n",
    "        matches = re.findall(summary_pattern, output)\n",
    "        \n",
    "        for match in matches:\n",
    "            if match[0]:  # passed\n",
    "                results['passed_tests'] = int(match[0])\n",
    "            elif match[1]:  # failed\n",
    "                results['failed_tests'] = int(match[1])\n",
    "            elif match[2]:  # skipped\n",
    "                results['skipped_tests'] = int(match[2])\n",
    "            elif match[3]:  # error\n",
    "                results['error_tests'] = int(match[3])\n",
    "        \n",
    "        results['total_tests'] = (results['passed_tests'] + results['failed_tests'] + \n",
    "                                 results['skipped_tests'] + results['error_tests'])\n",
    "        \n",
    "        if results['total_tests'] > 0:\n",
    "            results['success_rate'] = (results['passed_tests'] / results['total_tests']) * 100\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Initialize coverage analyzer\n",
    "coverage_analyzer = CoverageAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "697940aa-34b1-4e14-b1a8-b35dcd063654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Intelligent Test Council initialized with Hybrid Clustering!\n",
      "   🎭 Role-based LLM test generation\n",
      "   🔬 AST-based structural clustering\n",
      "   🤖 Cluster-wise LLM synthesis\n",
      "   ✨ LLM-powered final test file generation (Gemini Flash 2)\n",
      "   📊 Comprehensive coverage analysis\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Main Pipeline Orchestrator (Updated - Add save_results method back)\n",
    "class IntelligentTestCouncil:\n",
    "    \"\"\"Main orchestrator for intelligent role-based test generation with hybrid clustering\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.code_analyzer = CodeAnalyzer()\n",
    "        self.llm_council = LLMCouncil(config)\n",
    "        self.test_classifier = TestClassifier()\n",
    "        self.test_synthesizer = TestSynthesizer(self.llm_council)\n",
    "        self.coverage_analyzer = CoverageAnalyzer()\n",
    "        \n",
    "    def generate_comprehensive_tests(self, function_code: str, \n",
    "                                     clustering_method: str = 'vector',\n",
    "                                     output_dir: str = 'test_results') -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Main pipeline for generating comprehensive test suite with hybrid clustering\n",
    "        \n",
    "        Args:\n",
    "            function_code: Source code of function to test\n",
    "            clustering_method: 'hash' for fast clustering, 'vector' for advanced DBSCAN clustering\n",
    "            output_dir: Directory to save results\n",
    "        \"\"\"\n",
    "        print(\"🚀 Starting Role-Based Intelligent Test Council with Hybrid Clustering\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Step 1: Analyze the input function\n",
    "        print(\"\\n📝 Step 1: Analyzing input function...\")\n",
    "        function_info = self.code_analyzer.extract_function_info(function_code)\n",
    "        \n",
    "        if not function_info['functions']:\n",
    "            error_msg = 'No functions found in the provided code'\n",
    "            if 'syntax_error' in function_info:\n",
    "                error_msg += f\". Syntax error: {function_info['syntax_error']}\"\n",
    "            return {'error': error_msg}\n",
    "        \n",
    "        print(f\"✅ Found {function_info['total_functions']} function(s)\")\n",
    "        \n",
    "        # Step 2: Generate tests using role-based LLM council\n",
    "        print(\"\\n🎭 Step 2: Consulting Role-Based LLM Council...\")\n",
    "        council_results = self.llm_council.generate_tests_from_council(function_info)\n",
    "        \n",
    "        # Step 3: Classify all test cases\n",
    "        print(\"\\n🏷️  Step 3: Classifying test cases by category and role...\")\n",
    "        all_classified_tests = self.test_classifier.classify_council_results(council_results)\n",
    "        \n",
    "        print(f\"✅ Total tests generated: {len(all_classified_tests)}\")\n",
    "        \n",
    "        # Display distributions\n",
    "        role_counts = Counter(test['role_name'] for test in all_classified_tests)\n",
    "        category_counts = Counter(test['category'] for test in all_classified_tests)\n",
    "        \n",
    "        print(\"\\n🎭 Role distribution:\")\n",
    "        for role_name, count in role_counts.items():\n",
    "            print(f\"   • {role_name}: {count} tests\")\n",
    "        \n",
    "        print(\"\\n📊 Category distribution:\")\n",
    "        for category, count in category_counts.items():\n",
    "            print(f\"   • {category}: {count} tests\")\n",
    "        \n",
    "        # Step 4: Hybrid Cluster-then-Synthesize approach\n",
    "        print(f\"\\n🔬 Step 4: Hybrid Cluster-then-Synthesize Deduplication...\")\n",
    "        synthesis_results = self.test_synthesizer.synthesize_final_test_file(\n",
    "            all_classified_tests, function_info, clustering_method=clustering_method\n",
    "        )\n",
    "        \n",
    "        # Step 5: Save results to output directory\n",
    "        print(f\"\\n💾 Step 5: Saving results to {output_dir}/...\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save source function\n",
    "        function_file_path = os.path.join(output_dir, 'function.py')\n",
    "        with open(function_file_path, 'w') as f:\n",
    "            f.write(function_code)\n",
    "        print(f\"   ✅ Saved source function to: {function_file_path}\")\n",
    "        \n",
    "        # Save test file\n",
    "        test_file_path = os.path.join(output_dir, 'test_function.py')\n",
    "        with open(test_file_path, 'w') as f:\n",
    "            f.write(synthesis_results['synthesized_content'])\n",
    "        print(f\"   ✅ Saved test file to: {test_file_path}\")\n",
    "        \n",
    "        # Step 6: Analyze coverage using the saved files\n",
    "        print(\"\\n📊 Step 6: Analyzing code coverage...\")\n",
    "        coverage_results = self.coverage_analyzer.analyze_coverage(\n",
    "            function_code, \n",
    "            synthesis_results['synthesized_content'],\n",
    "            output_dir=output_dir\n",
    "        )\n",
    "        \n",
    "        # Prepare comprehensive results\n",
    "        results = {\n",
    "            'function_info': function_info,\n",
    "            'council_results': council_results,\n",
    "            'all_classified_tests': all_classified_tests,\n",
    "            'synthesis_results': synthesis_results,\n",
    "            'final_test_file': synthesis_results['synthesized_content'],\n",
    "            'coverage_results': coverage_results,\n",
    "            'output_dir': output_dir,\n",
    "            'statistics': {\n",
    "                'original_test_count': len(all_classified_tests),\n",
    "                'final_test_count': synthesis_results['final_count'],\n",
    "                'cluster_count': synthesis_results['cluster_count'],\n",
    "                'reduction_ratio': synthesis_results['reduction_ratio'],\n",
    "                'clustering_method': clustering_method,\n",
    "                'coverage_percentage': coverage_results.get('coverage_percentage', 0.0),\n",
    "                'test_success_rate': coverage_results.get('success_rate', 0.0),\n",
    "                'total_tests_run': coverage_results.get('total_tests', 0),\n",
    "                'passed_tests': coverage_results.get('passed_tests', 0),\n",
    "                'failed_tests': coverage_results.get('failed_tests', 0),\n",
    "                'models_used': list(council_results.keys()),\n",
    "                'roles_used': list(set(test['role_name'] for test in all_classified_tests)),\n",
    "                'categories_found': list(category_counts.keys()),\n",
    "                'synthesizer_model': synthesis_results['synthesizer_model'],\n",
    "                'finalizer_model': synthesis_results.get('finalizer_model', 'fallback'),\n",
    "                'tests_per_role': dict(role_counts),\n",
    "                'tests_per_category': dict(category_counts),\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save additional metadata\n",
    "        self._save_metadata(results, output_dir)\n",
    "        \n",
    "        print(\"\\n🎉 Pipeline completed successfully!\")\n",
    "        print(f\"📊 Test Success Rate: {coverage_results.get('success_rate', 0.0):.1f}%\")\n",
    "        print(f\"📈 Code Coverage: {coverage_results.get('coverage_percentage', 0.0):.1f}%\")\n",
    "        print(f\"✅ Passed Tests: {coverage_results.get('passed_tests', 0)}/{coverage_results.get('total_tests', 0)}\")\n",
    "        print(f\"📁 Output directory: {output_dir}/\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save_results(self, results: Dict[str, Any], output_dir: str = None):\n",
    "        \"\"\"\n",
    "        Save comprehensive results to files (public method for backward compatibility)\n",
    "        \n",
    "        Args:\n",
    "            results: Results dictionary from generate_comprehensive_tests\n",
    "            output_dir: Optional output directory (uses results['output_dir'] if not specified)\n",
    "        \"\"\"\n",
    "        if output_dir is None:\n",
    "            output_dir = results.get('output_dir', 'test_results')\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save source function if available in results\n",
    "        if 'function_info' in results:\n",
    "            function_file_path = os.path.join(output_dir, 'function.py')\n",
    "            with open(function_file_path, 'w') as f:\n",
    "                f.write(results['function_info']['source_code'])\n",
    "        \n",
    "        # Save final test file\n",
    "        test_file_path = os.path.join(output_dir, 'test_function.py')\n",
    "        with open(test_file_path, 'w') as f:\n",
    "            f.write(results['final_test_file'])\n",
    "        \n",
    "        # Save metadata\n",
    "        self._save_metadata(results, output_dir)\n",
    "        \n",
    "        print(f\"\\n💾 Results saved to: {output_dir}/\")\n",
    "    \n",
    "    def _save_metadata(self, results: Dict[str, Any], output_dir: str):\n",
    "        \"\"\"Save additional metadata files\"\"\"\n",
    "        # Save statistics\n",
    "        stats_file = os.path.join(output_dir, 'statistics.json')\n",
    "        with open(stats_file, 'w') as f:\n",
    "            json.dump(results['statistics'], f, indent=2)\n",
    "        \n",
    "        # Save cluster information if available (with numpy type conversion)\n",
    "        if 'clusters' in results['synthesis_results']:\n",
    "            clusters = results['synthesis_results']['clusters']\n",
    "            \n",
    "            # Convert numpy int64 keys to native Python int\n",
    "            clusters_serializable = {\n",
    "                int(k): [int(idx) for idx in v]  # Convert both keys and values\n",
    "                for k, v in clusters.items()\n",
    "            }\n",
    "            \n",
    "            clusters_file = os.path.join(output_dir, 'clusters.json')\n",
    "            with open(clusters_file, 'w') as f:\n",
    "                json.dump(clusters_serializable, f, indent=2)\n",
    "\n",
    "# Initialize the intelligent council\n",
    "intelligent_council = IntelligentTestCouncil(config)\n",
    "\n",
    "print(\"✅ Intelligent Test Council initialized with Hybrid Clustering!\")\n",
    "print(\"   🎭 Role-based LLM test generation\")\n",
    "print(\"   🔬 AST-based structural clustering\")\n",
    "print(\"   🤖 Cluster-wise LLM synthesis\")\n",
    "print(\"   ✨ LLM-powered final test file generation (Gemini Flash 2)\")\n",
    "print(\"   📊 Comprehensive coverage analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac018163-e05f-45b8-bc9b-6d59d2d4030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 10: Role Assignment Optimization Experiment\n",
    "# import random\n",
    "# import json\n",
    "# from collections import defaultdict\n",
    "# from datetime import datetime\n",
    "# import pandas as pd\n",
    "# import asyncio\n",
    "# import nest_asyncio\n",
    "\n",
    "# # Enable nested event loops for Jupyter compatibility\n",
    "# nest_asyncio.apply()\n",
    "\n",
    "# class RoleAssignmentExperiment:\n",
    "#     \"\"\"Conducts experiments to determine optimal model-role assignments with concurrent processing\"\"\"\n",
    "    \n",
    "#     def __init__(self, llm_council, code_analyzer, test_classifier):\n",
    "#         self.llm_council = llm_council\n",
    "#         self.code_analyzer = code_analyzer\n",
    "#         self.test_classifier = test_classifier\n",
    "#         self.results = []\n",
    "        \n",
    "#     def load_dataset(self, dataset_path: str) -> List[Dict]:\n",
    "#         \"\"\"Load functions from dataset\"\"\"\n",
    "#         try:\n",
    "#             with open(dataset_path, 'r') as f:\n",
    "#                 data = json.load(f)\n",
    "#             functions = data.get('functions', [])\n",
    "#             print(f\"✅ Loaded {len(functions)} functions from dataset\")\n",
    "#             return functions\n",
    "#         except Exception as e:\n",
    "#             print(f\"❌ Error loading dataset: {e}\")\n",
    "#             return []\n",
    "    \n",
    "#     def sample_functions(self, functions: List[Dict], n: int = 20, seed: int = 42) -> List[Dict]:\n",
    "#         \"\"\"Randomly sample n functions from dataset\"\"\"\n",
    "#         random.seed(seed)\n",
    "#         sampled = random.sample(functions, min(n, len(functions)))\n",
    "#         print(f\"📊 Sampled {len(sampled)} functions for experiment\")\n",
    "#         return sampled\n",
    "    \n",
    "#     async def process_single_function_async(self, func_data: Dict, func_idx: int, total: int) -> Dict[str, Any]:\n",
    "#         \"\"\"Process a single function through the council asynchronously\"\"\"\n",
    "#         print(f\"\\n{'='*80}\")\n",
    "#         print(f\"Processing function {func_idx}/{total}: {func_data.get('name', 'unknown')}\")\n",
    "#         print(f\"Category: {func_data.get('category', 'unknown')}\")\n",
    "#         print(f\"{'='*80}\")\n",
    "        \n",
    "#         try:\n",
    "#             # Extract function info\n",
    "#             function_info = self.code_analyzer.extract_function_info(func_data['source'])\n",
    "            \n",
    "#             if not function_info['functions']:\n",
    "#                 print(f\"⚠️ Could not parse function {func_data.get('name', 'unknown')}\")\n",
    "#                 return None\n",
    "            \n",
    "#             # Generate tests from council using concurrent API calls\n",
    "#             council_results = await self.llm_council.generate_tests_from_council_async(\n",
    "#                 function_info, \n",
    "#                 max_concurrent=7\n",
    "#             )\n",
    "            \n",
    "#             # Classify tests\n",
    "#             classified_tests = self.test_classifier.classify_council_results(council_results)\n",
    "            \n",
    "#             # Aggregate statistics\n",
    "#             stats = self._aggregate_function_stats(func_data, classified_tests)\n",
    "            \n",
    "#             return stats\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"❌ Error processing function {func_data.get('name', 'unknown')}: {e}\")\n",
    "#             import traceback\n",
    "#             traceback.print_exc()\n",
    "#             return None\n",
    "    \n",
    "#     def _aggregate_function_stats(self, func_data: Dict, classified_tests: List[Dict]) -> Dict[str, Any]:\n",
    "#         \"\"\"Aggregate statistics for a single function\"\"\"\n",
    "#         stats = {\n",
    "#             'function_name': func_data.get('name', 'unknown'),\n",
    "#             'function_category': func_data.get('category', 'unknown'),\n",
    "#             'function_file': func_data.get('file', 'unknown'),\n",
    "#             'total_tests_generated': len(classified_tests),\n",
    "#             'model_role_category_matrix': defaultdict(lambda: defaultdict(lambda: defaultdict(int))),\n",
    "#             'model_totals': defaultdict(int),\n",
    "#             'role_totals': defaultdict(int),\n",
    "#             'category_totals': defaultdict(int),\n",
    "#             'tests': classified_tests\n",
    "#         }\n",
    "        \n",
    "#         # Count tests by model, role, and category\n",
    "#         for test in classified_tests:\n",
    "#             model = test['source_model']\n",
    "#             role = test['source_role']\n",
    "#             category = test['category']\n",
    "            \n",
    "#             stats['model_role_category_matrix'][model][role][category] += 1\n",
    "#             stats['model_totals'][model] += 1\n",
    "#             stats['role_totals'][role] += 1\n",
    "#             stats['category_totals'][category] += 1\n",
    "        \n",
    "#         return stats\n",
    "    \n",
    "#     async def run_experiment_async(self, dataset_path: str, n_functions: int = 20, seed: int = 42) -> Dict[str, Any]:\n",
    "#         \"\"\"Run the complete role assignment optimization experiment with concurrent processing\"\"\"\n",
    "#         print(\"🚀 Starting Role Assignment Optimization Experiment (Concurrent Mode)\")\n",
    "#         print(f\"Target: {n_functions} functions\")\n",
    "#         print(f\"Random seed: {seed}\")\n",
    "#         print(f\"Maximum concurrent API requests: 10\")\n",
    "#         print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "#         # Load and sample functions\n",
    "#         all_functions = self.load_dataset(dataset_path)\n",
    "#         if not all_functions:\n",
    "#             return {'error': 'Failed to load dataset'}\n",
    "        \n",
    "#         sampled_functions = self.sample_functions(all_functions, n_functions, seed)\n",
    "        \n",
    "#         # Process each function\n",
    "#         function_results = []\n",
    "#         for idx, func_data in enumerate(sampled_functions, 1):\n",
    "#             result = await self.process_single_function_async(func_data, idx, len(sampled_functions))\n",
    "#             if result:\n",
    "#                 function_results.append(result)\n",
    "#                 self._save_checkpoint(function_results, idx)\n",
    "        \n",
    "#         # Aggregate cross-function statistics\n",
    "#         aggregated_stats = self._aggregate_cross_function_stats(function_results)\n",
    "        \n",
    "#         # Generate recommendations\n",
    "#         recommendations = self._generate_role_recommendations(aggregated_stats)\n",
    "        \n",
    "#         experiment_results = {\n",
    "#             'timestamp': datetime.now().isoformat(),\n",
    "#             'n_functions_processed': len(function_results),\n",
    "#             'n_functions_target': n_functions,\n",
    "#             'seed': seed,\n",
    "#             'max_concurrent_requests': 10,\n",
    "#             'function_results': function_results,\n",
    "#             'aggregated_stats': aggregated_stats,\n",
    "#             'recommendations': recommendations\n",
    "#         }\n",
    "        \n",
    "#         # Save final results\n",
    "#         self._save_experiment_results(experiment_results)\n",
    "        \n",
    "#         # Print summary\n",
    "#         self._print_experiment_summary(experiment_results)\n",
    "        \n",
    "#         return experiment_results\n",
    "    \n",
    "#     def run_experiment(self, dataset_path: str, n_functions: int = 20, seed: int = 42) -> Dict[str, Any]:\n",
    "#         \"\"\"Wrapper to run async experiment from sync context (Jupyter-compatible)\"\"\"\n",
    "#         # Check if there's already a running event loop (Jupyter)\n",
    "#         try:\n",
    "#             loop = asyncio.get_running_loop()\n",
    "#             # We're in Jupyter, use the existing loop\n",
    "#             return loop.run_until_complete(self.run_experiment_async(dataset_path, n_functions, seed))\n",
    "#         except RuntimeError:\n",
    "#             # No running loop, create a new one\n",
    "#             return asyncio.run(self.run_experiment_async(dataset_path, n_functions, seed))\n",
    "    \n",
    "#     def _aggregate_cross_function_stats(self, function_results: List[Dict]) -> Dict[str, Any]:\n",
    "#         \"\"\"Aggregate statistics across all functions\"\"\"\n",
    "#         aggregated = {\n",
    "#             'total_tests': 0,\n",
    "#             'model_role_category_totals': defaultdict(lambda: defaultdict(lambda: defaultdict(int))),\n",
    "#             'model_category_totals': defaultdict(lambda: defaultdict(int)),\n",
    "#             'role_category_totals': defaultdict(lambda: defaultdict(int)),\n",
    "#             'model_totals': defaultdict(int),\n",
    "#             'role_totals': defaultdict(int),\n",
    "#             'category_totals': defaultdict(int)\n",
    "#         }\n",
    "        \n",
    "#         for func_result in function_results:\n",
    "#             aggregated['total_tests'] += func_result['total_tests_generated']\n",
    "            \n",
    "#             # Aggregate model × role × category\n",
    "#             for model, roles in func_result['model_role_category_matrix'].items():\n",
    "#                 for role, categories in roles.items():\n",
    "#                     for category, count in categories.items():\n",
    "#                         aggregated['model_role_category_totals'][model][role][category] += count\n",
    "#                         aggregated['model_category_totals'][model][category] += count\n",
    "#                         aggregated['role_category_totals'][role][category] += count\n",
    "            \n",
    "#             # Aggregate totals\n",
    "#             for model, count in func_result['model_totals'].items():\n",
    "#                 aggregated['model_totals'][model] += count\n",
    "#             for role, count in func_result['role_totals'].items():\n",
    "#                 aggregated['role_totals'][role] += count\n",
    "#             for category, count in func_result['category_totals'].items():\n",
    "#                 aggregated['category_totals'][category] += count\n",
    "        \n",
    "#         return aggregated\n",
    "    \n",
    "#     def _generate_role_recommendations(self, aggregated_stats: Dict) -> Dict[str, Any]:\n",
    "#         \"\"\"Generate role assignment recommendations based on performance\"\"\"\n",
    "#         recommendations = {\n",
    "#             'model_strengths': {},\n",
    "#             'optimal_assignments': {},\n",
    "#             'specialization_scores': {}\n",
    "#         }\n",
    "        \n",
    "#         # Calculate specialization scores for each model-role-category combination\n",
    "#         for model, roles in aggregated_stats['model_role_category_totals'].items():\n",
    "#             model_strengths = defaultdict(dict)\n",
    "            \n",
    "#             for role, categories in roles.items():\n",
    "#                 role_info = self.llm_council.roles[role]\n",
    "#                 focus_categories = role_info['focus_categories']\n",
    "                \n",
    "#                 # Calculate alignment score: tests in focus categories / total tests\n",
    "#                 focus_count = sum(categories.get(cat, 0) for cat in focus_categories)\n",
    "#                 total_count = sum(categories.values())\n",
    "                \n",
    "#                 if total_count > 0:\n",
    "#                     alignment_score = focus_count / total_count\n",
    "#                     productivity_score = total_count  # Raw number of tests\n",
    "                    \n",
    "#                     # Combined score: weighted average of alignment and productivity\n",
    "#                     combined_score = (alignment_score * 0.6) + (min(productivity_score / 10, 1.0) * 0.4)\n",
    "                    \n",
    "#                     model_strengths[role] = {\n",
    "#                         'alignment_score': alignment_score,\n",
    "#                         'productivity_score': productivity_score,\n",
    "#                         'combined_score': combined_score,\n",
    "#                         'focus_categories': focus_categories,\n",
    "#                         'category_distribution': dict(categories)\n",
    "#                     }\n",
    "            \n",
    "#             recommendations['model_strengths'][model] = dict(model_strengths)\n",
    "        \n",
    "#         # Determine optimal assignments (highest combined score for each model)\n",
    "#         for model, strengths in recommendations['model_strengths'].items():\n",
    "#             if strengths:\n",
    "#                 best_roles = sorted(strengths.items(), key=lambda x: x[1]['combined_score'], reverse=True)\n",
    "#                 recommendations['optimal_assignments'][model] = [role for role, _ in best_roles[:2]]  # Top 2 roles\n",
    "        \n",
    "#         return recommendations\n",
    "    \n",
    "#     def _save_checkpoint(self, function_results: List[Dict], checkpoint_num: int):\n",
    "#         \"\"\"Save checkpoint of results\"\"\"\n",
    "#         checkpoint_path = f'experiment_checkpoint_{checkpoint_num}.json'\n",
    "#         try:\n",
    "#             # Convert defaultdict to regular dict for JSON serialization\n",
    "#             serializable_results = []\n",
    "#             for result in function_results:\n",
    "#                 serializable_result = result.copy()\n",
    "#                 serializable_result['model_role_category_matrix'] = {\n",
    "#                     model: {\n",
    "#                         role: dict(categories)\n",
    "#                         for role, categories in roles.items()\n",
    "#                     }\n",
    "#                     for model, roles in result['model_role_category_matrix'].items()\n",
    "#                 }\n",
    "#                 serializable_result['model_totals'] = dict(result['model_totals'])\n",
    "#                 serializable_result['role_totals'] = dict(result['role_totals'])\n",
    "#                 serializable_result['category_totals'] = dict(result['category_totals'])\n",
    "#                 serializable_results.append(serializable_result)\n",
    "            \n",
    "#             with open(checkpoint_path, 'w') as f:\n",
    "#                 json.dump(serializable_results, f, indent=2)\n",
    "#         except Exception as e:\n",
    "#             print(f\"⚠️ Could not save checkpoint: {e}\")\n",
    "    \n",
    "#     def _save_experiment_results(self, experiment_results: Dict):\n",
    "#         \"\"\"Save final experiment results\"\"\"\n",
    "#         output_path = f'role_assignment_experiment_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n",
    "        \n",
    "#         try:\n",
    "#             # Make results JSON serializable\n",
    "#             serializable_results = self._make_serializable(experiment_results)\n",
    "            \n",
    "#             with open(output_path, 'w') as f:\n",
    "#                 json.dump(serializable_results, f, indent=2)\n",
    "            \n",
    "#             print(f\"\\n💾 Results saved to: {output_path}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"⚠️ Could not save experiment results: {e}\")\n",
    "    \n",
    "#     def _make_serializable(self, obj):\n",
    "#         \"\"\"Convert defaultdicts and other non-serializable objects to regular dicts\"\"\"\n",
    "#         if isinstance(obj, defaultdict):\n",
    "#             return {k: self._make_serializable(v) for k, v in obj.items()}\n",
    "#         elif isinstance(obj, dict):\n",
    "#             return {k: self._make_serializable(v) for k, v in obj.items()}\n",
    "#         elif isinstance(obj, list):\n",
    "#             return [self._make_serializable(item) for item in obj]\n",
    "#         else:\n",
    "#             return obj\n",
    "    \n",
    "#     def _print_experiment_summary(self, experiment_results: Dict):\n",
    "#         \"\"\"Print a comprehensive summary of experiment results\"\"\"\n",
    "#         print(f\"\\n{'='*80}\")\n",
    "#         print(\"📊 ROLE ASSIGNMENT OPTIMIZATION EXPERIMENT SUMMARY\")\n",
    "#         print(f\"{'='*80}\\n\")\n",
    "        \n",
    "#         stats = experiment_results['aggregated_stats']\n",
    "#         recs = experiment_results['recommendations']\n",
    "        \n",
    "#         print(f\"Functions Processed: {experiment_results['n_functions_processed']}/{experiment_results['n_functions_target']}\")\n",
    "#         print(f\"Total Tests Generated: {stats['total_tests']}\")\n",
    "#         print(f\"Max Concurrent Requests: {experiment_results.get('max_concurrent_requests', 'N/A')}\")\n",
    "#         print(f\"\\n{'─'*80}\\n\")\n",
    "        \n",
    "#         # Model productivity\n",
    "#         print(\"📈 MODEL PRODUCTIVITY (Total Tests Generated):\")\n",
    "#         for model, count in sorted(stats['model_totals'].items(), key=lambda x: x[1], reverse=True):\n",
    "#             print(f\"   {model}: {count} tests\")\n",
    "        \n",
    "#         print(f\"\\n{'─'*80}\\n\")\n",
    "        \n",
    "#         # Role distribution\n",
    "#         print(\"🎭 ROLE DISTRIBUTION (Total Tests per Role):\")\n",
    "#         for role, count in sorted(stats['role_totals'].items(), key=lambda x: x[1], reverse=True):\n",
    "#             role_name = self.llm_council.roles[role]['name']\n",
    "#             print(f\"   {role_name}: {count} tests\")\n",
    "        \n",
    "#         print(f\"\\n{'─'*80}\\n\")\n",
    "        \n",
    "#         # Category distribution\n",
    "#         print(\"📁 CATEGORY DISTRIBUTION (Total Tests per Category):\")\n",
    "#         for category, count in sorted(stats['category_totals'].items(), key=lambda x: x[1], reverse=True):\n",
    "#             print(f\"   {category}: {count} tests\")\n",
    "        \n",
    "#         print(f\"\\n{'='*80}\\n\")\n",
    "#         print(\"🎯 RECOMMENDED OPTIMAL ROLE ASSIGNMENTS:\")\n",
    "#         print(f\"{'='*80}\\n\")\n",
    "        \n",
    "#         for model, roles in recs['optimal_assignments'].items():\n",
    "#             print(f\"💡 {model}:\")\n",
    "#             for role in roles:\n",
    "#                 role_name = self.llm_council.roles[role]['name']\n",
    "#                 strength = recs['model_strengths'][model][role]\n",
    "#                 print(f\"   → {role_name}\")\n",
    "#                 print(f\"      Alignment: {strength['alignment_score']:.2%}\")\n",
    "#                 print(f\"      Productivity: {strength['productivity_score']} tests\")\n",
    "#                 print(f\"      Combined Score: {strength['combined_score']:.3f}\")\n",
    "#                 print()\n",
    "        \n",
    "#         print(f\"{'='*80}\\n\")\n",
    "        \n",
    "#         # Create detailed performance matrix\n",
    "#         self._print_performance_matrix(stats, recs)\n",
    "    \n",
    "#     def _print_performance_matrix(self, stats: Dict, recs: Dict):\n",
    "#         \"\"\"Print detailed performance matrix\"\"\"\n",
    "#         print(\"📊 DETAILED PERFORMANCE MATRIX (Model × Role × Category):\")\n",
    "#         print(f\"{'='*80}\\n\")\n",
    "        \n",
    "#         for model in stats['model_role_category_totals'].keys():\n",
    "#             print(f\"🤖 {model}:\")\n",
    "#             print(f\"{'─'*76}\")\n",
    "            \n",
    "#             if model in recs['model_strengths']:\n",
    "#                 for role, strength_data in recs['model_strengths'][model].items():\n",
    "#                     role_name = self.llm_council.roles[role]['name']\n",
    "#                     categories = strength_data['category_distribution']\n",
    "                    \n",
    "#                     print(f\"\\n   🎭 {role_name}:\")\n",
    "#                     print(f\"      Focus Categories: {', '.join(strength_data['focus_categories'])}\")\n",
    "#                     print(f\"      Performance:\")\n",
    "                    \n",
    "#                     for category in sorted(categories.keys()):\n",
    "#                         count = categories[category]\n",
    "#                         is_focus = category in strength_data['focus_categories']\n",
    "#                         marker = \"★\" if is_focus else \" \"\n",
    "#                         print(f\"         {marker} {category}: {count} tests\")\n",
    "                    \n",
    "#                     print(f\"      → Alignment: {strength_data['alignment_score']:.2%} | \"\n",
    "#                           f\"Combined Score: {strength_data['combined_score']:.3f}\")\n",
    "            \n",
    "#             print()\n",
    "\n",
    "# # Initialize and run experiment\n",
    "# experiment = RoleAssignmentExperiment(llm_council, code_analyzer, test_classifier)\n",
    "\n",
    "# # Run the experiment with 20 random functions using concurrent API calls\n",
    "# print(\"⚡ Starting experiment with concurrent API processing...\")\n",
    "# results = experiment.run_experiment(\n",
    "#     dataset_path='data/python_algorithms_dataset.json',\n",
    "#     n_functions=20,\n",
    "#     seed=42\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "194dbff7-2025-4ff8-95ea-d4ce96b449f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 11: Analyze Role Assignment Results from Checkpoints\n",
    "# import json\n",
    "# import glob\n",
    "# from collections import defaultdict\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import numpy as np\n",
    "\n",
    "# class RoleAssignmentAnalyzer:\n",
    "#     \"\"\"Analyze checkpoint results to determine optimal model-role assignments\"\"\"\n",
    "    \n",
    "#     def __init__(self, roles_config):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             roles_config: Dictionary of role configurations from llm_council.roles\n",
    "#         \"\"\"\n",
    "#         self.roles_config = roles_config\n",
    "#         self.aggregated_data = None\n",
    "#         self.recommendations = None\n",
    "    \n",
    "#     def load_checkpoints(self, checkpoint_pattern='experiment_checkpoint_*.json'):\n",
    "#         \"\"\"Load all checkpoint files and aggregate results\"\"\"\n",
    "#         checkpoint_files = sorted(glob.glob(checkpoint_pattern), \n",
    "#                                  key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "        \n",
    "#         if not checkpoint_files:\n",
    "#             print(f\"❌ No checkpoint files found matching pattern: {checkpoint_pattern}\")\n",
    "#             return None\n",
    "        \n",
    "#         print(f\"📂 Found {len(checkpoint_files)} checkpoint files\")\n",
    "        \n",
    "#         # Aggregate all checkpoint data\n",
    "#         all_function_results = []\n",
    "#         for checkpoint_file in checkpoint_files:\n",
    "#             with open(checkpoint_file, 'r') as f:\n",
    "#                 checkpoint_data = json.load(f)\n",
    "#                 # Each checkpoint file contains a list with one function result\n",
    "#                 all_function_results.extend(checkpoint_data)\n",
    "        \n",
    "#         print(f\"✅ Loaded {len(all_function_results)} function results\")\n",
    "        \n",
    "#         # Aggregate statistics\n",
    "#         self.aggregated_data = self._aggregate_statistics(all_function_results)\n",
    "#         return self.aggregated_data\n",
    "    \n",
    "#     def _aggregate_statistics(self, function_results):\n",
    "#         \"\"\"Aggregate statistics across all function results\"\"\"\n",
    "#         aggregated = {\n",
    "#             'total_tests': 0,\n",
    "#             'model_role_category_matrix': defaultdict(lambda: defaultdict(lambda: defaultdict(int))),\n",
    "#             'model_totals': defaultdict(int),\n",
    "#             'role_totals': defaultdict(int),\n",
    "#             'category_totals': defaultdict(int)\n",
    "#         }\n",
    "        \n",
    "#         for func_result in function_results:\n",
    "#             aggregated['total_tests'] += func_result['total_tests_generated']\n",
    "            \n",
    "#             # Aggregate model × role × category\n",
    "#             for model, roles in func_result['model_role_category_matrix'].items():\n",
    "#                 for role, categories in roles.items():\n",
    "#                     for category, count in categories.items():\n",
    "#                         aggregated['model_role_category_matrix'][model][role][category] += count\n",
    "            \n",
    "#             # Aggregate totals\n",
    "#             for model, count in func_result['model_totals'].items():\n",
    "#                 aggregated['model_totals'][model] += count\n",
    "#             for role, count in func_result['role_totals'].items():\n",
    "#                 aggregated['role_totals'][role] += count\n",
    "#             for category, count in func_result['category_totals'].items():\n",
    "#                 aggregated['category_totals'][category] += count\n",
    "        \n",
    "#         return aggregated\n",
    "    \n",
    "#     def calculate_scores(self):\n",
    "#         \"\"\"Calculate alignment, productivity, and combined scores for each model-role combination\"\"\"\n",
    "#         if self.aggregated_data is None:\n",
    "#             print(\"❌ No aggregated data. Run load_checkpoints() first.\")\n",
    "#             return None\n",
    "        \n",
    "#         scores = defaultdict(lambda: defaultdict(dict))\n",
    "        \n",
    "#         for model, roles in self.aggregated_data['model_role_category_matrix'].items():\n",
    "#             for role, categories in roles.items():\n",
    "#                 role_info = self.roles_config[role]\n",
    "#                 focus_categories = role_info['focus_categories']\n",
    "                \n",
    "#                 # Calculate metrics\n",
    "#                 focus_count = sum(categories.get(cat, 0) for cat in focus_categories)\n",
    "#                 total_count = sum(categories.values())\n",
    "                \n",
    "#                 if total_count > 0:\n",
    "#                     alignment_score = focus_count / total_count\n",
    "#                     productivity_score = total_count\n",
    "#                     # Combined score: weighted average (60% alignment, 40% normalized productivity)\n",
    "#                     combined_score = (alignment_score * 0.6) + (min(productivity_score / 10, 1.0) * 0.4)\n",
    "                    \n",
    "#                     scores[model][role] = {\n",
    "#                         'alignment': alignment_score,\n",
    "#                         'productivity': productivity_score,\n",
    "#                         'combined': combined_score,\n",
    "#                         'focus_categories': focus_categories,\n",
    "#                         'category_distribution': dict(categories)\n",
    "#                     }\n",
    "        \n",
    "#         self.recommendations = dict(scores)\n",
    "#         return self.recommendations\n",
    "    \n",
    "#     def create_summary_dataframes(self):\n",
    "#         \"\"\"Create pandas DataFrames for easy viewing and analysis\"\"\"\n",
    "#         if self.recommendations is None:\n",
    "#             print(\"❌ No recommendations. Run calculate_scores() first.\")\n",
    "#             return None\n",
    "        \n",
    "#         # Create separate DataFrames for each metric\n",
    "#         models = sorted(self.recommendations.keys())\n",
    "#         roles = sorted(set(role for model_roles in self.recommendations.values() \n",
    "#                           for role in model_roles.keys()))\n",
    "        \n",
    "#         # Productivity DataFrame\n",
    "#         productivity_data = []\n",
    "#         for model in models:\n",
    "#             row = {'Model': model}\n",
    "#             for role in roles:\n",
    "#                 if role in self.recommendations[model]:\n",
    "#                     row[self.roles_config[role]['name']] = self.recommendations[model][role]['productivity']\n",
    "#                 else:\n",
    "#                     row[self.roles_config[role]['name']] = 0\n",
    "#             productivity_data.append(row)\n",
    "#         df_productivity = pd.DataFrame(productivity_data).set_index('Model')\n",
    "        \n",
    "#         # Alignment DataFrame\n",
    "#         alignment_data = []\n",
    "#         for model in models:\n",
    "#             row = {'Model': model}\n",
    "#             for role in roles:\n",
    "#                 if role in self.recommendations[model]:\n",
    "#                     row[self.roles_config[role]['name']] = self.recommendations[model][role]['alignment']\n",
    "#                 else:\n",
    "#                     row[self.roles_config[role]['name']] = 0\n",
    "#             alignment_data.append(row)\n",
    "#         df_alignment = pd.DataFrame(alignment_data).set_index('Model')\n",
    "        \n",
    "#         # Combined Score DataFrame\n",
    "#         combined_data = []\n",
    "#         for model in models:\n",
    "#             row = {'Model': model}\n",
    "#             for role in roles:\n",
    "#                 if role in self.recommendations[model]:\n",
    "#                     row[self.roles_config[role]['name']] = self.recommendations[model][role]['combined']\n",
    "#                 else:\n",
    "#                     row[self.roles_config[role]['name']] = 0\n",
    "#             combined_data.append(row)\n",
    "#         df_combined = pd.DataFrame(combined_data).set_index('Model')\n",
    "        \n",
    "#         return {\n",
    "#             'productivity': df_productivity,\n",
    "#             'alignment': df_alignment,\n",
    "#             'combined': df_combined\n",
    "#         }\n",
    "    \n",
    "#     def visualize_results(self, figsize=(20, 12)):\n",
    "#         \"\"\"Create comprehensive visualizations of model-role performance\"\"\"\n",
    "#         dataframes = self.create_summary_dataframes()\n",
    "#         if dataframes is None:\n",
    "#             return\n",
    "        \n",
    "#         fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "#         fig.suptitle('Model-Role Performance Analysis', fontsize=16, fontweight='bold', y=1.00)\n",
    "        \n",
    "#         # 1. Productivity Heatmap\n",
    "#         ax1 = axes[0, 0]\n",
    "#         sns.heatmap(dataframes['productivity'], annot=True, fmt='.0f', cmap='YlOrRd', \n",
    "#                    ax=ax1, cbar_kws={'label': 'Tests Generated'})\n",
    "#         ax1.set_title('Productivity Score (Total Tests Generated)', fontweight='bold')\n",
    "#         ax1.set_xlabel('')\n",
    "        \n",
    "#         # 2. Alignment Heatmap\n",
    "#         ax2 = axes[0, 1]\n",
    "#         sns.heatmap(dataframes['alignment'], annot=True, fmt='.2%', cmap='YlGnBu', \n",
    "#                    ax=ax2, cbar_kws={'label': 'Alignment Score'}, vmin=0, vmax=1)\n",
    "#         ax2.set_title('Alignment Score (Focus Category Accuracy)', fontweight='bold')\n",
    "#         ax2.set_xlabel('')\n",
    "        \n",
    "#         # 3. Combined Score Heatmap\n",
    "#         ax3 = axes[1, 0]\n",
    "#         sns.heatmap(dataframes['combined'], annot=True, fmt='.3f', cmap='RdYlGn', \n",
    "#                    ax=ax3, cbar_kws={'label': 'Combined Score'}, vmin=0, vmax=1)\n",
    "#         ax3.set_title('Combined Score (60% Alignment + 40% Productivity)', fontweight='bold')\n",
    "#         ax3.set_xlabel('')\n",
    "        \n",
    "#         # 4. Best Model per Role (Bar Chart)\n",
    "#         ax4 = axes[1, 1]\n",
    "#         best_models_per_role = {}\n",
    "#         for role in dataframes['combined'].columns:\n",
    "#             best_model = dataframes['combined'][role].idxmax()\n",
    "#             best_score = dataframes['combined'][role].max()\n",
    "#             best_models_per_role[role] = (best_model, best_score)\n",
    "        \n",
    "#         roles_list = list(best_models_per_role.keys())\n",
    "#         scores_list = [score for _, score in best_models_per_role.values()]\n",
    "#         models_list = [model for model, _ in best_models_per_role.values()]\n",
    "        \n",
    "#         colors = plt.cm.Set3(np.linspace(0, 1, len(set(models_list))))\n",
    "#         model_to_color = {model: colors[i] for i, model in enumerate(sorted(set(models_list)))}\n",
    "#         bar_colors = [model_to_color[model] for model in models_list]\n",
    "        \n",
    "#         bars = ax4.barh(roles_list, scores_list, color=bar_colors)\n",
    "#         ax4.set_xlabel('Combined Score', fontweight='bold')\n",
    "#         ax4.set_title('Best Model per Role', fontweight='bold')\n",
    "#         ax4.set_xlim(0, 1)\n",
    "        \n",
    "#         # Add model names on bars\n",
    "#         for i, (bar, model) in enumerate(zip(bars, models_list)):\n",
    "#             width = bar.get_width()\n",
    "#             ax4.text(width + 0.02, bar.get_y() + bar.get_height()/2, \n",
    "#                     f'{model}', ha='left', va='center', fontsize=9)\n",
    "        \n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "    \n",
    "#     def print_recommendations(self):\n",
    "#         \"\"\"Print detailed recommendations for role assignments\"\"\"\n",
    "#         if self.recommendations is None:\n",
    "#             print(\"❌ No recommendations. Run calculate_scores() first.\")\n",
    "#             return\n",
    "        \n",
    "#         print(f\"\\n{'='*80}\")\n",
    "#         print(\"🎯 ROLE ASSIGNMENT RECOMMENDATIONS\")\n",
    "#         print(f\"{'='*80}\\n\")\n",
    "        \n",
    "#         # For each role, show which model performs best\n",
    "#         roles = sorted(set(role for model_roles in self.recommendations.values() \n",
    "#                           for role in model_roles.keys()))\n",
    "        \n",
    "#         for role in roles:\n",
    "#             role_name = self.roles_config[role]['name']\n",
    "#             focus_cats = self.roles_config[role]['focus_categories']\n",
    "            \n",
    "#             print(f\"\\n{'─'*80}\")\n",
    "#             print(f\"📋 Role: {role_name}\")\n",
    "#             print(f\"   Focus Categories: {', '.join(focus_cats)}\")\n",
    "#             print(f\"{'─'*80}\")\n",
    "            \n",
    "#             # Collect scores for this role across all models\n",
    "#             model_scores = []\n",
    "#             for model in sorted(self.recommendations.keys()):\n",
    "#                 if role in self.recommendations[model]:\n",
    "#                     scores = self.recommendations[model][role]\n",
    "#                     model_scores.append((model, scores))\n",
    "            \n",
    "#             # Sort by combined score\n",
    "#             model_scores.sort(key=lambda x: x[1]['combined'], reverse=True)\n",
    "            \n",
    "#             print(f\"\\n   Model Performance Ranking:\\n\")\n",
    "#             for rank, (model, scores) in enumerate(model_scores, 1):\n",
    "#                 marker = \"🥇\" if rank == 1 else \"🥈\" if rank == 2 else \"🥉\" if rank == 3 else f\"  {rank}.\"\n",
    "#                 print(f\"   {marker} {model}\")\n",
    "#                 print(f\"      ├─ Productivity: {scores['productivity']} tests\")\n",
    "#                 print(f\"      ├─ Alignment:    {scores['alignment']:.1%}\")\n",
    "#                 print(f\"      └─ Combined:     {scores['combined']:.3f}\")\n",
    "                \n",
    "#                 if rank == 1:\n",
    "#                     print(f\"      ✅ RECOMMENDED for this role\")\n",
    "#                 print()\n",
    "        \n",
    "#         print(f\"\\n{'='*80}\")\n",
    "#         print(\"💡 SUMMARY: Optimal Model-Role Assignments\")\n",
    "#         print(f\"{'='*80}\\n\")\n",
    "        \n",
    "#         # Show best model for each role\n",
    "#         for role in roles:\n",
    "#             role_name = self.roles_config[role]['name']\n",
    "#             best_model = None\n",
    "#             best_score = -1\n",
    "            \n",
    "#             for model in self.recommendations.keys():\n",
    "#                 if role in self.recommendations[model]:\n",
    "#                     if self.recommendations[model][role]['combined'] > best_score:\n",
    "#                         best_score = self.recommendations[model][role]['combined']\n",
    "#                         best_model = model\n",
    "            \n",
    "#             if best_model:\n",
    "#                 print(f\"   {role_name:30s} → {best_model}\")\n",
    "        \n",
    "#         print(f\"\\n{'='*80}\\n\")\n",
    "\n",
    "# # Create analyzer instance\n",
    "# analyzer = RoleAssignmentAnalyzer(llm_council.roles)\n",
    "\n",
    "# # Load and analyze checkpoint data\n",
    "# print(\"📊 Loading checkpoint files...\")\n",
    "# aggregated_data = analyzer.load_checkpoints('experiment_checkpoint_*.json')\n",
    "\n",
    "# if aggregated_data:\n",
    "#     print(\"\\n🔍 Calculating performance scores...\")\n",
    "#     recommendations = analyzer.calculate_scores()\n",
    "    \n",
    "#     print(\"\\n📈 Creating visualizations...\")\n",
    "#     analyzer.visualize_results(figsize=(20, 12))\n",
    "    \n",
    "#     print(\"\\n📋 Generating recommendations...\")\n",
    "#     analyzer.print_recommendations()\n",
    "    \n",
    "#     # Show summary tables\n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "#     print(\"📊 DETAILED SCORE TABLES\")\n",
    "#     print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "#     dfs = analyzer.create_summary_dataframes()\n",
    "    \n",
    "#     print(\"\\n1️⃣ PRODUCTIVITY SCORES (Total Tests Generated):\")\n",
    "#     print(dfs['productivity'].to_string())\n",
    "    \n",
    "#     print(\"\\n\\n2️⃣ ALIGNMENT SCORES (Focus Category Accuracy):\")\n",
    "#     print(dfs['alignment'].applymap(lambda x: f\"{x:.1%}\").to_string())\n",
    "    \n",
    "#     print(\"\\n\\n3️⃣ COMBINED SCORES (Weighted Performance):\")\n",
    "#     print(dfs['combined'].applymap(lambda x: f\"{x:.3f}\").to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73375304-0d18-482b-9ea6-dcda44d41ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting async demo with concurrent API calls...\n",
      "🎯 Demonstrating Intelligent Test Council (Async + Concurrent Mode)\n",
      "======================================================================\n",
      "⚡ Maximum concurrent API requests: 7\n",
      "======================================================================\n",
      "🚀 Starting Role-Based Intelligent Test Council Pipeline (Async Mode)\n",
      "======================================================================\n",
      "\n",
      "📝 Step 1: Analyzing input function...\n",
      "✅ Found 1 function(s)\n",
      "\n",
      "🎭 Step 2: Consulting Role-Based LLM Council (Concurrent Mode)...\n",
      "🤖 Consulting Role-Based LLM Council for test generation (Concurrent Mode)...\n",
      "======================================================================\n",
      "⚡ Maximum concurrent requests: 7\n",
      "📊 Total API calls to make: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concurrent API calls: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:43<00:00,  6.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ gemini-2.0-flash as 'By-the-Book QA Engineer': 12 tests\n",
      "✅ gemini-2.0-flash as 'Abstract Thinker': 15 tests\n",
      "✅ qwen3-235b-a22b as 'Agent of Chaos': 20 tests\n",
      "✅ grok-3-mini as 'By-the-Book QA Engineer': 9 tests\n",
      "✅ qwen3-235b-a22b as 'Agent of Chaos': 20 tests\n",
      "✅ gemini-2.0-flash as 'Abstract Thinker': 16 tests\n",
      "✅ grok-3-mini as 'Paranoid Security Auditor': 15 tests\n",
      "======================================================================\n",
      "\n",
      "🏷️  Step 3: Classifying test cases by category and role...\n",
      "✅ Total tests generated: 107\n",
      "\n",
      "🎭 Role distribution:\n",
      "   • By-the-Book QA Engineer: 21 tests\n",
      "   • Abstract Thinker: 31 tests\n",
      "   • Agent of Chaos: 40 tests\n",
      "   • Paranoid Security Auditor: 15 tests\n",
      "\n",
      "📊 Category distribution:\n",
      "   • positive: 30 tests\n",
      "   • boundary: 16 tests\n",
      "   • negative: 21 tests\n",
      "   • edge_case: 21 tests\n",
      "   • security: 19 tests\n",
      "\n",
      "🔬 Model-Role Performance Matrix:\n",
      "\n",
      "   gemini-2.0-flash:\n",
      "      └─ By-the-Book QA Engineer: 12 tests\n",
      "      └─ Abstract Thinker: 15 tests\n",
      "      └─ Abstract Thinker: 16 tests\n",
      "\n",
      "   qwen3-235b-a22b:\n",
      "      └─ Agent of Chaos: 20 tests\n",
      "      └─ Agent of Chaos: 20 tests\n",
      "\n",
      "   grok-3-mini:\n",
      "      └─ By-the-Book QA Engineer: 9 tests\n",
      "      └─ Paranoid Security Auditor: 15 tests\n",
      "\n",
      "🔬 Step 4: Hybrid Cluster-then-Synthesize Deduplication...\n",
      "🔬 Starting Hybrid Cluster-then-Synthesize Pipeline\n",
      "======================================================================\n",
      "\n",
      "📊 Stage 1: AST-Based Structural Clustering\n",
      "🔬 Clustering 107 tests using vector method...\n",
      "⚠️  Syntax error in test code: invalid syntax (<unknown>, line 7)\n",
      "⚠️  Syntax error in test code: invalid syntax (<unknown>, line 7)\n",
      "⚠️  Syntax error in test code: unexpected EOF while parsing (<unknown>, line 9)\n",
      "⚠️  Syntax error in test code: invalid syntax (<unknown>, line 8)\n",
      "⚠️  Syntax error in test code: invalid syntax (<unknown>, line 5)\n",
      "⚠️  Syntax error in test code: invalid syntax (<unknown>, line 10)\n",
      "⚠️  Syntax error in test code: invalid syntax (<unknown>, line 14)\n",
      "✅ Vector-based clustering complete:\n",
      "   • Total clusters: 30\n",
      "   • Singleton clusters: 15\n",
      "   • Multi-test clusters: 15\n",
      "   • Average cluster size: 3.57\n",
      "\n",
      "🤖 Stage 2: LLM-Powered Cluster Synthesis\n",
      "   Synthesizing 30 clusters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Synthesizing clusters: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [02:06<00:00,  4.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 Stage 3: LLM-Powered Final Test File Generation...\n",
      "   Using gemini-2.0-flash to generate clean, unified test file...\n",
      "✅ LLM-generated final test file created successfully\n",
      "\n",
      "✅ Hybrid synthesis complete!\n",
      "   📊 Original tests: 107\n",
      "   🔬 Clusters identified: 30\n",
      "   ✨ Final unique tests: 30\n",
      "   📉 Reduction: 72.0%\n",
      "======================================================================\n",
      "\n",
      "💾 Step 5: Saving results to test_results/...\n",
      "   ✅ Saved source function to: test_results/function.py\n",
      "   ✅ Saved test file to: test_results/test_function.py\n",
      "\n",
      "📊 Step 6: Analyzing code coverage...\n",
      "📊 Analyzing code coverage...\n",
      "   Using output directory: test_results\n",
      "✅ Coverage analysis complete:\n",
      "   • Code coverage: 0.0%\n",
      "   • Tests run: 0\n",
      "   • Tests passed: 0\n",
      "   • Tests failed: 0\n",
      "\n",
      "🎉 Pipeline completed successfully!\n",
      "📊 Test Success Rate: 0.0%\n",
      "📈 Code Coverage: 0.0%\n",
      "✅ Passed Tests: 0/0\n",
      "📁 Output directory: test_results/\n",
      "======================================================================\n",
      "\n",
      "📊 Key Statistics:\n",
      "   • Execution time: 211.00 seconds\n",
      "   • Original tests generated: 107\n",
      "   • Final tests after synthesis: 30\n",
      "   • Reduction ratio: 71.96%\n",
      "   • Code coverage: 0.0%\n",
      "   • Test success rate: 0.0%\n",
      "   • Models used: gemini-2.0-flash, qwen3-235b-a22b, grok-3-mini\n",
      "   • Roles used: Agent of Chaos, Paranoid Security Auditor, By-the-Book QA Engineer, Abstract Thinker\n",
      "   • Test categories: positive, boundary, negative, edge_case, security\n",
      "   • Synthesizer model: gemini-2.0-flash\n",
      "\n",
      "🎭 Role-Based Generation Summary:\n",
      "   • By-the-Book QA Engineer: 21 tests\n",
      "   • Abstract Thinker: 31 tests\n",
      "   • Agent of Chaos: 40 tests\n",
      "   • Paranoid Security Auditor: 15 tests\n",
      "\n",
      "💾 All results already saved to: test_results/\n",
      "\n",
      "📋 Final Test File Preview:\n",
      "----------------------------------------------------------------------\n",
      "\"\"\"\n",
      "This is an auto-generated comprehensive test suite for the divide_numbers function.\n",
      "It covers positive, negative, boundary, edge case, and security scenarios.\n",
      "\"\"\"\n",
      "\n",
      "import pytest\n",
      "import math\n",
      "import sys\n",
      "from function import divide_numbers\n",
      "\n",
      "# ======================================================================\n",
      "# CATEGORY: POSITIVE (4 tests)\n",
      "# ======================================================================\n",
      "\n",
      "def test_divide_numbers_various_valid_inputs():\n",
      "    \"\"\"\n",
      "    Verifies division with a variety of valid inputs, including positive integers,\n",
      "    positive floats, zero numerator, dividing by one, dividing by itself,\n",
      "    and dividing by a very large number.\n",
      "    \"\"\"\n",
      "    assert divide_numbers(10, 2) == 5.0, \"Should return the correct quotient for positive integers\"\n",
      "    assert divide_numbers(7.5, 2.5) == 3.0, \"Should return the correct quotient for floating-point numbers\"\n",
      "    assert divide_numbers(0, 5) == 0.0, \"Should return zero when numerator is zero\"\n",
      "    assert divide_numbers(5, 1) == 5.0, \"Dividing by 1 should return the original number\"\n",
      "    assert divide_numbers(7, 7) == 1.0, \"Dividing a number by itself should return 1.0\"\n",
      "    assert divide_numbers(1000000, 100) == 10000.0, \"Should return the correct quotient for large numbers\"\n",
      "    assert divide_numbers(10, 1e1000) == 0.0, \"Result should be approximately 0.0 when dividing by a very large number\"\n",
      "\n",
      "\n",
      "def test_divide_numbers_mixed_signs():\n",
      "    \"\"\"\n",
      "    Verifies the division of numbers with mixed signs, including cases w\n",
      "... (truncated)\n",
      "\n",
      "⏱️  Total execution time: 211.00 seconds\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Async Demo with Concurrent API Calls (Updated)\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "\n",
    "# Enable nested asyncio for Jupyter\n",
    "nest_asyncio.apply()\n",
    "\n",
    "class AsyncIntelligentTestCouncil(IntelligentTestCouncil):\n",
    "    \"\"\"Async version of the test council with concurrent API calls\"\"\"\n",
    "    \n",
    "    async def generate_comprehensive_tests_async(self, function_code: str, \n",
    "                                                 max_concurrent: int = 7,\n",
    "                                                 clustering_method: str = 'vector',\n",
    "                                                 output_dir: str = 'test_results') -> Dict[str, Any]:\n",
    "        \"\"\"Async version of main pipeline with concurrent API calls\"\"\"\n",
    "        print(\"🚀 Starting Role-Based Intelligent Test Council Pipeline (Async Mode)\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Step 1: Analyze the input function\n",
    "        print(\"\\n📝 Step 1: Analyzing input function...\")\n",
    "        function_info = self.code_analyzer.extract_function_info(function_code)\n",
    "        \n",
    "        if not function_info['functions']:\n",
    "            error_msg = 'No functions found in the provided code'\n",
    "            if 'syntax_error' in function_info:\n",
    "                error_msg += f\". Syntax error: {function_info['syntax_error']}\"\n",
    "            return {'error': error_msg}\n",
    "        \n",
    "        print(f\"✅ Found {function_info['total_functions']} function(s)\")\n",
    "        \n",
    "        # Step 2: Generate tests using role-based LLM council with CONCURRENT API calls\n",
    "        print(f\"\\n🎭 Step 2: Consulting Role-Based LLM Council (Concurrent Mode)...\")\n",
    "        council_results = await self.llm_council.generate_tests_from_council_async(\n",
    "            function_info, \n",
    "            max_concurrent=max_concurrent\n",
    "        )\n",
    "        \n",
    "        # Step 3: Classify all test cases\n",
    "        print(\"\\n🏷️  Step 3: Classifying test cases by category and role...\")\n",
    "        all_classified_tests = self.test_classifier.classify_council_results(council_results)\n",
    "        \n",
    "        print(f\"✅ Total tests generated: {len(all_classified_tests)}\")\n",
    "        \n",
    "        # Display role distribution\n",
    "        role_counts = Counter(test['role_name'] for test in all_classified_tests)\n",
    "        print(\"\\n🎭 Role distribution:\")\n",
    "        for role_name, count in role_counts.items():\n",
    "            print(f\"   • {role_name}: {count} tests\")\n",
    "        \n",
    "        # Display category distribution\n",
    "        category_counts = Counter(test['category'] for test in all_classified_tests)\n",
    "        print(\"\\n📊 Category distribution:\")\n",
    "        for category, count in category_counts.items():\n",
    "            print(f\"   • {category}: {count} tests\")\n",
    "        \n",
    "        # Display model-role performance matrix\n",
    "        print(\"\\n🔬 Model-Role Performance Matrix:\")\n",
    "        for model_name, role_results in council_results.items():\n",
    "            print(f\"\\n   {model_name}:\")\n",
    "            for role_id, results in role_results.items():\n",
    "                print(f\"      └─ {results['role_name']}: {results['test_count']} tests\")\n",
    "        \n",
    "        # Step 4: Hybrid Cluster-then-Synthesize approach\n",
    "        print(f\"\\n🔬 Step 4: Hybrid Cluster-then-Synthesize Deduplication...\")\n",
    "        synthesis_results = self.test_synthesizer.synthesize_final_test_file(\n",
    "            all_classified_tests, function_info, clustering_method=clustering_method\n",
    "        )\n",
    "        \n",
    "        # Step 5: Save results to output directory\n",
    "        print(f\"\\n💾 Step 5: Saving results to {output_dir}/...\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save source function\n",
    "        function_file_path = os.path.join(output_dir, 'function.py')\n",
    "        with open(function_file_path, 'w') as f:\n",
    "            f.write(function_code)\n",
    "        print(f\"   ✅ Saved source function to: {function_file_path}\")\n",
    "        \n",
    "        # Save test file\n",
    "        test_file_path = os.path.join(output_dir, 'test_function.py')\n",
    "        with open(test_file_path, 'w') as f:\n",
    "            f.write(synthesis_results['synthesized_content'])\n",
    "        print(f\"   ✅ Saved test file to: {test_file_path}\")\n",
    "        \n",
    "        # Step 6: Analyze coverage using the saved files\n",
    "        print(\"\\n📊 Step 6: Analyzing code coverage...\")\n",
    "        coverage_results = self.coverage_analyzer.analyze_coverage(\n",
    "            function_code, \n",
    "            synthesis_results['synthesized_content'],\n",
    "            output_dir=output_dir\n",
    "        )\n",
    "        \n",
    "        # Prepare comprehensive results\n",
    "        results = {\n",
    "            'function_info': function_info,\n",
    "            'council_results': council_results,\n",
    "            'all_classified_tests': all_classified_tests,\n",
    "            'synthesis_results': synthesis_results,\n",
    "            'final_test_file': synthesis_results['synthesized_content'],\n",
    "            'coverage_results': coverage_results,\n",
    "            'output_dir': output_dir,\n",
    "            'statistics': {\n",
    "                'original_test_count': len(all_classified_tests),\n",
    "                'final_test_count': synthesis_results['final_count'],\n",
    "                'cluster_count': synthesis_results['cluster_count'],\n",
    "                'reduction_ratio': synthesis_results['reduction_ratio'],\n",
    "                'clustering_method': clustering_method,\n",
    "                'coverage_percentage': coverage_results.get('coverage_percentage', 0.0),\n",
    "                'test_success_rate': coverage_results.get('success_rate', 0.0),\n",
    "                'total_tests_run': coverage_results.get('total_tests', 0),\n",
    "                'passed_tests': coverage_results.get('passed_tests', 0),\n",
    "                'failed_tests': coverage_results.get('failed_tests', 0),\n",
    "                'skipped_tests': coverage_results.get('skipped_tests', 0),\n",
    "                'error_tests': coverage_results.get('error_tests', 0),\n",
    "                'models_used': list(council_results.keys()),\n",
    "                'roles_used': list(set(test['role_name'] for test in all_classified_tests)),\n",
    "                'categories_found': list(category_counts.keys()),\n",
    "                'synthesizer_model': synthesis_results['synthesizer_model'],\n",
    "                'finalizer_model': synthesis_results.get('finalizer_model', 'fallback'),\n",
    "                'tests_per_role': dict(role_counts),\n",
    "                'tests_per_category': dict(category_counts),\n",
    "                'model_role_matrix': {\n",
    "                    model: {role: results['test_count'] for role, results in roles.items()}\n",
    "                    for model, roles in council_results.items()\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save additional metadata\n",
    "        self._save_metadata(results, output_dir)\n",
    "        \n",
    "        print(\"\\n🎉 Pipeline completed successfully!\")\n",
    "        print(f\"📊 Test Success Rate: {coverage_results.get('success_rate', 0.0):.1f}%\")\n",
    "        print(f\"📈 Code Coverage: {coverage_results.get('coverage_percentage', 0.0):.1f}%\")\n",
    "        print(f\"✅ Passed Tests: {coverage_results.get('passed_tests', 0)}/{coverage_results.get('total_tests', 0)}\")\n",
    "        print(f\"📁 Output directory: {output_dir}/\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Example function to test\n",
    "example_function_1 = '''def divide_numbers(a, b):\n",
    "    \"\"\"\n",
    "    Divide two numbers with error handling\n",
    "\n",
    "    Args:\n",
    "        a (float): Numerator\n",
    "        b (float): Denominator\n",
    "\n",
    "    Returns:\n",
    "        float: Result of division\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If denominator is zero\n",
    "        TypeError: If inputs are not numeric\n",
    "    \"\"\"\n",
    "    if not isinstance(a, (int, float)) or not isinstance(b, (int, float)):\n",
    "        raise TypeError(\"Both arguments must be numeric\")\n",
    "\n",
    "    if b == 0:\n",
    "        raise ValueError(\"Cannot divide by zero\")\n",
    "\n",
    "    return a / b'''\n",
    "\n",
    "# Example 2: More complex function (PROPERLY INDENTED)\n",
    "example_function_2 = '''def validate_password(password):\n",
    "    \"\"\"\n",
    "    Validate password strength\n",
    "\n",
    "    Args:\n",
    "        password (str): Password to validate\n",
    "\n",
    "    Returns:\n",
    "        dict: Validation results with 'valid' boolean and 'errors' list\n",
    "    \"\"\"\n",
    "    if not isinstance(password, str):\n",
    "        return {'valid': False, 'errors': ['Password must be a string']}\n",
    "\n",
    "    errors = []\n",
    "\n",
    "    if len(password) < 8:\n",
    "        errors.append('Password must be at least 8 characters long')\n",
    "\n",
    "    if not any(c.isupper() for c in password):\n",
    "        errors.append('Password must contain at least one uppercase letter')\n",
    "\n",
    "    if not any(c.islower() for c in password):\n",
    "        errors.append('Password must contain at least one lowercase letter')\n",
    "\n",
    "    if not any(c.isdigit() for c in password):\n",
    "        errors.append('Password must contain at least one digit')\n",
    "\n",
    "    special_chars = '!@#$%^&*(),.?\":{}|<>'\n",
    "    if not any(c in special_chars for c in password):\n",
    "        errors.append('Password must contain at least one special character')\n",
    "\n",
    "    return {'valid': len(errors) == 0, 'errors': errors}'''\n",
    "\n",
    "async def demonstrate_council_async(max_concurrent: int = 7):\n",
    "    \"\"\"Async demonstration of the intelligent council with concurrent API calls\"\"\"\n",
    "    \n",
    "    print(\"🎯 Demonstrating Intelligent Test Council (Async + Concurrent Mode)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"⚡ Maximum concurrent API requests: {max_concurrent}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Initialize async version of the council\n",
    "    async_council = AsyncIntelligentTestCouncil(config)\n",
    "    \n",
    "    # Choose example to run\n",
    "    selected_function = example_function_1\n",
    "    \n",
    "    try:\n",
    "        # Run the intelligent council pipeline with concurrent API calls\n",
    "        results = await async_council.generate_comprehensive_tests_async(\n",
    "            selected_function,\n",
    "            max_concurrent=max_concurrent\n",
    "        )\n",
    "        \n",
    "        if 'error' in results:\n",
    "            print(f\"❌ Error: {results['error']}\")\n",
    "            return results\n",
    "        \n",
    "        # Calculate execution time\n",
    "        end_time = datetime.now()\n",
    "        execution_time = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        # Display key statistics\n",
    "        stats = results['statistics']\n",
    "        print(f\"\\n📊 Key Statistics:\")\n",
    "        print(f\"   • Execution time: {execution_time:.2f} seconds\")\n",
    "        print(f\"   • Original tests generated: {stats['original_test_count']}\")\n",
    "        print(f\"   • Final tests after synthesis: {stats['final_test_count']}\")\n",
    "        print(f\"   • Reduction ratio: {stats['reduction_ratio']:.2%}\")\n",
    "        print(f\"   • Code coverage: {stats['coverage_percentage']:.1f}%\")\n",
    "        print(f\"   • Test success rate: {stats['test_success_rate']:.1f}%\")\n",
    "        print(f\"   • Models used: {', '.join(stats['models_used'])}\")\n",
    "        print(f\"   • Roles used: {', '.join(stats['roles_used'])}\")\n",
    "        print(f\"   • Test categories: {', '.join(stats['categories_found'])}\")\n",
    "        print(f\"   • Synthesizer model: {stats['synthesizer_model']}\")\n",
    "        \n",
    "        # Display role-based metrics\n",
    "        print(f\"\\n🎭 Role-Based Generation Summary:\")\n",
    "        for role, count in stats['tests_per_role'].items():\n",
    "            print(f\"   • {role}: {count} tests\")\n",
    "        \n",
    "        # Note: Results are already saved during generate_comprehensive_tests_async\n",
    "        print(f\"\\n💾 All results already saved to: {results['output_dir']}/\")\n",
    "        \n",
    "        # Display final test file preview\n",
    "        print(f\"\\n📋 Final Test File Preview:\")\n",
    "        print(\"-\" * 70)\n",
    "        preview_length = 1500\n",
    "        if len(results['final_test_file']) > preview_length:\n",
    "            print(results['final_test_file'][:preview_length] + \"\\n... (truncated)\")\n",
    "        else:\n",
    "            print(results['final_test_file'])\n",
    "        \n",
    "        print(f\"\\n⏱️  Total execution time: {execution_time:.2f} seconds\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Pipeline failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {'error': str(e)}\n",
    "\n",
    "# Run the async demo\n",
    "print(\"🚀 Starting async demo with concurrent API calls...\")\n",
    "demo_results = await demonstrate_council_async(max_concurrent=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d1665fd-d2c6-41b8-af17-aca35b4e2738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 14: Batch Testing and Evaluation (FIXED)\n",
    "# def batch_evaluate_functions(function_list: List[str], output_file: str = \"batch_evaluation.csv\"):\n",
    "#     \"\"\"Evaluate multiple functions in batch and generate comparison report\"\"\"\n",
    "    \n",
    "#     print(f\"🔄 Starting batch evaluation of {len(function_list)} functions...\")\n",
    "    \n",
    "#     results_data = []\n",
    "    \n",
    "#     for i, func_code in enumerate(tqdm(function_list, desc=\"Processing functions\")):\n",
    "#         try:\n",
    "#             print(f\"\\n📝 Processing function {i+1}/{len(function_list)}\")\n",
    "            \n",
    "#             # Run council pipeline\n",
    "#             results = intelligent_council.generate_comprehensive_tests(func_code)\n",
    "            \n",
    "#             if 'error' in results:\n",
    "#                 print(f\"❌ Error processing function {i+1}: {results['error']}\")\n",
    "#                 row_data = {\n",
    "#                     'function_name': f'function_{i+1}',\n",
    "#                     'original_tests': 0,\n",
    "#                     'final_tests': 0,\n",
    "#                     'reduction_ratio': 0,\n",
    "#                     'coverage_percentage': 0,\n",
    "#                     'models_used': 0,\n",
    "#                     'categories_count': 0,\n",
    "#                     'categories': '',\n",
    "#                     'error': results['error'],\n",
    "#                     'success': False\n",
    "#                 }\n",
    "#                 results_data.append(row_data)\n",
    "#                 continue\n",
    "            \n",
    "#             # Extract key metrics\n",
    "#             stats = results['statistics']\n",
    "#             func_info = results['function_info']\n",
    "#             func_name = func_info['functions'][0]['name'] if func_info['functions'] else f\"function_{i+1}\"\n",
    "            \n",
    "#             row_data = {\n",
    "#                 'function_name': func_name,\n",
    "#                 'original_tests': stats['original_test_count'],\n",
    "#                 'final_tests': stats['final_test_count'],\n",
    "#                 'reduction_ratio': stats['reduction_ratio'],\n",
    "#                 'coverage_percentage': stats['coverage_percentage'],\n",
    "#                 'models_used': len(stats['models_used']),\n",
    "#                 'categories_count': len(stats['categories_found']),\n",
    "#                 'categories': ','.join(stats['categories_found']),\n",
    "#                 'synthesizer_model': stats['synthesizer_model'],\n",
    "#                 'success': True\n",
    "#             }\n",
    "            \n",
    "#             results_data.append(row_data)\n",
    "#             print(f\"✅ Function {i+1} processed successfully\")\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"❌ Error processing function {i+1}: {e}\")\n",
    "#             results_data.append({\n",
    "#                 'function_name': f'function_{i+1}',\n",
    "#                 'original_tests': 0,\n",
    "#                 'final_tests': 0,\n",
    "#                 'reduction_ratio': 0,\n",
    "#                 'coverage_percentage': 0,\n",
    "#                 'models_used': 0,\n",
    "#                 'categories_count': 0,\n",
    "#                 'categories': '',\n",
    "#                 'error': str(e),\n",
    "#                 'success': False\n",
    "#             })\n",
    "    \n",
    "#     # Create DataFrame and save results\n",
    "#     df = pd.DataFrame(results_data)\n",
    "#     df.to_csv(output_file, index=False)\n",
    "    \n",
    "#     # Generate summary statistics\n",
    "#     successful_runs = df[df['success'] == True]\n",
    "    \n",
    "#     if len(successful_runs) > 0:\n",
    "#         print(f\"\\n📊 Batch Evaluation Summary:\")\n",
    "#         print(f\"   • Successful runs: {len(successful_runs)}/{len(function_list)}\")\n",
    "#         print(f\"   • Average original tests: {successful_runs['original_tests'].mean():.1f}\")\n",
    "#         print(f\"   • Average final tests: {successful_runs['final_tests'].mean():.1f}\")\n",
    "#         print(f\"   • Average reduction ratio: {successful_runs['reduction_ratio'].mean():.2%}\")\n",
    "#         print(f\"   • Average coverage: {successful_runs['coverage_percentage'].mean():.1f}%\")\n",
    "        \n",
    "#         # Visualization\n",
    "#         fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "#         # Test count distribution\n",
    "#         axes[0, 0].hist(successful_runs['final_tests'], bins=10, alpha=0.7, edgecolor='black')\n",
    "#         axes[0, 0].set_title('Distribution of Final Test Counts')\n",
    "#         axes[0, 0].set_xlabel('Number of Final Tests')\n",
    "#         axes[0, 0].set_ylabel('Frequency')\n",
    "        \n",
    "#         # Coverage distribution\n",
    "#         axes[0, 1].hist(successful_runs['coverage_percentage'], bins=10, alpha=0.7, edgecolor='black')\n",
    "#         axes[0, 1].set_title('Distribution of Code Coverage')\n",
    "#         axes[0, 1].set_xlabel('Coverage Percentage')\n",
    "#         axes[0, 1].set_ylabel('Frequency')\n",
    "        \n",
    "#         # Reduction ratio distribution\n",
    "#         axes[1, 0].hist(successful_runs['reduction_ratio'], bins=10, alpha=0.7, edgecolor='black')\n",
    "#         axes[1, 0].set_title('Distribution of Test Reduction Ratios')\n",
    "#         axes[1, 0].set_xlabel('Reduction Ratio')\n",
    "#         axes[1, 0].set_ylabel('Frequency')\n",
    "        \n",
    "#         # Correlation plot\n",
    "#         axes[1, 1].scatter(successful_runs['original_tests'], successful_runs['coverage_percentage'])\n",
    "#         axes[1, 1].set_title('Original Tests vs Coverage')\n",
    "#         axes[1, 1].set_xlabel('Original Test Count')\n",
    "#         axes[1, 1].set_ylabel('Coverage Percentage')\n",
    "        \n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "#     else:\n",
    "#         print(\"❌ No successful runs to analyze\")\n",
    "    \n",
    "#     print(f\"📁 Detailed results saved to {output_file}\")\n",
    "#     return df\n",
    "\n",
    "# # Example batch evaluation with the corrected functions\n",
    "# example_functions = [example_function_1, example_function_2]\n",
    "# batch_results = batch_evaluate_functions(example_functions)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "TestGen Council",
   "language": "python",
   "name": "testgen-council"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
